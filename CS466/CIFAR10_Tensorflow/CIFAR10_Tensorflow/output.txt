########################################################################################
########################################################################################
########################################################################################
2017-04-30 23:34:51.979702: Running on MSI...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-04-30 23:35:11.082622: step 0, loss = 4.67 (7591.2 examples/sec; 0.017 sec/batch)
2017-04-30 23:35:14.880885: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-01 14:03:23.369124: Running on MSI...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-01 14:03:43.588935: step 0, loss = 4.67 (7318.6 examples/sec; 0.017 sec/batch)
2017-05-01 14:03:47.274226: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-01 14:07:30.386224: Running on MSI...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-01 14:07:49.082088: step 0, loss = 4.68 (7771.3 examples/sec; 0.016 sec/batch)
2017-05-01 14:08:28.672942: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-01 15:41:47.989052: Running on MSI...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-01 15:42:08.087093: step 0, loss = 4.68 (7200.1 examples/sec; 0.018 sec/batch)
2017-05-01 15:42:47.957178: DONE
########################################################################################
########################################################################################
########################################################################################
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 03:10:50.709811: step 0, loss = 4.68 (8012.4 examples/sec; 0.016 sec/batch)
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 05:11:27.176241: step 0, loss = 6.38 (1982.8 examples/sec; 0.065 sec/batch)
########################################################################################
########################################################################################
########################################################################################
2017-05-02 19:42:16.698996: Running on MSI...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 19:42:26.526405: step 0, loss = 6.38 (1818.8 examples/sec; 0.070 sec/batch)
2017-05-02 19:44:14.946251: step 100, loss = 5.90 (118.1 examples/sec; 1.084 sec/batch)
2017-05-02 19:46:18.165279: step 200, loss = 5.10 (103.9 examples/sec; 1.232 sec/batch)
2017-05-02 19:48:03.199919: precision @ 1 = 0.545
2017-05-02 19:48:03.914295: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 19:49:23.382655: Running on MSI...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 19:51:44.222357: precision @ 1 = 0.566
2017-05-02 19:51:44.827254: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 23:10:34.805595: Running on MSI...
########################################################################################
########################################################################################
########################################################################################
2017-05-02 23:14:53.416338: Running on MSI...
########################################################################################
########################################################################################
########################################################################################
2017-05-02 23:16:05.473184: Running on MSI...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 23:16:14.502855: step 0, loss = 6.38 (1902.7 examples/sec; 0.067 sec/batch)
########################################################################################
########################################################################################
########################################################################################
2017-05-02 23:18:10.791801: Running on MSI...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 23:19:07.150263: precision @ 1 = 0.495
2017-05-02 23:19:07.848757: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 13:22:25.640494: Running on server...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 13:23:34.025582: precision @ 1 = 0.000
2017-05-02 13:23:34.447543: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 13:36:30.237930: Running on server...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 13:37:32.690747: precision @ 1 = 0.498
2017-05-02 13:37:33.090898: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 13:38:09.926342: Running on server...
########################################################################################
########################################################################################
########################################################################################
2017-05-02 13:39:30.727659: Running on server...
########################################################################################
########################################################################################
########################################################################################
2017-05-02 13:39:59.824929: Running on server...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 13:41:56.159177: precision @ 1 = 0.512
2017-05-02 13:41:56.551452: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 13:43:31.491927: Running on server...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 13:45:20.958808: precision @ 1 = 0.000
2017-05-02 13:45:21.373092: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 13:48:43.781418: Running on server...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 13:50:32.484337: precision @ 1 = 0.487
2017-05-02 13:50:32.883299: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 13:53:59.767160: Running on server...
########################################################################################
########################################################################################
########################################################################################
2017-05-02 13:54:29.556481: Running on server...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 14:50:37.834941: precision @ 1 = 0.834
2017-05-02 14:50:38.205392: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 16:33:44.146097: Running on server...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 16:34:14.374875: step 0, loss = 6.38 (12.4 examples/sec; 10.295 sec/batch)
2017-05-02 16:34:49.113811: step 100, loss = 5.64 (1429.9 examples/sec; 0.090 sec/batch)
2017-05-02 16:35:14.933199: step 200, loss = 4.78 (1252.8 examples/sec; 0.102 sec/batch)
2017-05-02 16:35:38.773353: step 300, loss = 4.65 (1268.5 examples/sec; 0.101 sec/batch)
2017-05-02 16:35:59.403723: step 400, loss = 4.35 (1201.6 examples/sec; 0.107 sec/batch)
2017-05-02 16:36:20.196845: step 500, loss = 3.86 (1295.0 examples/sec; 0.099 sec/batch)
2017-05-02 16:36:39.009702: step 600, loss = 3.71 (1431.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:36:59.917474: step 700, loss = 3.27 (1318.8 examples/sec; 0.097 sec/batch)
2017-05-02 16:37:22.136216: step 800, loss = 3.04 (1358.8 examples/sec; 0.094 sec/batch)
2017-05-02 16:37:46.449561: step 900, loss = 2.99 (1377.9 examples/sec; 0.093 sec/batch)
2017-05-02 16:38:15.421375: precision @ 1 = 0.713
2017-05-02 16:38:15.933319: DONE
########################################################################################
########################################################################################
########################################################################################
The experiment details:
max_steps = 1000 log_frequency = 100 num_gpus = 2
########################################################################################
########################################################################################
########################################################################################
2017-05-02 17:31:56.498448: Running on server...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 17:32:30.506995: step 0, loss = 6.37 (10.6 examples/sec; 12.072 sec/batch)
2017-05-02 17:33:10.174777: step 100, loss = 5.51 (1112.8 examples/sec; 0.115 sec/batch)
2017-05-02 17:33:34.751237: step 200, loss = 5.03 (1362.7 examples/sec; 0.094 sec/batch)
2017-05-02 17:33:54.359336: step 300, loss = 4.62 (1323.7 examples/sec; 0.097 sec/batch)
2017-05-02 17:34:13.587034: step 400, loss = 4.30 (1388.9 examples/sec; 0.092 sec/batch)
2017-05-02 17:34:33.441458: step 500, loss = 3.97 (1386.1 examples/sec; 0.092 sec/batch)
2017-05-02 17:34:52.375089: step 600, loss = 3.54 (1453.1 examples/sec; 0.088 sec/batch)
2017-05-02 17:35:11.520342: step 700, loss = 3.48 (1278.9 examples/sec; 0.100 sec/batch)
2017-05-02 17:35:33.253571: step 800, loss = 3.34 (1445.3 examples/sec; 0.089 sec/batch)
2017-05-02 17:35:51.299611: step 900, loss = 2.84 (1471.6 examples/sec; 0.087 sec/batch)
2017-05-02 17:36:17.918064: precision @ 1 = 0.711
2017-05-02 17:36:18.330605: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 17:45:31.199255: Running on server...
The experiment details:
max_steps = 1000 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 17:46:06.482917: step 0, loss = 6.38 (9.9 examples/sec; 12.885 sec/batch)
2017-05-02 17:46:27.050001: step 100, loss = 5.71 (1423.8 examples/sec; 0.090 sec/batch)
2017-05-02 17:46:47.448390: step 200, loss = 5.21 (1389.7 examples/sec; 0.092 sec/batch)
2017-05-02 17:47:05.692106: step 300, loss = 4.49 (1399.2 examples/sec; 0.091 sec/batch)
2017-05-02 17:47:23.638216: step 400, loss = 4.35 (1487.6 examples/sec; 0.086 sec/batch)
2017-05-02 17:47:42.062470: step 500, loss = 3.86 (1474.6 examples/sec; 0.087 sec/batch)
########################################################################################
########################################################################################
########################################################################################
2017-05-02 17:50:15.249141: Running on server...
The experiment details:
max_steps = 1000 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 17:50:48.026697: step 0, loss = 6.38 (11.5 examples/sec; 11.083 sec/batch)
2017-05-02 17:51:11.178423: step 100, loss = 5.64 (1450.8 examples/sec; 0.088 sec/batch)
2017-05-02 17:51:30.233787: step 200, loss = 5.18 (1453.2 examples/sec; 0.088 sec/batch)
2017-05-02 17:51:49.429536: step 300, loss = 4.51 (1457.5 examples/sec; 0.088 sec/batch)
2017-05-02 17:52:07.644800: step 400, loss = 4.27 (1426.9 examples/sec; 0.090 sec/batch)
2017-05-02 17:52:27.556866: step 500, loss = 4.13 (1436.4 examples/sec; 0.089 sec/batch)
2017-05-02 17:52:53.431334: step 600, loss = 3.91 (1453.8 examples/sec; 0.088 sec/batch)
2017-05-02 17:53:12.830494: step 700, loss = 3.49 (1266.9 examples/sec; 0.101 sec/batch)
2017-05-02 17:53:31.149599: step 800, loss = 3.17 (1446.2 examples/sec; 0.089 sec/batch)
2017-05-02 17:53:49.069866: step 900, loss = 2.99 (1513.2 examples/sec; 0.085 sec/batch)
2017-05-02 17:54:13.430937: precision @ 1 = 0.710
2017-05-02 17:54:13.822117: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 17:58:43.667286: Running on server...
The experiment details:
max_steps = 100 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 17:59:12.759130: step 0, loss = 6.38 (11.9 examples/sec; 10.756 sec/batch)
2017-05-02 17:59:41.542099: precision @ 1 = 0.446
2017-05-02 17:59:41.957548: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 17:59:42.022295: Running on server...
The experiment details:
max_steps = 100 log_frequency = 100 num_gpus = 2
########################################################################################
########################################################################################
########################################################################################
2017-05-02 18:00:39.185265: Running on server...
The experiment details:
max_steps = 100 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 18:01:15.921001: step 0, loss = 6.38 (8.9 examples/sec; 14.317 sec/batch)
2017-05-02 18:01:56.185569: precision @ 1 = 0.446
2017-05-02 18:01:57.050571: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 18:17:44.470974: Running on server...
The experiment details:
max_steps = 30000 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 18:18:09.650383: step 0, loss = 6.37 (14.5 examples/sec; 8.802 sec/batch)
2017-05-02 18:18:29.276130: step 100, loss = 5.62 (1515.9 examples/sec; 0.084 sec/batch)
2017-05-02 18:18:46.677021: step 200, loss = 4.93 (1508.0 examples/sec; 0.085 sec/batch)
2017-05-02 18:19:04.087997: step 300, loss = 4.67 (1505.4 examples/sec; 0.085 sec/batch)
2017-05-02 18:19:21.195210: step 400, loss = 4.16 (1525.5 examples/sec; 0.084 sec/batch)
2017-05-02 18:19:38.585594: step 500, loss = 3.85 (1481.9 examples/sec; 0.086 sec/batch)
2017-05-02 18:19:55.776764: step 600, loss = 3.53 (1460.8 examples/sec; 0.088 sec/batch)
2017-05-02 18:20:14.028559: step 700, loss = 3.56 (1498.4 examples/sec; 0.085 sec/batch)
2017-05-02 18:20:32.181412: step 800, loss = 3.25 (1507.6 examples/sec; 0.085 sec/batch)
2017-05-02 18:20:49.636057: step 900, loss = 2.88 (1436.0 examples/sec; 0.089 sec/batch)
2017-05-02 18:21:07.140694: step 1000, loss = 2.87 (1514.2 examples/sec; 0.085 sec/batch)
2017-05-02 18:21:26.605593: step 1100, loss = 2.51 (476.3 examples/sec; 0.269 sec/batch)
2017-05-02 18:21:43.848308: step 1200, loss = 2.41 (1505.8 examples/sec; 0.085 sec/batch)
2017-05-02 18:22:01.567575: step 1300, loss = 2.13 (1464.1 examples/sec; 0.087 sec/batch)
2017-05-02 18:22:19.052845: step 1400, loss = 2.13 (1506.6 examples/sec; 0.085 sec/batch)
2017-05-02 18:22:37.700160: step 1500, loss = 2.17 (1431.9 examples/sec; 0.089 sec/batch)
2017-05-02 18:22:55.126680: step 1600, loss = 1.96 (1505.5 examples/sec; 0.085 sec/batch)
2017-05-02 18:23:12.911478: step 1700, loss = 1.89 (1392.8 examples/sec; 0.092 sec/batch)
2017-05-02 18:23:30.338358: step 1800, loss = 1.69 (1503.2 examples/sec; 0.085 sec/batch)
2017-05-02 18:23:47.984820: step 1900, loss = 1.64 (1506.9 examples/sec; 0.085 sec/batch)
2017-05-02 18:24:05.374922: step 2000, loss = 1.43 (1506.8 examples/sec; 0.085 sec/batch)
2017-05-02 18:24:24.677808: step 2100, loss = 1.56 (1491.1 examples/sec; 0.086 sec/batch)
2017-05-02 18:24:42.103722: step 2200, loss = 1.39 (1519.1 examples/sec; 0.084 sec/batch)
2017-05-02 18:24:59.442160: step 2300, loss = 1.17 (1492.1 examples/sec; 0.086 sec/batch)
2017-05-02 18:25:17.050851: step 2400, loss = 1.27 (1478.5 examples/sec; 0.087 sec/batch)
2017-05-02 18:25:34.731751: step 2500, loss = 1.17 (1531.8 examples/sec; 0.084 sec/batch)
2017-05-02 18:25:52.045027: step 2600, loss = 1.10 (1443.3 examples/sec; 0.089 sec/batch)
2017-05-02 18:26:09.913263: step 2700, loss = 0.96 (1474.6 examples/sec; 0.087 sec/batch)
2017-05-02 18:26:27.471754: step 2800, loss = 0.96 (1511.3 examples/sec; 0.085 sec/batch)
2017-05-02 18:26:44.964049: step 2900, loss = 0.90 (1491.2 examples/sec; 0.086 sec/batch)
2017-05-02 18:27:02.746464: step 3000, loss = 0.92 (1490.0 examples/sec; 0.086 sec/batch)
2017-05-02 18:27:22.314915: step 3100, loss = 0.91 (1540.9 examples/sec; 0.083 sec/batch)
2017-05-02 18:27:39.791499: step 3200, loss = 0.86 (1514.1 examples/sec; 0.085 sec/batch)
2017-05-02 18:27:57.821654: step 3300, loss = 0.94 (1477.9 examples/sec; 0.087 sec/batch)
2017-05-02 18:28:15.275126: step 3400, loss = 0.86 (1507.1 examples/sec; 0.085 sec/batch)
2017-05-02 18:28:32.697662: step 3500, loss = 0.77 (1479.2 examples/sec; 0.087 sec/batch)
2017-05-02 18:28:50.135750: step 3600, loss = 0.94 (1529.0 examples/sec; 0.084 sec/batch)
2017-05-02 18:29:07.664664: step 3700, loss = 0.82 (1466.9 examples/sec; 0.087 sec/batch)
2017-05-02 18:29:25.337218: step 3800, loss = 0.73 (1367.5 examples/sec; 0.094 sec/batch)
2017-05-02 18:29:42.958317: step 3900, loss = 0.68 (1465.6 examples/sec; 0.087 sec/batch)
2017-05-02 18:30:00.772204: step 4000, loss = 0.71 (1507.0 examples/sec; 0.085 sec/batch)
2017-05-02 18:30:20.193414: step 4100, loss = 0.94 (1487.9 examples/sec; 0.086 sec/batch)
2017-05-02 18:30:37.958400: step 4200, loss = 0.70 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-02 18:30:55.555286: step 4300, loss = 0.58 (1518.7 examples/sec; 0.084 sec/batch)
2017-05-02 18:31:13.060451: step 4400, loss = 0.56 (1494.6 examples/sec; 0.086 sec/batch)
2017-05-02 18:31:30.545714: step 4500, loss = 0.65 (1471.5 examples/sec; 0.087 sec/batch)
2017-05-02 18:31:48.904718: step 4600, loss = 0.53 (1499.3 examples/sec; 0.085 sec/batch)
2017-05-02 18:32:06.706775: step 4700, loss = 0.46 (1522.6 examples/sec; 0.084 sec/batch)
2017-05-02 18:32:24.281454: step 4800, loss = 0.62 (1470.5 examples/sec; 0.087 sec/batch)
2017-05-02 18:32:42.161571: step 4900, loss = 0.51 (1480.0 examples/sec; 0.086 sec/batch)
2017-05-02 18:32:59.939388: step 5000, loss = 0.61 (1472.7 examples/sec; 0.087 sec/batch)
2017-05-02 18:33:19.270810: step 5100, loss = 0.56 (1512.8 examples/sec; 0.085 sec/batch)
2017-05-02 18:33:37.044476: step 5200, loss = 0.51 (1490.4 examples/sec; 0.086 sec/batch)
2017-05-02 18:33:54.593386: step 5300, loss = 0.49 (1487.5 examples/sec; 0.086 sec/batch)
2017-05-02 18:34:13.406068: step 5400, loss = 0.44 (1382.2 examples/sec; 0.093 sec/batch)
2017-05-02 18:34:30.918694: step 5500, loss = 0.57 (1484.1 examples/sec; 0.086 sec/batch)
2017-05-02 18:34:48.392447: step 5600, loss = 0.49 (1464.6 examples/sec; 0.087 sec/batch)
2017-05-02 18:35:05.878322: step 5700, loss = 0.44 (1510.8 examples/sec; 0.085 sec/batch)
2017-05-02 18:35:23.342209: step 5800, loss = 0.52 (1533.5 examples/sec; 0.083 sec/batch)
2017-05-02 18:35:40.737003: step 5900, loss = 0.43 (1478.7 examples/sec; 0.087 sec/batch)
2017-05-02 18:35:59.257812: step 6000, loss = 0.39 (1490.7 examples/sec; 0.086 sec/batch)
2017-05-02 18:36:19.932405: step 6100, loss = 0.63 (1448.9 examples/sec; 0.088 sec/batch)
2017-05-02 18:36:37.539974: step 6200, loss = 0.46 (1463.9 examples/sec; 0.087 sec/batch)
2017-05-02 18:36:55.027342: step 6300, loss = 1.11 (1513.6 examples/sec; 0.085 sec/batch)
2017-05-02 18:37:12.620446: step 6400, loss = 0.48 (1435.0 examples/sec; 0.089 sec/batch)
2017-05-02 18:37:30.256258: step 6500, loss = 0.43 (1466.4 examples/sec; 0.087 sec/batch)
2017-05-02 18:37:48.481925: step 6600, loss = 0.42 (1446.8 examples/sec; 0.088 sec/batch)
2017-05-02 18:38:06.351433: step 6700, loss = 0.41 (1486.3 examples/sec; 0.086 sec/batch)
2017-05-02 18:38:24.185703: step 6800, loss = 0.54 (1462.1 examples/sec; 0.088 sec/batch)
2017-05-02 18:38:41.892307: step 6900, loss = 0.42 (1468.0 examples/sec; 0.087 sec/batch)
2017-05-02 18:38:59.628980: step 7000, loss = 0.46 (1496.4 examples/sec; 0.086 sec/batch)
2017-05-02 18:39:18.859909: step 7100, loss = 0.41 (1513.6 examples/sec; 0.085 sec/batch)
2017-05-02 18:39:36.311547: step 7200, loss = 0.34 (1480.3 examples/sec; 0.086 sec/batch)
2017-05-02 18:39:53.808609: step 7300, loss = 0.40 (1490.6 examples/sec; 0.086 sec/batch)
2017-05-02 18:40:11.235237: step 7400, loss = 0.34 (1492.4 examples/sec; 0.086 sec/batch)
2017-05-02 18:40:29.179641: step 7500, loss = 0.33 (1473.0 examples/sec; 0.087 sec/batch)
2017-05-02 18:40:46.849693: step 7600, loss = 0.46 (1470.3 examples/sec; 0.087 sec/batch)
2017-05-02 18:41:05.014433: step 7700, loss = 0.47 (1451.6 examples/sec; 0.088 sec/batch)
2017-05-02 18:41:22.665599: step 7800, loss = 0.35 (1453.1 examples/sec; 0.088 sec/batch)
2017-05-02 18:41:40.239491: step 7900, loss = 1.75 (1415.8 examples/sec; 0.090 sec/batch)
2017-05-02 18:41:57.873170: step 8000, loss = 0.44 (1469.7 examples/sec; 0.087 sec/batch)
2017-05-02 18:42:19.276744: step 8100, loss = 0.39 (1437.0 examples/sec; 0.089 sec/batch)
2017-05-02 18:42:37.163847: step 8200, loss = 0.42 (1454.6 examples/sec; 0.088 sec/batch)
2017-05-02 18:42:55.021150: step 8300, loss = 0.41 (1488.5 examples/sec; 0.086 sec/batch)
2017-05-02 18:43:12.204493: step 8400, loss = 0.46 (1538.0 examples/sec; 0.083 sec/batch)
2017-05-02 18:43:29.514160: step 8500, loss = 0.40 (1527.3 examples/sec; 0.084 sec/batch)
2017-05-02 18:43:47.282228: step 8600, loss = 0.46 (1441.1 examples/sec; 0.089 sec/batch)
2017-05-02 18:44:04.644396: step 8700, loss = 0.44 (1499.9 examples/sec; 0.085 sec/batch)
2017-05-02 18:44:22.103160: step 8800, loss = 0.43 (1509.3 examples/sec; 0.085 sec/batch)
2017-05-02 18:44:39.678403: step 8900, loss = 0.40 (1476.2 examples/sec; 0.087 sec/batch)
2017-05-02 18:44:57.180880: step 9000, loss = 0.43 (1464.4 examples/sec; 0.087 sec/batch)
2017-05-02 18:45:16.103237: step 9100, loss = 0.40 (1465.4 examples/sec; 0.087 sec/batch)
2017-05-02 18:45:33.694017: step 9200, loss = 0.38 (1520.8 examples/sec; 0.084 sec/batch)
2017-05-02 18:45:51.112190: step 9300, loss = 0.38 (1480.7 examples/sec; 0.086 sec/batch)
2017-05-02 18:46:08.605784: step 9400, loss = 0.35 (1498.5 examples/sec; 0.085 sec/batch)
2017-05-02 18:46:26.219260: step 9500, loss = 0.43 (1423.1 examples/sec; 0.090 sec/batch)
2017-05-02 18:46:43.814697: step 9600, loss = 0.33 (1431.4 examples/sec; 0.089 sec/batch)
2017-05-02 18:47:01.252524: step 9700, loss = 0.37 (1482.1 examples/sec; 0.086 sec/batch)
2017-05-02 18:47:18.921002: step 9800, loss = 0.35 (1389.1 examples/sec; 0.092 sec/batch)
2017-05-02 18:47:36.361758: step 9900, loss = 0.37 (1473.4 examples/sec; 0.087 sec/batch)
2017-05-02 18:47:53.957981: step 10000, loss = 0.59 (1468.6 examples/sec; 0.087 sec/batch)
2017-05-02 18:48:13.168870: step 10100, loss = 0.39 (1492.0 examples/sec; 0.086 sec/batch)
2017-05-02 18:48:30.775244: step 10200, loss = 0.35 (1503.4 examples/sec; 0.085 sec/batch)
2017-05-02 18:48:48.338551: step 10300, loss = 0.41 (1486.3 examples/sec; 0.086 sec/batch)
2017-05-02 18:49:06.008107: step 10400, loss = 0.37 (1469.9 examples/sec; 0.087 sec/batch)
2017-05-02 18:49:23.472385: step 10500, loss = 0.34 (1489.5 examples/sec; 0.086 sec/batch)
2017-05-02 18:49:40.918897: step 10600, loss = 0.47 (1497.5 examples/sec; 0.085 sec/batch)
2017-05-02 18:49:58.334472: step 10700, loss = 0.37 (1519.7 examples/sec; 0.084 sec/batch)
2017-05-02 18:50:15.759204: step 10800, loss = 0.32 (1511.3 examples/sec; 0.085 sec/batch)
2017-05-02 18:50:33.226115: step 10900, loss = 0.43 (1480.4 examples/sec; 0.086 sec/batch)
2017-05-02 18:50:51.024537: step 11000, loss = 0.39 (1490.4 examples/sec; 0.086 sec/batch)
2017-05-02 18:51:10.162526: step 11100, loss = 0.37 (1483.3 examples/sec; 0.086 sec/batch)
2017-05-02 18:51:27.709181: step 11200, loss = 0.41 (1477.9 examples/sec; 0.087 sec/batch)
2017-05-02 18:51:45.200273: step 11300, loss = 0.39 (1491.6 examples/sec; 0.086 sec/batch)
2017-05-02 18:52:02.699287: step 11400, loss = 0.37 (1438.8 examples/sec; 0.089 sec/batch)
2017-05-02 18:52:20.389737: step 11500, loss = 0.47 (1481.1 examples/sec; 0.086 sec/batch)
2017-05-02 18:52:38.020897: step 11600, loss = 0.42 (1518.0 examples/sec; 0.084 sec/batch)
2017-05-02 18:52:55.755700: step 11700, loss = 0.36 (1356.2 examples/sec; 0.094 sec/batch)
2017-05-02 18:53:13.345455: step 11800, loss = 0.33 (1498.2 examples/sec; 0.085 sec/batch)
2017-05-02 18:53:30.880150: step 11900, loss = 0.42 (1480.6 examples/sec; 0.086 sec/batch)
2017-05-02 18:53:48.340382: step 12000, loss = 0.38 (1491.6 examples/sec; 0.086 sec/batch)
2017-05-02 18:54:07.494958: step 12100, loss = 0.32 (1457.7 examples/sec; 0.088 sec/batch)
2017-05-02 18:54:24.898706: step 12200, loss = 0.41 (1512.9 examples/sec; 0.085 sec/batch)
2017-05-02 18:54:42.518339: step 12300, loss = 0.36 (1483.2 examples/sec; 0.086 sec/batch)
2017-05-02 18:55:00.063674: step 12400, loss = 0.30 (1472.9 examples/sec; 0.087 sec/batch)
2017-05-02 18:55:17.535631: step 12500, loss = 0.35 (1473.2 examples/sec; 0.087 sec/batch)
2017-05-02 18:55:35.128185: step 12600, loss = 0.34 (1490.4 examples/sec; 0.086 sec/batch)
2017-05-02 18:55:52.546169: step 12700, loss = 0.53 (1495.7 examples/sec; 0.086 sec/batch)
2017-05-02 18:56:10.137206: step 12800, loss = 0.41 (1511.1 examples/sec; 0.085 sec/batch)
2017-05-02 18:56:27.714641: step 12900, loss = 0.70 (1432.8 examples/sec; 0.089 sec/batch)
2017-05-02 18:56:45.345258: step 13000, loss = 0.43 (1463.2 examples/sec; 0.087 sec/batch)
2017-05-02 18:57:04.552542: step 13100, loss = 0.50 (1466.3 examples/sec; 0.087 sec/batch)
2017-05-02 18:57:22.283365: step 13200, loss = 0.35 (1484.0 examples/sec; 0.086 sec/batch)
2017-05-02 18:57:39.843609: step 13300, loss = 0.43 (1460.2 examples/sec; 0.088 sec/batch)
2017-05-02 18:57:57.546242: step 13400, loss = 0.42 (1417.6 examples/sec; 0.090 sec/batch)
2017-05-02 18:58:15.165322: step 13500, loss = 0.44 (1487.5 examples/sec; 0.086 sec/batch)
2017-05-02 18:58:32.706169: step 13600, loss = 0.41 (1497.7 examples/sec; 0.085 sec/batch)
2017-05-02 18:58:50.245692: step 13700, loss = 0.35 (1508.1 examples/sec; 0.085 sec/batch)
2017-05-02 18:59:07.719432: step 13800, loss = 0.41 (1492.7 examples/sec; 0.086 sec/batch)
2017-05-02 18:59:25.140024: step 13900, loss = 0.37 (1529.6 examples/sec; 0.084 sec/batch)
2017-05-02 18:59:42.951909: step 14000, loss = 0.36 (1461.3 examples/sec; 0.088 sec/batch)
2017-05-02 19:00:02.209814: step 14100, loss = 0.42 (1461.9 examples/sec; 0.088 sec/batch)
2017-05-02 19:00:19.957586: step 14200, loss = 0.36 (1487.8 examples/sec; 0.086 sec/batch)
2017-05-02 19:00:37.779381: step 14300, loss = 0.48 (1502.7 examples/sec; 0.085 sec/batch)
2017-05-02 19:00:55.317083: step 14400, loss = 0.42 (1479.2 examples/sec; 0.087 sec/batch)
2017-05-02 19:01:12.807641: step 14500, loss = 0.31 (1462.5 examples/sec; 0.088 sec/batch)
2017-05-02 19:01:30.341507: step 14600, loss = 0.39 (1447.7 examples/sec; 0.088 sec/batch)
2017-05-02 19:01:48.007340: step 14700, loss = 0.42 (1450.5 examples/sec; 0.088 sec/batch)
2017-05-02 19:02:05.796259: step 14800, loss = 0.37 (1450.8 examples/sec; 0.088 sec/batch)
2017-05-02 19:02:23.404595: step 14900, loss = 0.30 (1492.5 examples/sec; 0.086 sec/batch)
2017-05-02 19:02:41.047996: step 15000, loss = 0.35 (1481.9 examples/sec; 0.086 sec/batch)
2017-05-02 19:03:02.008369: step 15100, loss = 0.35 (1499.4 examples/sec; 0.085 sec/batch)
2017-05-02 19:03:19.538124: step 15200, loss = 0.36 (1470.4 examples/sec; 0.087 sec/batch)
2017-05-02 19:03:37.046255: step 15300, loss = 0.37 (1534.8 examples/sec; 0.083 sec/batch)
2017-05-02 19:03:54.932137: step 15400, loss = 0.37 (1426.7 examples/sec; 0.090 sec/batch)
2017-05-02 19:04:12.498391: step 15500, loss = 0.41 (1518.9 examples/sec; 0.084 sec/batch)
2017-05-02 19:04:30.001789: step 15600, loss = 0.49 (1461.4 examples/sec; 0.088 sec/batch)
2017-05-02 19:04:47.458093: step 15700, loss = 0.44 (1485.4 examples/sec; 0.086 sec/batch)
2017-05-02 19:05:04.953351: step 15800, loss = 0.34 (1432.7 examples/sec; 0.089 sec/batch)
2017-05-02 19:05:22.449971: step 15900, loss = 0.35 (1476.8 examples/sec; 0.087 sec/batch)
2017-05-02 19:05:40.013330: step 16000, loss = 0.30 (1502.1 examples/sec; 0.085 sec/batch)
2017-05-02 19:05:59.042067: step 16100, loss = 0.36 (1441.3 examples/sec; 0.089 sec/batch)
2017-05-02 19:06:16.663428: step 16200, loss = 0.32 (1465.0 examples/sec; 0.087 sec/batch)
2017-05-02 19:06:34.994918: step 16300, loss = 0.58 (1501.6 examples/sec; 0.085 sec/batch)
2017-05-02 19:06:52.582933: step 16400, loss = 0.34 (1462.1 examples/sec; 0.088 sec/batch)
2017-05-02 19:07:10.345489: step 16500, loss = 0.46 (1476.1 examples/sec; 0.087 sec/batch)
2017-05-02 19:07:28.538790: step 16600, loss = 0.39 (1443.3 examples/sec; 0.089 sec/batch)
2017-05-02 19:07:46.259003: step 16700, loss = 0.33 (1454.3 examples/sec; 0.088 sec/batch)
2017-05-02 19:08:03.843245: step 16800, loss = 0.35 (1499.7 examples/sec; 0.085 sec/batch)
2017-05-02 19:08:21.582435: step 16900, loss = 0.41 (1385.7 examples/sec; 0.092 sec/batch)
2017-05-02 19:08:39.609675: step 17000, loss = 0.51 (1336.7 examples/sec; 0.096 sec/batch)
2017-05-02 19:08:59.686966: step 17100, loss = 0.33 (1425.5 examples/sec; 0.090 sec/batch)
2017-05-02 19:09:18.109851: step 17200, loss = 0.34 (1455.9 examples/sec; 0.088 sec/batch)
2017-05-02 19:09:36.768066: step 17300, loss = 0.32 (1461.0 examples/sec; 0.088 sec/batch)
2017-05-02 19:09:54.993777: step 17400, loss = 0.31 (1429.3 examples/sec; 0.090 sec/batch)
2017-05-02 19:10:13.112570: step 17500, loss = 0.36 (1444.4 examples/sec; 0.089 sec/batch)
2017-05-02 19:10:31.035481: step 17600, loss = 0.44 (1362.7 examples/sec; 0.094 sec/batch)
2017-05-02 19:10:49.445350: step 17700, loss = 0.29 (1452.7 examples/sec; 0.088 sec/batch)
2017-05-02 19:11:07.164353: step 17800, loss = 0.30 (1503.7 examples/sec; 0.085 sec/batch)
2017-05-02 19:11:24.963356: step 17900, loss = 0.39 (1416.2 examples/sec; 0.090 sec/batch)
2017-05-02 19:11:43.033899: step 18000, loss = 0.34 (1424.0 examples/sec; 0.090 sec/batch)
2017-05-02 19:12:02.383392: step 18100, loss = 0.35 (1499.8 examples/sec; 0.085 sec/batch)
2017-05-02 19:12:20.446696: step 18200, loss = 0.31 (1478.3 examples/sec; 0.087 sec/batch)
2017-05-02 19:12:38.608421: step 18300, loss = 0.28 (1386.4 examples/sec; 0.092 sec/batch)
2017-05-02 19:12:56.499342: step 18400, loss = 0.38 (1458.3 examples/sec; 0.088 sec/batch)
2017-05-02 19:13:14.464512: step 18500, loss = 0.35 (1412.2 examples/sec; 0.091 sec/batch)
2017-05-02 19:13:32.291663: step 18600, loss = 0.29 (1429.0 examples/sec; 0.090 sec/batch)
2017-05-02 19:13:50.120186: step 18700, loss = 0.39 (1474.8 examples/sec; 0.087 sec/batch)
2017-05-02 19:14:08.142327: step 18800, loss = 0.35 (1430.8 examples/sec; 0.089 sec/batch)
2017-05-02 19:14:26.365027: step 18900, loss = 0.32 (1416.7 examples/sec; 0.090 sec/batch)
2017-05-02 19:14:45.572758: step 19000, loss = 0.42 (1339.0 examples/sec; 0.096 sec/batch)
2017-05-02 19:15:05.424098: step 19100, loss = 0.43 (1362.7 examples/sec; 0.094 sec/batch)
2017-05-02 19:15:23.837732: step 19200, loss = 0.35 (1434.8 examples/sec; 0.089 sec/batch)
2017-05-02 19:15:42.500734: step 19300, loss = 0.38 (1341.4 examples/sec; 0.095 sec/batch)
2017-05-02 19:16:00.707296: step 19400, loss = 0.32 (1364.7 examples/sec; 0.094 sec/batch)
2017-05-02 19:16:19.518199: step 19500, loss = 0.36 (1391.7 examples/sec; 0.092 sec/batch)
2017-05-02 19:16:39.068137: step 19600, loss = 0.39 (1170.1 examples/sec; 0.109 sec/batch)
2017-05-02 19:16:56.897011: step 19700, loss = 0.35 (1453.3 examples/sec; 0.088 sec/batch)
2017-05-02 19:17:15.674760: step 19800, loss = 0.35 (1454.1 examples/sec; 0.088 sec/batch)
2017-05-02 19:17:33.826030: step 19900, loss = 0.30 (1470.1 examples/sec; 0.087 sec/batch)
2017-05-02 19:17:52.196728: step 20000, loss = 0.38 (1363.8 examples/sec; 0.094 sec/batch)
2017-05-02 19:18:12.524483: step 20100, loss = 0.38 (1389.8 examples/sec; 0.092 sec/batch)
2017-05-02 19:18:30.734670: step 20200, loss = 0.30 (1446.2 examples/sec; 0.089 sec/batch)
2017-05-02 19:18:49.244318: step 20300, loss = 0.33 (1340.2 examples/sec; 0.096 sec/batch)
2017-05-02 19:19:08.377790: step 20400, loss = 0.35 (1327.9 examples/sec; 0.096 sec/batch)
2017-05-02 19:19:26.790488: step 20500, loss = 0.35 (1488.1 examples/sec; 0.086 sec/batch)
2017-05-02 19:19:45.237230: step 20600, loss = 0.29 (1417.5 examples/sec; 0.090 sec/batch)
2017-05-02 19:20:03.878885: step 20700, loss = 0.32 (1394.9 examples/sec; 0.092 sec/batch)
2017-05-02 19:20:23.941247: step 20800, loss = 0.29 (1374.7 examples/sec; 0.093 sec/batch)
2017-05-02 19:20:43.356740: step 20900, loss = 0.36 (1526.0 examples/sec; 0.084 sec/batch)
2017-05-02 19:21:01.675827: step 21000, loss = 0.45 (1477.4 examples/sec; 0.087 sec/batch)
2017-05-02 19:21:23.742398: step 21100, loss = 0.55 (1486.5 examples/sec; 0.086 sec/batch)
2017-05-02 19:21:42.449329: step 21200, loss = 0.57 (1399.9 examples/sec; 0.091 sec/batch)
2017-05-02 19:22:00.511527: step 21300, loss = 0.41 (1466.4 examples/sec; 0.087 sec/batch)
2017-05-02 19:22:18.614471: step 21400, loss = 0.49 (1458.0 examples/sec; 0.088 sec/batch)
2017-05-02 19:22:39.361516: step 21500, loss = 0.50 (1455.4 examples/sec; 0.088 sec/batch)
2017-05-02 19:22:58.840931: step 21600, loss = 0.52 (1423.1 examples/sec; 0.090 sec/batch)
2017-05-02 19:23:18.064610: step 21700, loss = 0.41 (1455.2 examples/sec; 0.088 sec/batch)
2017-05-02 19:23:39.494070: step 21800, loss = 0.48 (3388.3 examples/sec; 0.038 sec/batch)
2017-05-02 19:23:58.993090: step 21900, loss = 0.41 (1438.9 examples/sec; 0.089 sec/batch)
2017-05-02 19:24:18.398755: step 22000, loss = 0.41 (1474.2 examples/sec; 0.087 sec/batch)
2017-05-02 19:24:38.792511: step 22100, loss = 0.47 (1435.0 examples/sec; 0.089 sec/batch)
2017-05-02 19:24:58.967042: step 22200, loss = 0.42 (1437.4 examples/sec; 0.089 sec/batch)
2017-05-02 19:25:18.655158: step 22300, loss = 0.37 (1428.0 examples/sec; 0.090 sec/batch)
2017-05-02 19:25:37.371493: step 22400, loss = 0.42 (1439.6 examples/sec; 0.089 sec/batch)
2017-05-02 19:25:56.138613: step 22500, loss = 0.45 (1425.2 examples/sec; 0.090 sec/batch)
2017-05-02 19:26:15.428069: step 22600, loss = 0.34 (1428.0 examples/sec; 0.090 sec/batch)
2017-05-02 19:26:35.268016: step 22700, loss = 0.40 (1471.1 examples/sec; 0.087 sec/batch)
2017-05-02 19:26:54.563911: step 22800, loss = 0.36 (1470.3 examples/sec; 0.087 sec/batch)
2017-05-02 19:27:13.194422: step 22900, loss = 0.53 (1512.5 examples/sec; 0.085 sec/batch)
2017-05-02 19:27:33.999811: step 23000, loss = 0.37 (1451.7 examples/sec; 0.088 sec/batch)
2017-05-02 19:27:56.804232: step 23100, loss = 0.44 (2730.9 examples/sec; 0.047 sec/batch)
2017-05-02 19:28:16.515597: step 23200, loss = 0.36 (1379.7 examples/sec; 0.093 sec/batch)
2017-05-02 19:28:36.943751: step 23300, loss = 0.39 (1473.8 examples/sec; 0.087 sec/batch)
2017-05-02 19:28:56.408942: step 23400, loss = 0.44 (1302.8 examples/sec; 0.098 sec/batch)
2017-05-02 19:29:14.876579: step 23500, loss = 0.34 (1388.4 examples/sec; 0.092 sec/batch)
2017-05-02 19:29:33.806582: step 23600, loss = 0.36 (1420.4 examples/sec; 0.090 sec/batch)
2017-05-02 19:29:52.324032: step 23700, loss = 0.34 (1783.9 examples/sec; 0.072 sec/batch)
2017-05-02 19:30:10.968760: step 23800, loss = 0.37 (1515.0 examples/sec; 0.084 sec/batch)
2017-05-02 19:30:29.064489: step 23900, loss = 0.36 (1404.6 examples/sec; 0.091 sec/batch)
2017-05-02 19:30:47.335530: step 24000, loss = 0.33 (1471.3 examples/sec; 0.087 sec/batch)
2017-05-02 19:31:07.387064: step 24100, loss = 0.35 (1500.5 examples/sec; 0.085 sec/batch)
2017-05-02 19:31:26.352003: step 24200, loss = 0.35 (1448.9 examples/sec; 0.088 sec/batch)
2017-05-02 19:31:44.566633: step 24300, loss = 0.40 (1467.6 examples/sec; 0.087 sec/batch)
2017-05-02 19:32:03.042462: step 24400, loss = 0.31 (1464.3 examples/sec; 0.087 sec/batch)
2017-05-02 19:32:21.053969: step 24500, loss = 0.45 (1509.4 examples/sec; 0.085 sec/batch)
2017-05-02 19:32:39.303010: step 24600, loss = 0.33 (1452.5 examples/sec; 0.088 sec/batch)
2017-05-02 19:32:57.239311: step 24700, loss = 0.30 (1466.7 examples/sec; 0.087 sec/batch)
2017-05-02 19:33:15.257511: step 24800, loss = 0.52 (1496.2 examples/sec; 0.086 sec/batch)
2017-05-02 19:33:33.179198: step 24900, loss = 0.39 (1399.8 examples/sec; 0.091 sec/batch)
2017-05-02 19:33:50.923114: step 25000, loss = 0.39 (1476.7 examples/sec; 0.087 sec/batch)
2017-05-02 19:34:10.406094: step 25100, loss = 0.36 (1488.8 examples/sec; 0.086 sec/batch)
2017-05-02 19:34:29.471843: step 25200, loss = 0.37 (1443.1 examples/sec; 0.089 sec/batch)
2017-05-02 19:34:47.716232: step 25300, loss = 0.31 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-02 19:35:06.617739: step 25400, loss = 0.38 (1361.6 examples/sec; 0.094 sec/batch)
2017-05-02 19:35:25.294312: step 25500, loss = 0.32 (1456.6 examples/sec; 0.088 sec/batch)
2017-05-02 19:35:44.472570: step 25600, loss = 0.36 (1494.5 examples/sec; 0.086 sec/batch)
2017-05-02 19:36:02.596812: step 25700, loss = 0.37 (1484.9 examples/sec; 0.086 sec/batch)
2017-05-02 19:36:20.922565: step 25800, loss = 0.35 (1463.8 examples/sec; 0.087 sec/batch)
2017-05-02 19:36:39.107621: step 25900, loss = 0.36 (1479.7 examples/sec; 0.087 sec/batch)
2017-05-02 19:36:58.040363: step 26000, loss = 0.35 (1434.3 examples/sec; 0.089 sec/batch)
2017-05-02 19:37:18.175585: step 26100, loss = 0.39 (1410.2 examples/sec; 0.091 sec/batch)
2017-05-02 19:37:36.565564: step 26200, loss = 0.34 (1429.2 examples/sec; 0.090 sec/batch)
2017-05-02 19:37:55.132365: step 26300, loss = 0.32 (1634.4 examples/sec; 0.078 sec/batch)
2017-05-02 19:38:13.263245: step 26400, loss = 0.39 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-02 19:38:31.294745: step 26500, loss = 0.28 (1350.5 examples/sec; 0.095 sec/batch)
2017-05-02 19:38:50.471828: step 26600, loss = 0.35 (1477.4 examples/sec; 0.087 sec/batch)
2017-05-02 19:39:09.409920: step 26700, loss = 0.29 (1440.0 examples/sec; 0.089 sec/batch)
2017-05-02 19:39:27.805426: step 26800, loss = 0.35 (1463.7 examples/sec; 0.087 sec/batch)
2017-05-02 19:39:46.368611: step 26900, loss = 0.31 (1451.6 examples/sec; 0.088 sec/batch)
2017-05-02 19:40:04.373904: step 27000, loss = 0.31 (1435.3 examples/sec; 0.089 sec/batch)
2017-05-02 19:40:23.941890: step 27100, loss = 0.41 (1437.9 examples/sec; 0.089 sec/batch)
2017-05-02 19:40:42.649431: step 27200, loss = 0.36 (1709.5 examples/sec; 0.075 sec/batch)
2017-05-02 19:41:00.727540: step 27300, loss = 0.32 (1436.0 examples/sec; 0.089 sec/batch)
2017-05-02 19:41:18.954181: step 27400, loss = 0.29 (1430.6 examples/sec; 0.089 sec/batch)
2017-05-02 19:41:37.557101: step 27500, loss = 0.34 (601.7 examples/sec; 0.213 sec/batch)
2017-05-02 19:41:55.594021: step 27600, loss = 0.43 (1412.4 examples/sec; 0.091 sec/batch)
2017-05-02 19:42:14.221796: step 27700, loss = 0.36 (1441.5 examples/sec; 0.089 sec/batch)
2017-05-02 19:42:32.411910: step 27800, loss = 0.35 (1405.2 examples/sec; 0.091 sec/batch)
2017-05-02 19:42:50.486484: step 27900, loss = 0.35 (1487.7 examples/sec; 0.086 sec/batch)
2017-05-02 19:43:08.803950: step 28000, loss = 0.37 (1454.8 examples/sec; 0.088 sec/batch)
2017-05-02 19:43:29.006027: step 28100, loss = 0.40 (1417.5 examples/sec; 0.090 sec/batch)
2017-05-02 19:43:47.729508: step 28200, loss = 0.32 (1433.1 examples/sec; 0.089 sec/batch)
2017-05-02 19:44:06.498220: step 28300, loss = 0.31 (1441.5 examples/sec; 0.089 sec/batch)
2017-05-02 19:44:24.973333: step 28400, loss = 0.35 (1471.0 examples/sec; 0.087 sec/batch)
2017-05-02 19:44:44.209580: step 28500, loss = 0.33 (1448.7 examples/sec; 0.088 sec/batch)
2017-05-02 19:45:02.517580: step 28600, loss = 0.32 (1415.1 examples/sec; 0.090 sec/batch)
2017-05-02 19:45:22.081939: step 28700, loss = 0.30 (1428.1 examples/sec; 0.090 sec/batch)
2017-05-02 19:45:40.661183: step 28800, loss = 0.28 (1429.3 examples/sec; 0.090 sec/batch)
2017-05-02 19:45:59.584009: step 28900, loss = 0.28 (1432.7 examples/sec; 0.089 sec/batch)
2017-05-02 19:46:17.758723: step 29000, loss = 0.31 (1465.0 examples/sec; 0.087 sec/batch)
2017-05-02 19:46:37.360546: step 29100, loss = 0.33 (1515.3 examples/sec; 0.084 sec/batch)
2017-05-02 19:46:56.190999: step 29200, loss = 0.35 (1466.8 examples/sec; 0.087 sec/batch)
2017-05-02 19:47:14.485961: step 29300, loss = 0.51 (1405.1 examples/sec; 0.091 sec/batch)
2017-05-02 19:47:33.094404: step 29400, loss = 0.36 (1450.3 examples/sec; 0.088 sec/batch)
2017-05-02 19:47:52.732181: step 29500, loss = 0.45 (1448.6 examples/sec; 0.088 sec/batch)
2017-05-02 19:48:11.134463: step 29600, loss = 0.36 (1425.5 examples/sec; 0.090 sec/batch)
2017-05-02 19:48:29.940185: step 29700, loss = 0.35 (1417.4 examples/sec; 0.090 sec/batch)
2017-05-02 19:48:49.019701: step 29800, loss = 0.32 (1462.7 examples/sec; 0.088 sec/batch)
2017-05-02 19:49:07.870342: step 29900, loss = 0.35 (1487.0 examples/sec; 0.086 sec/batch)
2017-05-02 19:49:29.605143: precision @ 1 = 0.828
2017-05-02 19:49:30.002464: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-04 15:27:55.319481: Running on server...
The experiment details:
max_steps = 30000 log_frequency = 100 num_gpus = 2
########################################################################################
########################################################################################
########################################################################################
2017-05-04 15:29:28.270118: Running on server...
The experiment details:
max_steps = 30000 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
########################################################################################
########################################################################################
########################################################################################
2017-05-04 15:32:21.192801: Running on server...
The experiment details:
max_steps = 30000 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-04 15:32:37.506947: step 0, loss = 6.39 (23.5 examples/sec; 5.438 sec/batch)
2017-05-04 15:33:46.752164: step 100, loss = 5.47 (379.4 examples/sec; 0.337 sec/batch)
2017-05-04 15:34:51.384072: step 200, loss = 5.11 (407.0 examples/sec; 0.314 sec/batch)
2017-05-04 15:35:56.324246: step 300, loss = 4.48 (365.6 examples/sec; 0.350 sec/batch)
2017-05-04 15:37:02.308163: step 400, loss = 4.42 (355.6 examples/sec; 0.360 sec/batch)
2017-05-04 15:38:06.766475: step 500, loss = 3.76 (358.8 examples/sec; 0.357 sec/batch)
########################################################################################
########################################################################################
########################################################################################
2017-05-04 15:54:41.232254: Running on server...
The experiment details:
max_steps = 30000 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-04 15:54:56.279781: step 0, loss = 6.37 (34.2 examples/sec; 3.738 sec/batch)
########################################################################################
########################################################################################
########################################################################################
2017-05-04 18:04:58.192638: Running on server...
The experiment details:
max_steps = 300 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-04 18:05:16.219195: step 0, loss = 6.38 (30.7 examples/sec; 4.172 sec/batch)
2017-05-04 18:06:07.799099: step 100, loss = 5.65 (560.7 examples/sec; 0.228 sec/batch)
2017-05-04 18:06:52.800191: step 200, loss = 5.15 (584.1 examples/sec; 0.219 sec/batch)
2017-05-04 18:07:45.775962: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-04 18:21:50.823812: Running on server...
The experiment details:
max_steps = 300 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-04 18:22:06.352270: step 0, loss = 6.38 (32.9 examples/sec; 3.892 sec/batch)
2017-05-04 18:23:32.955750: step 100, loss = 6.27 (582.6 examples/sec; 0.220 sec/batch)
2017-05-04 18:24:28.498689: step 200, loss = 4.96 (330.7 examples/sec; 0.387 sec/batch)
2017-05-04 18:25:38.203079: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-05 05:11:09.077603: Running on server...
The experiment details:
max_steps = 300 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
########################################################################################
########################################################################################
########################################################################################
2017-05-05 05:15:23.137523: Running on server...
The experiment details:
max_steps = 300 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
########################################################################################
########################################################################################
########################################################################################
2017-05-05 05:17:23.667890: Running on server...
The experiment details:
max_steps = 300 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
########################################################################################
########################################################################################
########################################################################################
2017-05-05 05:20:43.348450: Running on server...
The experiment details:
max_steps = 300 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
########################################################################################
########################################################################################
########################################################################################
2017-05-05 05:21:22.899434: Running on server...
The experiment details:
max_steps = 300 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
########################################################################################
########################################################################################
########################################################################################
2017-05-05 05:22:41.916198: Running on server...
The experiment details:
max_steps = 300 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
########################################################################################
########################################################################################
########################################################################################
2017-05-05 05:23:42.231484: Running on server...
The experiment details:
max_steps = 300 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
########################################################################################
########################################################################################
########################################################################################
2017-05-05 05:28:00.132401: Running on server...
The experiment details:
max_steps = 300 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-05 05:28:15.008402: step 0, loss = 6.39 (38.0 examples/sec; 3.366 sec/batch)
2017-05-05 05:28:56.969274: step 100, loss = 5.49 (635.4 examples/sec; 0.201 sec/batch)
2017-05-05 05:29:37.349513: step 200, loss = 5.10 (643.8 examples/sec; 0.199 sec/batch)
2017-05-05 05:30:19.921523: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-05 05:33:58.522844: Running on server...
The experiment details:
max_steps = 300 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-05 05:34:23.025541: step 0, loss = 6.38 (14.1 examples/sec; 9.104 sec/batch)
2017-05-05 05:34:41.851379: step 100, loss = 5.44 (1533.1 examples/sec; 0.083 sec/batch)
2017-05-05 05:34:58.932237: step 200, loss = 5.06 (1515.3 examples/sec; 0.084 sec/batch)
2017-05-05 05:35:18.475877: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-05 06:48:02.677543: Running on server...
The experiment details:
max_steps = 300 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
########################################################################################
########################################################################################
########################################################################################
2017-05-05 08:00:08.044298: Running on server...
The experiment details:
max_steps = 300 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
########################################################################################
########################################################################################
########################################################################################
2017-05-05 08:01:50.482709: Running on server...
The experiment details:
max_steps = 300 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-05 08:02:10.850287: step 0, loss = 6.38 (16.4 examples/sec; 7.794 sec/batch)
2017-05-05 08:02:29.547910: step 100, loss = 5.62 (1540.5 examples/sec; 0.083 sec/batch)
2017-05-05 08:02:46.396116: step 200, loss = 5.04 (1508.4 examples/sec; 0.085 sec/batch)
########################################################################################
########################################################################################
########################################################################################
2017-05-05 08:05:01.365308: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-05 08:05:24.228249: step 0, loss = 6.39 (15.3 examples/sec; 8.368 sec/batch)
2017-05-05 08:05:30.971510: precision @ 1 = 0.229
2017-05-05 08:05:31.362618: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-05 08:25:08.174037: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
########################################################################################
########################################################################################
########################################################################################
2017-05-05 08:27:51.445085: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-05 08:28:12.265474: step 0, loss = 6.37 (16.3 examples/sec; 7.858 sec/batch)
2017-05-05 08:28:18.602107: precision @ 1 = 0.129
2017-05-05 08:28:18.829956: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-05 08:30:55.925526: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-05 08:31:16.683638: step 0, loss = 6.38 (16.1 examples/sec; 7.950 sec/batch)
2017-05-05 08:31:22.778469: precision @ 1 = 0.159
2017-05-05 08:31:23.005261: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-05 12:28:32.922085: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-05 12:28:53.584858: step 0, loss = 6.39 (16.6 examples/sec; 7.689 sec/batch)
2017-05-05 12:29:00.272211: precision @ 1 = 0.158
2017-05-05 12:29:01.288800: DONE
########################################################################################
########################################################################################
########################################################################################
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
########################################################################################
########################################################################################
########################################################################################
2017-05-05 14:39:05.871238: Running on server...
The experiment details:
max_steps = 30000 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-05 14:39:28.706775: step 0, loss = 6.38 (15.3 examples/sec; 8.379 sec/batch)
2017-05-05 14:39:44.783964: step 100, loss = 5.55 (1765.6 examples/sec; 0.072 sec/batch)
2017-05-05 14:39:59.572611: step 200, loss = 5.06 (1720.8 examples/sec; 0.074 sec/batch)
2017-05-05 14:40:14.389180: step 300, loss = 4.73 (1783.0 examples/sec; 0.072 sec/batch)
2017-05-05 14:40:29.263731: step 400, loss = 4.45 (1713.2 examples/sec; 0.075 sec/batch)
2017-05-05 14:40:44.101340: step 500, loss = 4.10 (1772.8 examples/sec; 0.072 sec/batch)
2017-05-05 14:40:58.955501: step 600, loss = 3.69 (1707.1 examples/sec; 0.075 sec/batch)
########################################################################################
########################################################################################
########################################################################################
2017-05-05 14:41:28.558576: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-05 14:41:49.363749: step 0, loss = 6.38 (16.6 examples/sec; 7.726 sec/batch)
2017-05-05 14:41:55.844095: precision @ 1 = 0.151
2017-05-05 14:41:56.084779: DONE
########################################################################################
########################################################################################
########################################################################################
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
########################################################################################
########################################################################################
########################################################################################
2017-05-06 03:00:40.892519: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 03:01:05.673525: step 0, loss = 6.38 (15.4 examples/sec; 8.313 sec/batch)
2017-05-06 03:01:11.699825: precision @ 1 = 0.231
2017-05-06 03:01:11.975903: DONE
########################################################################################
########################################################################################
########################################################################################
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
########################################################################################
########################################################################################
########################################################################################
2017-05-06 03:15:44.585966: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 03:16:09.283549: step 0, loss = 6.38 (15.2 examples/sec; 8.415 sec/batch)
2017-05-06 03:16:15.128308: precision @ 1 = 0.183
2017-05-06 03:16:15.363658: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-06 03:24:11.158403: Running on server...
The experiment details:
max_steps = 30000 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 03:24:35.522398: step 0, loss = 6.37 (15.4 examples/sec; 8.327 sec/batch)
2017-05-06 03:24:55.447786: step 100, loss = 5.70 (1517.9 examples/sec; 0.084 sec/batch)
2017-05-06 03:25:12.552273: step 200, loss = 4.96 (1531.8 examples/sec; 0.084 sec/batch)
2017-05-06 03:25:29.674169: step 300, loss = 4.91 (1525.5 examples/sec; 0.084 sec/batch)
2017-05-06 03:25:46.754378: step 400, loss = 4.20 (1371.9 examples/sec; 0.093 sec/batch)
2017-05-06 03:26:03.827820: step 500, loss = 3.88 (1521.7 examples/sec; 0.084 sec/batch)
2017-05-06 03:26:20.893637: step 600, loss = 3.65 (1514.5 examples/sec; 0.085 sec/batch)
2017-05-06 03:26:37.979644: step 700, loss = 3.30 (1541.6 examples/sec; 0.083 sec/batch)
2017-05-06 03:26:55.041914: step 800, loss = 3.14 (1496.1 examples/sec; 0.086 sec/batch)
2017-05-06 03:27:12.157129: step 900, loss = 2.87 (1522.2 examples/sec; 0.084 sec/batch)
2017-05-06 03:27:29.219873: step 1000, loss = 2.60 (1546.9 examples/sec; 0.083 sec/batch)
2017-05-06 03:27:47.615227: step 1100, loss = 2.48 (1534.9 examples/sec; 0.083 sec/batch)
2017-05-06 03:28:04.735429: step 1200, loss = 2.44 (1392.9 examples/sec; 0.092 sec/batch)
2017-05-06 03:28:21.810860: step 1300, loss = 2.25 (1489.5 examples/sec; 0.086 sec/batch)
2017-05-06 03:28:38.874157: step 1400, loss = 2.35 (1536.4 examples/sec; 0.083 sec/batch)
2017-05-06 03:28:55.952617: step 1500, loss = 2.01 (1519.0 examples/sec; 0.084 sec/batch)
2017-05-06 03:29:13.048300: step 1600, loss = 1.83 (1540.9 examples/sec; 0.083 sec/batch)
2017-05-06 03:29:30.119148: step 1700, loss = 1.95 (1502.7 examples/sec; 0.085 sec/batch)
2017-05-06 03:29:47.196252: step 1800, loss = 1.67 (1509.4 examples/sec; 0.085 sec/batch)
2017-05-06 03:30:04.279597: step 1900, loss = 1.56 (1485.2 examples/sec; 0.086 sec/batch)
2017-05-06 03:30:21.387832: step 2000, loss = 1.52 (1539.9 examples/sec; 0.083 sec/batch)
2017-05-06 03:30:39.826470: step 2100, loss = 1.99 (1530.7 examples/sec; 0.084 sec/batch)
2017-05-06 03:30:57.001952: step 2200, loss = 1.26 (1501.4 examples/sec; 0.085 sec/batch)
2017-05-06 03:31:14.099177: step 2300, loss = 1.17 (1536.5 examples/sec; 0.083 sec/batch)
2017-05-06 03:31:31.197149: step 2400, loss = 1.41 (1498.6 examples/sec; 0.085 sec/batch)
2017-05-06 03:31:48.315813: step 2500, loss = 1.24 (1481.2 examples/sec; 0.086 sec/batch)
2017-05-06 03:32:05.503048: step 2600, loss = 1.05 (1496.9 examples/sec; 0.086 sec/batch)
2017-05-06 03:32:22.586565: step 2700, loss = 1.04 (1491.2 examples/sec; 0.086 sec/batch)
2017-05-06 03:32:39.663913: step 2800, loss = 1.01 (1545.0 examples/sec; 0.083 sec/batch)
2017-05-06 03:32:56.735148: step 2900, loss = 0.94 (1508.0 examples/sec; 0.085 sec/batch)
2017-05-06 03:33:13.817258: step 3000, loss = 1.32 (1530.7 examples/sec; 0.084 sec/batch)
2017-05-06 03:33:32.242256: step 3100, loss = 0.88 (1488.8 examples/sec; 0.086 sec/batch)
2017-05-06 03:33:49.325780: step 3200, loss = 0.81 (1525.0 examples/sec; 0.084 sec/batch)
2017-05-06 03:34:06.389275: step 3300, loss = 1.00 (1534.0 examples/sec; 0.083 sec/batch)
2017-05-06 03:34:23.464614: step 3400, loss = 0.77 (1520.1 examples/sec; 0.084 sec/batch)
2017-05-06 03:34:40.539346: step 3500, loss = 0.75 (1511.1 examples/sec; 0.085 sec/batch)
2017-05-06 03:34:57.620175: step 3600, loss = 0.64 (1547.7 examples/sec; 0.083 sec/batch)
2017-05-06 03:35:14.698984: step 3700, loss = 0.76 (1507.8 examples/sec; 0.085 sec/batch)
2017-05-06 03:35:31.804360: step 3800, loss = 0.69 (1516.2 examples/sec; 0.084 sec/batch)
2017-05-06 03:35:48.931194: step 3900, loss = 0.80 (1543.3 examples/sec; 0.083 sec/batch)
2017-05-06 03:36:06.100287: step 4000, loss = 0.68 (1528.1 examples/sec; 0.084 sec/batch)
2017-05-06 03:36:24.553386: step 4100, loss = 0.68 (1522.2 examples/sec; 0.084 sec/batch)
2017-05-06 03:36:41.735390: step 4200, loss = 0.66 (1537.0 examples/sec; 0.083 sec/batch)
2017-05-06 03:36:58.840075: step 4300, loss = 0.58 (1534.1 examples/sec; 0.083 sec/batch)
2017-05-06 03:37:16.043830: step 4400, loss = 0.66 (1525.2 examples/sec; 0.084 sec/batch)
2017-05-06 03:37:33.144203: step 4500, loss = 0.63 (1534.4 examples/sec; 0.083 sec/batch)
2017-05-06 03:37:50.273974: step 4600, loss = 0.61 (1531.6 examples/sec; 0.084 sec/batch)
2017-05-06 03:38:07.457821: step 4700, loss = 0.59 (1531.8 examples/sec; 0.084 sec/batch)
2017-05-06 03:38:24.568549: step 4800, loss = 0.56 (1504.7 examples/sec; 0.085 sec/batch)
2017-05-06 03:38:41.766350: step 4900, loss = 0.52 (1412.7 examples/sec; 0.091 sec/batch)
2017-05-06 03:38:58.885456: step 5000, loss = 0.60 (1503.0 examples/sec; 0.085 sec/batch)
2017-05-06 03:39:17.314352: step 5100, loss = 0.47 (1571.5 examples/sec; 0.081 sec/batch)
2017-05-06 03:39:34.530290: step 5200, loss = 0.49 (1528.9 examples/sec; 0.084 sec/batch)
2017-05-06 03:39:51.703140: step 5300, loss = 0.46 (1535.7 examples/sec; 0.083 sec/batch)
2017-05-06 03:40:08.864551: step 5400, loss = 0.44 (1521.0 examples/sec; 0.084 sec/batch)
2017-05-06 03:40:26.106556: step 5500, loss = 0.54 (1453.7 examples/sec; 0.088 sec/batch)
2017-05-06 03:40:43.341919: step 5600, loss = 0.54 (1519.6 examples/sec; 0.084 sec/batch)
2017-05-06 03:41:00.592320: step 5700, loss = 0.45 (1478.7 examples/sec; 0.087 sec/batch)
2017-05-06 03:41:17.825703: step 5800, loss = 0.44 (1517.3 examples/sec; 0.084 sec/batch)
2017-05-06 03:41:35.021986: step 5900, loss = 0.64 (1515.9 examples/sec; 0.084 sec/batch)
2017-05-06 03:41:52.172596: step 6000, loss = 0.48 (1516.9 examples/sec; 0.084 sec/batch)
2017-05-06 03:42:10.579390: step 6100, loss = 0.47 (1531.4 examples/sec; 0.084 sec/batch)
2017-05-06 03:42:27.815143: step 6200, loss = 0.43 (1530.5 examples/sec; 0.084 sec/batch)
2017-05-06 03:42:44.969624: step 6300, loss = 0.56 (1479.7 examples/sec; 0.087 sec/batch)
########################################################################################
########################################################################################
########################################################################################
2017-05-06 03:45:02.496969: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 03:45:23.500964: step 0, loss = 6.38 (16.6 examples/sec; 7.717 sec/batch)
2017-05-06 03:45:30.045663: precision @ 1 = 0.189
2017-05-06 03:45:30.278521: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-06 04:38:00.763874: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Number of hidden parameters of conv1: 2432
Number of hidden parameters of conv2: 51264
Number of hidden parameters of local3: 1573248
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 1702794
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Number of hidden parameters of conv1: 2432
Number of hidden parameters of conv2: 51264
Number of hidden parameters of local3: 1573248
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 1702794
2017-05-06 04:38:24.430194: step 0, loss = 6.38 (15.3 examples/sec; 8.365 sec/batch)
Number of hidden parameters of conv1: 2432
Number of hidden parameters of conv2: 51264
Number of hidden parameters of local3: 1573248
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 1702794
Evaluation results:
2017-05-06 04:38:30.544127: Total Predictions = 1024
2017-05-06 04:38:30.553391: Correct Predictions = 126
2017-05-06 04:38:30.563941: Wrong Predictions = 898
2017-05-06 04:38:30.575084: precision @ 1 = 0.123
2017-05-06 04:38:30.782233: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-06 04:39:47.880547: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Number of hidden parameters of conv1: 896
Number of hidden parameters of conv2: 51264
Number of hidden parameters of local3: 1573248
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 1701258
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Number of hidden parameters of conv1: 896
Number of hidden parameters of conv2: 51264
Number of hidden parameters of local3: 1573248
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 1701258
2017-05-06 04:40:08.677058: step 0, loss = 6.38 (16.2 examples/sec; 7.882 sec/batch)
Number of hidden parameters of conv1: 896
Number of hidden parameters of conv2: 51264
Number of hidden parameters of local3: 1573248
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 1701258
Evaluation results:
2017-05-06 04:40:15.077228: Total Predictions = 1024
2017-05-06 04:40:15.085067: Correct Predictions = 184
2017-05-06 04:40:15.093210: Wrong Predictions = 840
2017-05-06 04:40:15.101206: precision @ 1 = 0.180
2017-05-06 04:40:15.326650: DONE
########################################################################################
########################################################################################
########################################################################################
Number of hidden parameters of conv1: 896
Number of hidden parameters of conv2: 51264
Number of hidden parameters of local3: 147840
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 275850
Number of hidden parameters of conv1: 896
Number of hidden parameters of conv2: 51264
Number of hidden parameters of local3: 147840
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 275850
########################################################################################
########################################################################################
########################################################################################
2017-05-06 04:53:28.941588: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 04:53:49.460756: step 0, loss = 6.38 (16.6 examples/sec; 7.719 sec/batch)
Evaluation results:
2017-05-06 04:53:55.551054: Total Predictions = 1024
2017-05-06 04:53:55.557673: Correct Predictions = 125
2017-05-06 04:53:55.566454: Wrong Predictions = 899
2017-05-06 04:53:55.573311: precision @ 1 = 0.122
2017-05-06 04:53:55.774691: DONE
########################################################################################
########################################################################################
########################################################################################
Number of hidden parameters of conv1: 1792
Number of hidden parameters of conv2: 102464
Number of hidden parameters of local3: 147840
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 327946
Number of hidden parameters of conv1: 1792
Number of hidden parameters of conv2: 204928
Number of hidden parameters of local3: 147840
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 430410
########################################################################################
########################################################################################
########################################################################################
2017-05-06 04:56:00.791206: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 04:56:22.772179: step 0, loss = 10.27 (15.1 examples/sec; 8.457 sec/batch)
Evaluation results:
2017-05-06 04:56:29.652982: Total Predictions = 1024
2017-05-06 04:56:29.666123: Correct Predictions = 227
2017-05-06 04:56:29.679720: Wrong Predictions = 797
2017-05-06 04:56:29.688107: precision @ 1 = 0.222
2017-05-06 04:56:30.050833: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [3, 3, 3, 64]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 64, 128]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 384  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 1792
Number of hidden parameters of conv2: 204928
Number of hidden parameters of local3: 147840
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 430410
########################################################################################
########################################################################################
########################################################################################
2017-05-06 05:10:33.745760: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 05:10:54.262434: step 0, loss = 10.27 (17.0 examples/sec; 7.547 sec/batch)
Evaluation results:
2017-05-06 05:11:01.089945: Total Predictions = 1024
2017-05-06 05:11:01.095895: Correct Predictions = 163
2017-05-06 05:11:01.102314: Wrong Predictions = 861
2017-05-06 05:11:01.108698: precision @ 1 = 0.159
2017-05-06 05:11:01.472038: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [3, 3, 3, 64]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 64, 128]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 384  | local3OutputDepth: 768
local4InputDepth: 768  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 1792
Number of hidden parameters of conv2: 204928
Number of hidden parameters of local3: 295680
Number of hidden parameters of local4: 147648
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 651978
Network summary:
conv1Shape: [3, 3, 3, 64]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 64, 128]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 384  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 1792
Number of hidden parameters of conv2: 204928
Number of hidden parameters of local3: 147840
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 430410
########################################################################################
########################################################################################
########################################################################################
2017-05-06 05:13:33.481494: Running on server...
The experiment details:
max_steps = 1000 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 05:13:54.253448: step 0, loss = 10.28 (16.7 examples/sec; 7.660 sec/batch)
2017-05-06 05:14:15.509998: step 100, loss = 9.07 (1303.8 examples/sec; 0.098 sec/batch)
2017-05-06 05:14:39.413816: step 200, loss = 8.67 (1331.5 examples/sec; 0.096 sec/batch)
2017-05-06 05:14:59.257208: step 300, loss = 7.77 (1327.7 examples/sec; 0.096 sec/batch)
2017-05-06 05:15:19.047048: step 400, loss = 7.25 (1333.1 examples/sec; 0.096 sec/batch)
2017-05-06 05:15:38.835037: step 500, loss = 6.82 (1301.0 examples/sec; 0.098 sec/batch)
2017-05-06 05:16:03.012721: step 600, loss = 6.21 (1338.6 examples/sec; 0.096 sec/batch)
2017-05-06 05:16:22.769840: step 700, loss = 5.56 (1311.8 examples/sec; 0.098 sec/batch)
2017-05-06 05:16:43.896609: step 800, loss = 5.40 (1259.0 examples/sec; 0.102 sec/batch)
2017-05-06 05:17:04.015154: step 900, loss = 5.00 (1308.7 examples/sec; 0.098 sec/batch)
Evaluation results:
2017-05-06 05:17:28.341723: Total Predictions = 1024
2017-05-06 05:17:28.350941: Correct Predictions = 711
2017-05-06 05:17:28.358815: Wrong Predictions = 313
2017-05-06 05:17:28.368494: precision @ 1 = 0.694
2017-05-06 05:17:28.725636: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-06 05:20:37.629658: Running on server...
The experiment details:
max_steps = 4000 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 05:21:03.312783: step 0, loss = 10.28 (12.5 examples/sec; 10.261 sec/batch)
2017-05-06 05:21:24.767219: step 100, loss = 9.07 (1279.0 examples/sec; 0.100 sec/batch)
2017-05-06 05:21:44.548738: step 200, loss = 8.50 (1316.2 examples/sec; 0.097 sec/batch)
2017-05-06 05:22:05.408034: step 300, loss = 7.86 (1318.2 examples/sec; 0.097 sec/batch)
2017-05-06 05:22:25.192726: step 400, loss = 7.34 (1322.5 examples/sec; 0.097 sec/batch)
2017-05-06 05:22:44.894839: step 500, loss = 6.44 (1322.3 examples/sec; 0.097 sec/batch)
2017-05-06 05:23:04.652071: step 600, loss = 6.19 (1326.4 examples/sec; 0.097 sec/batch)
2017-05-06 05:23:24.389363: step 700, loss = 5.56 (1222.5 examples/sec; 0.105 sec/batch)
2017-05-06 05:23:47.087676: step 800, loss = 5.45 (1327.4 examples/sec; 0.096 sec/batch)
2017-05-06 05:24:06.829623: step 900, loss = 4.92 (1262.2 examples/sec; 0.101 sec/batch)
2017-05-06 05:24:26.558771: step 1000, loss = 4.58 (1321.9 examples/sec; 0.097 sec/batch)
2017-05-06 05:24:47.511441: step 1100, loss = 4.38 (1319.5 examples/sec; 0.097 sec/batch)
2017-05-06 05:25:07.255013: step 1200, loss = 4.22 (1296.0 examples/sec; 0.099 sec/batch)
2017-05-06 05:25:31.948879: step 1300, loss = 3.73 (1319.2 examples/sec; 0.097 sec/batch)
2017-05-06 05:25:53.349751: step 1400, loss = 3.36 (1361.9 examples/sec; 0.094 sec/batch)
2017-05-06 05:26:15.883314: step 1500, loss = 3.14 (1328.3 examples/sec; 0.096 sec/batch)
2017-05-06 05:26:39.914647: step 1600, loss = 3.08 (1337.8 examples/sec; 0.096 sec/batch)
2017-05-06 05:26:59.689280: step 1700, loss = 2.92 (1325.3 examples/sec; 0.097 sec/batch)
2017-05-06 05:27:19.405027: step 1800, loss = 2.77 (1307.5 examples/sec; 0.098 sec/batch)
2017-05-06 05:27:39.151103: step 1900, loss = 2.51 (1328.5 examples/sec; 0.096 sec/batch)
2017-05-06 05:27:58.900511: step 2000, loss = 2.29 (1330.4 examples/sec; 0.096 sec/batch)
2017-05-06 05:28:20.443044: step 2100, loss = 2.16 (1298.2 examples/sec; 0.099 sec/batch)
2017-05-06 05:28:42.609356: step 2200, loss = 2.06 (1328.1 examples/sec; 0.096 sec/batch)
2017-05-06 05:29:03.348807: step 2300, loss = 1.87 (223.1 examples/sec; 0.574 sec/batch)
2017-05-06 05:29:26.126846: step 2400, loss = 1.93 (1295.8 examples/sec; 0.099 sec/batch)
2017-05-06 05:29:45.911712: step 2500, loss = 1.80 (1269.1 examples/sec; 0.101 sec/batch)
2017-05-06 05:30:15.865749: step 2600, loss = 1.57 (1319.9 examples/sec; 0.097 sec/batch)
2017-05-06 05:30:50.866291: step 2700, loss = 1.53 (1132.0 examples/sec; 0.113 sec/batch)
2017-05-06 05:31:13.085583: step 2800, loss = 1.37 (1309.8 examples/sec; 0.098 sec/batch)
2017-05-06 05:31:32.839624: step 2900, loss = 1.27 (1326.6 examples/sec; 0.096 sec/batch)
2017-05-06 05:31:52.567144: step 3000, loss = 1.35 (1333.0 examples/sec; 0.096 sec/batch)
2017-05-06 05:32:13.821877: step 3100, loss = 1.32 (1333.4 examples/sec; 0.096 sec/batch)
2017-05-06 05:32:33.595946: step 3200, loss = 1.32 (1318.2 examples/sec; 0.097 sec/batch)
2017-05-06 05:32:53.391983: step 3300, loss = 1.15 (1305.3 examples/sec; 0.098 sec/batch)
2017-05-06 05:33:13.217718: step 3400, loss = 1.08 (1287.7 examples/sec; 0.099 sec/batch)
2017-05-06 05:33:33.012048: step 3500, loss = 1.08 (1311.8 examples/sec; 0.098 sec/batch)
2017-05-06 05:33:52.805668: step 3600, loss = 1.00 (1308.1 examples/sec; 0.098 sec/batch)
2017-05-06 05:34:12.589351: step 3700, loss = 0.92 (1305.5 examples/sec; 0.098 sec/batch)
2017-05-06 05:34:32.413867: step 3800, loss = 0.84 (1395.1 examples/sec; 0.092 sec/batch)
2017-05-06 05:34:52.189158: step 3900, loss = 1.05 (1321.3 examples/sec; 0.097 sec/batch)
Evaluation results:
2017-05-06 05:35:15.308766: Total Predictions = 1024
2017-05-06 05:35:15.315709: Correct Predictions = 821
2017-05-06 05:35:15.322752: Wrong Predictions = 203
2017-05-06 05:35:15.328866: precision @ 1 = 0.802
2017-05-06 05:35:15.800302: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [5, 5, 3, 64]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 64, 128]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 384  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 4864
Number of hidden parameters of conv2: 73856
Number of hidden parameters of local3: 147840
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 302410
Network summary:
conv1Shape: [5, 5, 3, 64]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 64, 128]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 384  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 4864
Number of hidden parameters of conv2: 73856
Number of hidden parameters of local3: 147840
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 302410
########################################################################################
########################################################################################
########################################################################################
2017-05-06 05:43:59.492485: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
########################################################################################
########################################################################################
########################################################################################
2017-05-06 05:57:38.198102: Running on server...
The experiment details:
max_steps = 30000 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Network summary:
conv1Shape: [5, 5, 3, 64]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 64, 128]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 384  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 4864
Number of hidden parameters of conv2: 73856
Number of hidden parameters of local3: 147840
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 302410
########################################################################################
########################################################################################
########################################################################################
2017-05-06 05:59:20.161644: Running on server...
The experiment details:
max_steps = 30000 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 8192  | local3OutputDepth: 384
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 8192  | local3OutputDepth: 384
Network summary:
conv1Shape: [5, 5, 3, 64]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 64, 128]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 8192  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 4864
Number of hidden parameters of conv2: 73856
Number of hidden parameters of local3: 3146112
Number of hidden parameters of local4: 73920
Network summary:
conv1Shape: [5, 5, 3, 64]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 64, 128]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 8192  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 4864
Number of hidden parameters of conv2: 73856
Number of hidden parameters of local3: 3146112
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 3300682
########################################################################################
########################################################################################
########################################################################################
2017-05-06 06:01:43.396430: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 8192  | local3OutputDepth: 384
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 8192  | local3OutputDepth: 384
Network summary:
conv1Shape: [5, 5, 3, 64]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 64, 128]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 8192  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 4864
Number of hidden parameters of conv2: 73856
Number of hidden parameters of local3: 3146112
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 3300682
########################################################################################
########################################################################################
########################################################################################
2017-05-06 06:04:06.247239: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 0
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 8192  | local3OutputDepth: 384
2017-05-06 06:04:40.900405: step 0, loss = 10.27 (729.8 examples/sec; 0.175 sec/batch)
local3InputDepth: 8192  | local3OutputDepth: 384
Network summary:
conv1Shape: [5, 5, 3, 64]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 64, 128]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 8192  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 4864
Number of hidden parameters of conv2: 73856
Number of hidden parameters of local3: 3146112
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 3300682
########################################################################################
########################################################################################
########################################################################################
2017-05-06 06:08:59.986651: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 8192  | local3OutputDepth: 384
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 8192  | local3OutputDepth: 384
2017-05-06 06:09:22.829189: step 0, loss = 10.28 (15.9 examples/sec; 8.047 sec/batch)
local3InputDepth: 8192  | local3OutputDepth: 384
Evaluation results:
2017-05-06 06:09:29.625926: Total Predictions = 1024
2017-05-06 06:09:29.639124: Correct Predictions = 188
2017-05-06 06:09:29.645442: Wrong Predictions = 836
2017-05-06 06:09:29.651969: precision @ 1 = 0.184
2017-05-06 06:09:30.026550: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-06 06:12:20.606502: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 8192  | local3OutputDepth: 384
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 8192  | local3OutputDepth: 384
2017-05-06 06:12:41.377016: step 0, loss = 10.27 (16.1 examples/sec; 7.966 sec/batch)
local3InputDepth: 8192  | local3OutputDepth: 384
Evaluation results:
2017-05-06 06:12:48.135161: Total Predictions = 1024
2017-05-06 06:12:48.152295: Correct Predictions = 193
2017-05-06 06:12:48.159719: Wrong Predictions = 831
2017-05-06 06:12:48.166384: precision @ 1 = 0.188
2017-05-06 06:12:48.532660: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [5, 5, 3, 64]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 64, 128]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 8192  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 4864
Number of hidden parameters of conv2: 73856
Number of hidden parameters of local3: 3146112
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 3300682
Network summary:
conv1Shape: [5, 5, 3, 64]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 64, 128]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 8192  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 4864
Number of hidden parameters of conv2: 73856
Number of hidden parameters of local3: 3146112
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 3300682
########################################################################################
########################################################################################
########################################################################################
2017-05-06 06:17:43.820815: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 8192  | local3OutputDepth: 384
Network summary:
conv1Shape: [5, 5, 3, 64]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 64, 128]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 8192  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 4864
Number of hidden parameters of conv2: 73856
Number of hidden parameters of local3: 3146112
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 3300682
########################################################################################
########################################################################################
########################################################################################
2017-05-06 06:18:13.453243: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 8192  | local3OutputDepth: 384
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 8192  | local3OutputDepth: 384
2017-05-06 06:18:35.346724: step 0, loss = 10.26 (15.1 examples/sec; 8.492 sec/batch)
local3InputDepth: 8192  | local3OutputDepth: 384
Evaluation results:
2017-05-06 06:18:42.384536: Total Predictions = 1024
2017-05-06 06:18:42.392218: Correct Predictions = 146
2017-05-06 06:18:42.398863: Wrong Predictions = 878
2017-05-06 06:18:42.406506: precision @ 1 = 0.143
2017-05-06 06:18:42.787808: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [5, 5, 3, 64]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 64, 128]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 8192  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 4864
Number of hidden parameters of conv2: 73856
Number of hidden parameters of local3: 3146112
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 3300682
########################################################################################
########################################################################################
########################################################################################
2017-05-06 06:18:53.907879: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 8192  | local3OutputDepth: 384
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 8192  | local3OutputDepth: 384
Network summary:
conv1Shape: [5, 5, 3, 64]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 64, 128]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 8192  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 4864
Number of hidden parameters of conv2: 73856
Number of hidden parameters of local3: 3146112
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 3300682
########################################################################################
########################################################################################
########################################################################################
2017-05-06 06:19:44.064801: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 8192  | local3OutputDepth: 384
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 8192  | local3OutputDepth: 384
2017-05-06 06:20:08.459960: step 0, loss = 10.28 (15.9 examples/sec; 8.050 sec/batch)
local3InputDepth: 8192  | local3OutputDepth: 384
Evaluation results:
2017-05-06 06:20:14.980212: Total Predictions = 1024
2017-05-06 06:20:14.986877: Correct Predictions = 160
2017-05-06 06:20:14.993811: Wrong Predictions = 864
2017-05-06 06:20:15.000366: precision @ 1 = 0.156
2017-05-06 06:20:15.370333: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [5, 5, 3, 64]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 64, 128]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 8192  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 4864
Number of hidden parameters of conv2: 73856
Number of hidden parameters of local3: 3146112
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 3300682
########################################################################################
########################################################################################
########################################################################################
2017-05-06 06:20:50.604240: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 8192  | local3OutputDepth: 384
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 8192  | local3OutputDepth: 384
Network summary:
conv1Shape: [5, 5, 3, 32]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 32, 64]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 8192  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 2432
Number of hidden parameters of conv2: 18496
Number of hidden parameters of local3: 3146112
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 3242890
########################################################################################
########################################################################################
########################################################################################
2017-05-06 06:22:32.127957: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 4096  | local3OutputDepth: 384
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 4096  | local3OutputDepth: 384
2017-05-06 06:22:52.658848: step 0, loss = 6.38 (16.7 examples/sec; 7.680 sec/batch)
local3InputDepth: 4096  | local3OutputDepth: 384
Evaluation results:
2017-05-06 06:22:58.542087: Total Predictions = 1024
2017-05-06 06:22:58.550506: Correct Predictions = 164
2017-05-06 06:22:58.559030: Wrong Predictions = 860
2017-05-06 06:22:58.567960: precision @ 1 = 0.160
2017-05-06 06:22:58.770578: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [5, 5, 3, 32]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 32, 64]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 8192  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 2432
Number of hidden parameters of conv2: 18496
Number of hidden parameters of local3: 3146112
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 3242890
########################################################################################
########################################################################################
########################################################################################
2017-05-06 06:31:07.701223: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 4096  | local3OutputDepth: 384
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 4096  | local3OutputDepth: 384
2017-05-06 06:31:30.915765: step 0, loss = 6.37 (15.4 examples/sec; 8.337 sec/batch)
local3InputDepth: 4096  | local3OutputDepth: 384
Evaluation results:
2017-05-06 06:31:36.641091: Total Predictions = 1024
2017-05-06 06:31:36.647901: Correct Predictions = 92
2017-05-06 06:31:36.654678: Wrong Predictions = 932
2017-05-06 06:31:36.662769: precision @ 1 = 0.090
2017-05-06 06:31:36.869550: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [5, 5, 3, 32]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 32, 64]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 8192  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 2432
Number of hidden parameters of conv2: 18496
Number of hidden parameters of local3: 3146112
Number of hidden parameters of local4: 73920
Number of hidden parameters of softmax: 1930
Total number of hidden parameters: 3242890
########################################################################################
########################################################################################
########################################################################################
2017-05-06 06:36:36.065638: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 4096  | local3OutputDepth: 384
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 4096  | local3OutputDepth: 384
2017-05-06 06:36:58.879079: step 0, loss = 6.38 (15.1 examples/sec; 8.479 sec/batch)
local3InputDepth: 4096  | local3OutputDepth: 384
Evaluation results:
2017-05-06 06:37:04.614365: Total Predictions = 1024
2017-05-06 06:37:04.621001: Correct Predictions = 128
2017-05-06 06:37:04.627236: Wrong Predictions = 896
2017-05-06 06:37:04.634165: precision @ 1 = 0.125
2017-05-06 06:37:04.840675: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [5, 5, 3, 32]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 32, 64]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 8192  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 2400
Number of hidden parameters of conv1Biases: 32
Number of hidden parameters of conv2: 18432
Number of hidden parameters of conv2Biases: 64
Number of hidden parameters of local3: 3145728
Number of hidden parameters of local3Biases: 384
Number of hidden parameters of local4: 73728
Number of hidden parameters of local4Biases: 192
Number of hidden parameters of softmax: 1920
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 3242890
Network summary:
conv1Shape: [5, 5, 3, 32]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 32, 64]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 4096  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 2400
Number of hidden parameters of conv1Biases: 32
Number of hidden parameters of conv2: 18432
Number of hidden parameters of conv2Biases: 64
Number of hidden parameters of local3: 1572864
Number of hidden parameters of local3Biases: 384
Number of hidden parameters of local4: 73728
Number of hidden parameters of local4Biases: 192
Number of hidden parameters of softmax: 1920
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 1670026
########################################################################################
########################################################################################
########################################################################################
2017-05-06 06:58:44.699465: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 4096  | local3OutputDepth: 384
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 4096  | local3OutputDepth: 384
2017-05-06 06:59:05.462878: step 0, loss = 6.37 (16.1 examples/sec; 7.948 sec/batch)
local3InputDepth: 4096  | local3OutputDepth: 384
Evaluation results:
2017-05-06 06:59:11.815690: Total Predictions = 1024
2017-05-06 06:59:11.822945: Correct Predictions = 140
2017-05-06 06:59:11.829586: Wrong Predictions = 884
2017-05-06 06:59:11.845663: precision @ 1 = 0.137
2017-05-06 06:59:12.055966: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [5, 5, 3, 32]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 32, 64]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 4096  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 2400
Number of hidden parameters of conv1Biases: 32
Number of hidden parameters of conv2: 18432
Number of hidden parameters of conv2Biases: 64
Number of hidden parameters of local3: 1572864
Number of hidden parameters of local3Biases: 384
Number of hidden parameters of local4: 73728
Number of hidden parameters of local4Biases: 192
Number of hidden parameters of softmax: 1920
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 1670026
########################################################################################
########################################################################################
########################################################################################
2017-05-06 07:04:52.750422: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 4096  | local3OutputDepth: 384 (128, 4096)
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 4096  | local3OutputDepth: 384 (128, 4096)
2017-05-06 07:05:13.738170: step 0, loss = 6.38 (16.0 examples/sec; 7.995 sec/batch)
local3InputDepth: 4096  | local3OutputDepth: 384 (128, 4096)
Evaluation results:
2017-05-06 07:05:19.690125: Total Predictions = 1024
2017-05-06 07:05:19.700096: Correct Predictions = 117
2017-05-06 07:05:19.708054: Wrong Predictions = 907
2017-05-06 07:05:19.715906: precision @ 1 = 0.114
2017-05-06 07:05:19.918027: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [3, 3, 3, 32]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 32, 64]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 4096  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 864
Number of hidden parameters of conv1Biases: 32
Number of hidden parameters of conv2: 18432
Number of hidden parameters of conv2Biases: 64
Number of hidden parameters of local3: 1572864
Number of hidden parameters of local3Biases: 384
Number of hidden parameters of local4: 73728
Number of hidden parameters of local4Biases: 192
Number of hidden parameters of softmax: 1920
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 1668490
########################################################################################
########################################################################################
########################################################################################
2017-05-06 07:09:39.720679: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 4096  | local3OutputDepth: 384 (128, 4096)
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 4096  | local3OutputDepth: 384 (128, 4096)
2017-05-06 07:10:00.793159: step 0, loss = 6.38 (16.0 examples/sec; 8.007 sec/batch)
local3InputDepth: 4096  | local3OutputDepth: 384 (128, 4096)
Evaluation results:
2017-05-06 07:10:06.809796: Total Predictions = 1024
2017-05-06 07:10:06.818063: Correct Predictions = 120
2017-05-06 07:10:06.826424: Wrong Predictions = 904
2017-05-06 07:10:06.833174: precision @ 1 = 0.117
2017-05-06 07:10:07.874667: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [3, 3, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 4096  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 108
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 360
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 1572864
Number of hidden parameters of local3Biases: 384
Number of hidden parameters of local4: 73728
Number of hidden parameters of local4Biases: 192
Number of hidden parameters of softmax: 1920
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 1649580
########################################################################################
########################################################################################
########################################################################################
2017-05-06 07:12:06.730556: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 640  | local3OutputDepth: 384 (128, 640)
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 640  | local3OutputDepth: 384 (128, 640)
2017-05-06 07:12:27.971007: step 0, loss = 3.09 (15.8 examples/sec; 8.096 sec/batch)
local3InputDepth: 640  | local3OutputDepth: 384 (128, 640)
Evaluation results:
2017-05-06 07:12:33.517821: Total Predictions = 1024
2017-05-06 07:12:33.525807: Correct Predictions = 162
2017-05-06 07:12:33.533785: Wrong Predictions = 862
2017-05-06 07:12:33.542031: precision @ 1 = 0.158
2017-05-06 07:12:33.605156: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [3, 3, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: [3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10]  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 108
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 360
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: [3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10, 3, 3, 4, 10]
Number of hidden parameters of local3Biases: 384
Network summary:
conv1Shape: [3, 3, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 108
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 360
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 245760
Number of hidden parameters of local3Biases: 384
Number of hidden parameters of local4: 73728
Number of hidden parameters of local4Biases: 192
Number of hidden parameters of softmax: 1920
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 322476
Network summary:
conv1Shape: [3, 3, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 108
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 245760
Number of hidden parameters of local3Biases: 384
Number of hidden parameters of local4: 73728
Number of hidden parameters of local4Biases: 192
Number of hidden parameters of softmax: 1920
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 322276
########################################################################################
########################################################################################
########################################################################################
2017-05-06 07:15:43.889352: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 640  | local3OutputDepth: 384 (128, 640)
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 640  | local3OutputDepth: 384 (128, 640)
2017-05-06 07:16:04.810506: step 0, loss = 3.09 (16.6 examples/sec; 7.733 sec/batch)
local3InputDepth: 640  | local3OutputDepth: 384 (128, 640)
Evaluation results:
2017-05-06 07:16:10.340646: Total Predictions = 1024
2017-05-06 07:16:10.347373: Correct Predictions = 105
2017-05-06 07:16:10.353932: Wrong Predictions = 919
2017-05-06 07:16:10.360522: precision @ 1 = 0.103
2017-05-06 07:16:10.422826: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [3, 3, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 3, 3, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 108
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 245760
Number of hidden parameters of local3Biases: 384
Number of hidden parameters of local4: 73728
Number of hidden parameters of local4Biases: 192
Number of hidden parameters of softmax: 1920
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 322276
########################################################################################
########################################################################################
########################################################################################
2017-05-06 07:16:23.641688: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 640  | local3OutputDepth: 384 (128, 640)
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 640  | local3OutputDepth: 384 (128, 640)
2017-05-06 07:16:44.352207: step 0, loss = 3.09 (16.7 examples/sec; 7.680 sec/batch)
local3InputDepth: 640  | local3OutputDepth: 384 (128, 640)
Evaluation results:
2017-05-06 07:16:49.933807: Total Predictions = 1024
2017-05-06 07:16:49.942310: Correct Predictions = 104
2017-05-06 07:16:49.949456: Wrong Predictions = 920
2017-05-06 07:16:49.958394: precision @ 1 = 0.102
2017-05-06 07:16:50.025931: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [3, 3, 3, 16]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 32]
pool2ksize: [1, 3, 3, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 2048  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 432
Number of hidden parameters of conv1Biases: 16
Number of hidden parameters of conv2: 512
Number of hidden parameters of conv2Biases: 32
Number of hidden parameters of local3: 786432
Number of hidden parameters of local3Biases: 384
Number of hidden parameters of local4: 73728
Number of hidden parameters of local4Biases: 192
Number of hidden parameters of softmax: 1920
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 863658
########################################################################################
########################################################################################
########################################################################################
2017-05-06 07:17:15.612074: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Network summary:
conv1Shape: [3, 3, 3, 16]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 16, 32]
pool2ksize: [1, 3, 3, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 2048  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 432
Number of hidden parameters of conv1Biases: 16
Number of hidden parameters of conv2: 2048
Number of hidden parameters of conv2Biases: 32
Number of hidden parameters of local3: 786432
Number of hidden parameters of local3Biases: 384
Number of hidden parameters of local4: 73728
Number of hidden parameters of local4Biases: 192
Number of hidden parameters of softmax: 1920
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 865194
########################################################################################
########################################################################################
########################################################################################
2017-05-06 07:18:57.336346: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 2048  | local3OutputDepth: 384 (128, 2048)
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 2048  | local3OutputDepth: 384 (128, 2048)
2017-05-06 07:19:18.320014: step 0, loss = 4.43 (16.4 examples/sec; 7.818 sec/batch)
local3InputDepth: 2048  | local3OutputDepth: 384 (128, 2048)
Evaluation results:
2017-05-06 07:19:26.190573: Total Predictions = 1024
2017-05-06 07:19:26.198727: Correct Predictions = 110
2017-05-06 07:19:26.207231: Wrong Predictions = 914
2017-05-06 07:19:26.214679: precision @ 1 = 0.107
2017-05-06 07:19:26.336365: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [3, 3, 3, 16]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 16, 16]
pool2ksize: [1, 3, 3, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 1024  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 432
Number of hidden parameters of conv1Biases: 16
Number of hidden parameters of conv2: 1024
Number of hidden parameters of conv2Biases: 16
Number of hidden parameters of local3: 393216
Number of hidden parameters of local3Biases: 384
Number of hidden parameters of local4: 73728
Number of hidden parameters of local4Biases: 192
Number of hidden parameters of softmax: 1920
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 470938
########################################################################################
########################################################################################
########################################################################################
2017-05-06 07:21:22.119180: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 1024  | local3OutputDepth: 384 (128, 1024)
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 1024  | local3OutputDepth: 384 (128, 1024)
2017-05-06 07:22:43.291283: step 0, loss = 3.46 (3.4 examples/sec; 37.997 sec/batch)
local3InputDepth: 1024  | local3OutputDepth: 384 (128, 1024)
Evaluation results:
2017-05-06 07:22:49.272812: Total Predictions = 1024
2017-05-06 07:22:49.280630: Correct Predictions = 107
2017-05-06 07:22:49.288197: Wrong Predictions = 917
2017-05-06 07:22:49.296679: precision @ 1 = 0.104
2017-05-06 07:22:49.420302: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [3, 3, 3, 16]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 16, 16]
pool2ksize: [1, 3, 3, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 1024  | local3OutputDepth: 1024
local4InputDepth: 1024  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 432
Number of hidden parameters of conv1Biases: 16
Number of hidden parameters of conv2: 1024
Number of hidden parameters of conv2Biases: 16
Number of hidden parameters of local3: 1048576
Number of hidden parameters of local3Biases: 1024
Number of hidden parameters of local4: 196608
Number of hidden parameters of local4Biases: 192
Number of hidden parameters of softmax: 1920
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 1249818
########################################################################################
########################################################################################
########################################################################################
2017-05-06 07:24:05.130188: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 1024  | local3OutputDepth: 1024 (128, 1024)
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 1024  | local3OutputDepth: 1024 (128, 1024)
2017-05-06 07:24:26.638589: step 0, loss = 5.38 (16.1 examples/sec; 7.960 sec/batch)
local3InputDepth: 1024  | local3OutputDepth: 1024 (128, 1024)
Evaluation results:
2017-05-06 07:24:32.475403: Total Predictions = 1024
2017-05-06 07:24:32.483926: Correct Predictions = 109
2017-05-06 07:24:32.491771: Wrong Predictions = 915
2017-05-06 07:24:32.499695: precision @ 1 = 0.106
2017-05-06 07:24:32.645549: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [5, 5, 3, 16]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 16, 32]
pool2ksize: [1, 3, 3, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 2048  | local3OutputDepth: 2048
local4InputDepth: 2048  | local4OutputDepth: 192
softmax_linearInput: 192

Number of hidden parameters of conv1: 1200
Number of hidden parameters of conv1Biases: 16
Number of hidden parameters of conv2: 12800
Number of hidden parameters of conv2Biases: 32
Number of hidden parameters of local3: 4194304
Number of hidden parameters of local3Biases: 2048
Number of hidden parameters of local4: 393216
Number of hidden parameters of local4Biases: 192
Number of hidden parameters of softmax: 1920
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 4605738
Network summary:
conv1Shape: [5, 5, 3, 16]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 16, 32]
pool2ksize: [1, 3, 3, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 2048  | local3OutputDepth: 2048
local4InputDepth: 2048  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 1200
Number of hidden parameters of conv1Biases: 16
Number of hidden parameters of conv2: 12800
Number of hidden parameters of conv2Biases: 32
Number of hidden parameters of local3: 4194304
Number of hidden parameters of local3Biases: 2048
Number of hidden parameters of local4: 131072
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 4342186
Network summary:
conv1Shape: [5, 5, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 4, 16]
pool2ksize: [1, 3, 3, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 1024  | local3OutputDepth: 1024
local4InputDepth: 1024  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 300
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 1600
Number of hidden parameters of conv2Biases: 16
Number of hidden parameters of local3: 1048576
Number of hidden parameters of local3Biases: 1024
Number of hidden parameters of local4: 65536
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 1117770
Network summary:
conv1Shape: [5, 5, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 4, 16]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 1024  | local3OutputDepth: 1024
local4InputDepth: 1024  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 300
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 1600
Number of hidden parameters of conv2Biases: 16
Number of hidden parameters of local3: 1048576
Number of hidden parameters of local3Biases: 1024
Number of hidden parameters of local4: 65536
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 1117770
Network summary:
conv1Shape: [5, 5, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 4, 16]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 1024  | local3OutputDepth: 1024
local4InputDepth: 1024  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 300
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 576
Number of hidden parameters of conv2Biases: 16
Number of hidden parameters of local3: 1048576
Number of hidden parameters of local3Biases: 1024
Number of hidden parameters of local4: 65536
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 1116746
Network summary:
conv1Shape: [3, 3, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 4, 16]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 1024  | local3OutputDepth: 1024
local4InputDepth: 1024  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 108
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 576
Number of hidden parameters of conv2Biases: 16
Number of hidden parameters of local3: 1048576
Number of hidden parameters of local3Biases: 1024
Number of hidden parameters of local4: 65536
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 1116554
Network summary:
conv1Shape: [3, 3, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 4, 6]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 384  | local3OutputDepth: 384
local4InputDepth: 384  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 108
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 216
Number of hidden parameters of conv2Biases: 6
Number of hidden parameters of local3: 147456
Number of hidden parameters of local3Biases: 384
Number of hidden parameters of local4: 24576
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 173464
Network summary:
conv1Shape: [3, 3, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 4, 16]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 1024  | local3OutputDepth: 1024
local4InputDepth: 1024  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 108
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 576
Number of hidden parameters of conv2Biases: 16
Number of hidden parameters of local3: 1048576
Number of hidden parameters of local3Biases: 1024
Number of hidden parameters of local4: 65536
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 1116554
########################################################################################
########################################################################################
########################################################################################
2017-05-06 07:32:23.927611: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 1024  | local3OutputDepth: 1024 (128, 1024)
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 1024  | local3OutputDepth: 1024 (128, 1024)
2017-05-06 07:32:45.889949: step 0, loss = 5.06 (16.2 examples/sec; 7.923 sec/batch)
local3InputDepth: 1024  | local3OutputDepth: 1024 (128, 1024)
Evaluation results:
2017-05-06 07:32:54.009965: Total Predictions = 1024
2017-05-06 07:32:54.017839: Correct Predictions = 95
2017-05-06 07:32:54.024043: Wrong Predictions = 929
2017-05-06 07:32:54.030323: precision @ 1 = 0.093
2017-05-06 07:32:54.094247: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [3, 3, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [3, 3, 4, 8]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 512  | local3OutputDepth: 512
local4InputDepth: 512  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 108
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 288
Number of hidden parameters of conv2Biases: 8
Number of hidden parameters of local3: 262144
Number of hidden parameters of local3Biases: 512
Number of hidden parameters of local4: 32768
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 296546
########################################################################################
########################################################################################
########################################################################################
2017-05-06 07:33:01.625049: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 512  | local3OutputDepth: 512 (128, 512)
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 512  | local3OutputDepth: 512 (128, 512)
2017-05-06 07:33:23.084358: step 0, loss = 3.03 (16.3 examples/sec; 7.849 sec/batch)
local3InputDepth: 512  | local3OutputDepth: 512 (128, 512)
Evaluation results:
2017-05-06 07:33:28.832604: Total Predictions = 1024
2017-05-06 07:33:28.852790: Correct Predictions = 110
2017-05-06 07:33:28.862862: Wrong Predictions = 914
2017-05-06 07:33:28.877477: precision @ 1 = 0.107
2017-05-06 07:33:28.939047: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 8]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 512  | local3OutputDepth: 512
local4InputDepth: 512  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 128
Number of hidden parameters of conv2Biases: 8
Number of hidden parameters of local3: 262144
Number of hidden parameters of local3Biases: 512
Number of hidden parameters of local4: 32768
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 296326
########################################################################################
########################################################################################
########################################################################################
2017-05-06 07:34:18.368913: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 512  | local3OutputDepth: 512 (128, 512)
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 512  | local3OutputDepth: 512 (128, 512)
2017-05-06 07:34:39.488517: step 0, loss = 3.03 (15.9 examples/sec; 8.045 sec/batch)
local3InputDepth: 512  | local3OutputDepth: 512 (128, 512)
Evaluation results:
2017-05-06 07:34:45.116743: Total Predictions = 1024
2017-05-06 07:34:45.123632: Correct Predictions = 102
2017-05-06 07:34:45.130015: Wrong Predictions = 922
2017-05-06 07:34:45.136684: precision @ 1 = 0.100
2017-05-06 07:34:45.203918: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 07:35:02.806373: Running on server...
The experiment details:
max_steps = 10 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 640  | local3OutputDepth: 640 (128, 640)
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 640  | local3OutputDepth: 640 (128, 640)
2017-05-06 07:35:25.953778: step 0, loss = 3.42 (15.0 examples/sec; 8.533 sec/batch)
local3InputDepth: 640  | local3OutputDepth: 640 (128, 640)
Evaluation results:
2017-05-06 07:35:31.825258: Total Predictions = 1024
2017-05-06 07:35:31.834623: Correct Predictions = 110
2017-05-06 07:35:31.844244: Wrong Predictions = 914
2017-05-06 07:35:31.855263: precision @ 1 = 0.107
2017-05-06 07:35:31.910195: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 07:36:16.559719: Running on server...
The experiment details:
max_steps = 4000 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 640  | local3OutputDepth: 640 (128, 640)
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
local3InputDepth: 640  | local3OutputDepth: 640 (128, 640)
2017-05-06 07:36:38.020791: step 0, loss = 3.42 (16.3 examples/sec; 7.838 sec/batch)
2017-05-06 07:36:50.364378: step 100, loss = 3.33 (2326.5 examples/sec; 0.055 sec/batch)
2017-05-06 07:37:01.486676: step 200, loss = 3.25 (2452.4 examples/sec; 0.052 sec/batch)
2017-05-06 07:37:12.617229: step 300, loss = 3.11 (2355.5 examples/sec; 0.054 sec/batch)
2017-05-06 07:37:23.756607: step 400, loss = 2.77 (2364.8 examples/sec; 0.054 sec/batch)
2017-05-06 07:37:34.923864: step 500, loss = 2.56 (2453.1 examples/sec; 0.052 sec/batch)
2017-05-06 07:37:46.072453: step 600, loss = 2.49 (2430.3 examples/sec; 0.053 sec/batch)
2017-05-06 07:37:57.158197: step 700, loss = 2.24 (2425.0 examples/sec; 0.053 sec/batch)
2017-05-06 07:38:08.237591: step 800, loss = 2.18 (2320.0 examples/sec; 0.055 sec/batch)
2017-05-06 07:38:19.353378: step 900, loss = 2.19 (2218.0 examples/sec; 0.058 sec/batch)
2017-05-06 07:38:30.471288: step 1000, loss = 2.08 (2383.4 examples/sec; 0.054 sec/batch)
2017-05-06 07:38:43.233239: step 1100, loss = 1.84 (2279.8 examples/sec; 0.056 sec/batch)
2017-05-06 07:38:54.365851: step 1200, loss = 1.86 (2483.6 examples/sec; 0.052 sec/batch)
2017-05-06 07:39:06.817261: step 1300, loss = 1.68 (2327.6 examples/sec; 0.055 sec/batch)
2017-05-06 07:39:20.007390: step 1400, loss = 1.98 (2347.9 examples/sec; 0.055 sec/batch)
2017-05-06 07:39:31.196121: step 1500, loss = 1.74 (2270.2 examples/sec; 0.056 sec/batch)
2017-05-06 07:39:42.372333: step 1600, loss = 1.82 (2332.9 examples/sec; 0.055 sec/batch)
2017-05-06 07:39:54.188438: step 1700, loss = 1.64 (2320.9 examples/sec; 0.055 sec/batch)
2017-05-06 07:40:05.396618: step 1800, loss = 1.59 (2298.7 examples/sec; 0.056 sec/batch)
2017-05-06 07:40:16.631801: step 1900, loss = 1.57 (2451.5 examples/sec; 0.052 sec/batch)
2017-05-06 07:40:27.782265: step 2000, loss = 1.76 (2508.9 examples/sec; 0.051 sec/batch)
2017-05-06 07:40:39.927714: step 2100, loss = 1.55 (2300.0 examples/sec; 0.056 sec/batch)
2017-05-06 07:40:51.110175: step 2200, loss = 1.41 (2238.7 examples/sec; 0.057 sec/batch)
2017-05-06 07:41:02.235215: step 2300, loss = 1.45 (2308.4 examples/sec; 0.055 sec/batch)
2017-05-06 07:41:13.362725: step 2400, loss = 1.36 (2455.9 examples/sec; 0.052 sec/batch)
2017-05-06 07:41:24.518605: step 2500, loss = 1.10 (2404.4 examples/sec; 0.053 sec/batch)
2017-05-06 07:41:35.667900: step 2600, loss = 1.39 (2227.2 examples/sec; 0.057 sec/batch)
2017-05-06 07:41:46.856220: step 2700, loss = 1.41 (2367.0 examples/sec; 0.054 sec/batch)
2017-05-06 07:41:58.075557: step 2800, loss = 1.42 (2294.0 examples/sec; 0.056 sec/batch)
2017-05-06 07:42:09.289064: step 2900, loss = 1.53 (2293.2 examples/sec; 0.056 sec/batch)
2017-05-06 07:42:20.454109: step 3000, loss = 1.10 (2450.0 examples/sec; 0.052 sec/batch)
2017-05-06 07:42:32.648030: step 3100, loss = 1.24 (2355.0 examples/sec; 0.054 sec/batch)
2017-05-06 07:42:43.866135: step 3200, loss = 1.16 (2278.9 examples/sec; 0.056 sec/batch)
2017-05-06 07:42:55.110855: step 3300, loss = 1.26 (2298.0 examples/sec; 0.056 sec/batch)
2017-05-06 07:43:06.325069: step 3400, loss = 1.11 (2397.6 examples/sec; 0.053 sec/batch)
2017-05-06 07:43:17.515225: step 3500, loss = 1.23 (2280.7 examples/sec; 0.056 sec/batch)
2017-05-06 07:43:28.688504: step 3600, loss = 1.11 (2457.1 examples/sec; 0.052 sec/batch)
2017-05-06 07:43:39.875295: step 3700, loss = 1.05 (2352.1 examples/sec; 0.054 sec/batch)
2017-05-06 07:43:51.051435: step 3800, loss = 1.07 (2274.9 examples/sec; 0.056 sec/batch)
2017-05-06 07:44:02.265056: step 3900, loss = 1.17 (2270.9 examples/sec; 0.056 sec/batch)
local3InputDepth: 640  | local3OutputDepth: 640 (128, 640)
Evaluation results:
2017-05-06 07:44:16.817533: Total Predictions = 1024
2017-05-06 07:44:16.826063: Correct Predictions = 706
2017-05-06 07:44:16.834331: Wrong Predictions = 318
2017-05-06 07:44:16.842618: precision @ 1 = 0.689
2017-05-06 07:44:16.912707: DONE
########################################################################################
########################################################################################
########################################################################################
Network summary:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 08:41:58.217594: Running on server...
The experiment details:
max_steps = 40 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 08:42:53.954232: Running on server...
The experiment details:
max_steps = 40 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 08:44:02.888455: Running on server...
The experiment details:
max_steps = 40 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 08:44:50.815616: Running on server...
The experiment details:
max_steps = 40 log_frequency = 100 num_gpus = 0
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 08:45:14.055959: step 0, loss = 3.56 (986.0 examples/sec; 0.130 sec/batch)
########################################################################################
########################################################################################
########################################################################################
2017-05-06 08:48:57.041925: Running on server...
The experiment details:
max_steps = 40 log_frequency = 100 num_gpus = 0
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 08:49:06.644328: step 0, loss = 3.56 (1505.7 examples/sec; 0.085 sec/batch)
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 08:50:04.344082: Running on server...
The experiment details:
max_steps = 40 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 08:50:29.005901: step 0, loss = 5.72 (16.3 examples/sec; 7.836 sec/batch)
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 08:57:47.611215: Running on server...
The experiment details:
max_steps = 40 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 08:58:11.229943: step 0, loss = 5.72 (15.3 examples/sec; 8.364 sec/batch)
Evaluation results:
2017-05-06 08:58:19.923464: Total Predictions = 1024
2017-05-06 08:58:19.930865: Correct Predictions = 92
2017-05-06 08:58:19.937344: Wrong Predictions = 932
2017-05-06 08:58:19.943780: precision @ 1 = 0.090
2017-05-06 08:58:20.006295: DONE
########################################################################################
########################################################################################
########################################################################################
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 09:13:11.681892: Running on server...
The experiment details:
max_steps = 40 log_frequency = 100 num_gpus = 0
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 09:16:14.099085: Running on server...
The experiment details:
max_steps = 40 log_frequency = 100 num_gpus = 0
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 09:18:52.495199: Running on server...
The experiment details:
max_steps = 40 log_frequency = 100 num_gpus = 0
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 09:38:45.208351: Running on server...
The experiment details:
max_steps = 40 log_frequency = 100 num_gpus = 0
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 09:39:00.586114: step 0, loss = 3.56 (1162.5 examples/sec; 0.110 sec/batch)
Evaluation results:
2017-05-06 09:39:04.960147: Total Predictions = 1024
2017-05-06 09:39:04.969071: Correct Predictions = 188
2017-05-06 09:39:04.976431: Wrong Predictions = 836
2017-05-06 09:39:04.984731: precision @ 1 = 0.184
2017-05-06 09:39:05.054260: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-06 09:40:06.712222: Running on server...
The experiment details:
max_steps = 40 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 09:40:28.638858: step 0, loss = 5.72 (15.5 examples/sec; 8.252 sec/batch)
Evaluation results:
2017-05-06 09:40:38.551648: Total Predictions = 1024
2017-05-06 09:40:38.560564: Correct Predictions = 100
2017-05-06 09:40:38.569193: Wrong Predictions = 924
2017-05-06 09:40:38.577031: precision @ 1 = 0.098
2017-05-06 09:40:38.639190: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-06 10:03:26.742229: Running on server...
The experiment details:
max_steps = 40 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 10:10:27.247437: Running on server...
The experiment details:
max_steps = 400 log_frequency = 100 num_gpus = 2
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 10:12:57.864763: Running on server...
The experiment details:
max_steps = 400 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 10:13:18.898883: step 0, loss = 5.72 (16.5 examples/sec; 7.758 sec/batch)
2017-05-06 10:13:31.038481: step 100, loss = 5.63 (2342.1 examples/sec; 0.055 sec/batch)
2017-05-06 10:13:41.788823: step 200, loss = 5.12 (2503.2 examples/sec; 0.051 sec/batch)
2017-05-06 10:13:52.520017: step 300, loss = 4.39 (2440.0 examples/sec; 0.052 sec/batch)
2017-05-06 10:14:05.730265: DONE
########################################################################################
########################################################################################
########################################################################################
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 10:14:37.154964: Running on server...
The experiment details:
max_steps = 400 log_frequency = 100 num_gpus = 2
########################################################################################
########################################################################################
########################################################################################
2017-05-06 10:14:39.545459: Running on server...
The experiment details:
max_steps = 400 log_frequency = 100 num_gpus = 2
########################################################################################
########################################################################################
########################################################################################
2017-05-06 10:15:37.995120: Running on server...
The experiment details:
max_steps = 400 log_frequency = 100 num_gpus = 2
########################################################################################
########################################################################################
########################################################################################
2017-05-06 10:15:51.278875: Running on server...
The experiment details:
max_steps = 400 log_frequency = 100 num_gpus = 2
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 10:17:07.426899: Running on server...
The experiment details:
max_steps = 400 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 10:17:32.075172: step 0, loss = 5.72 (15.9 examples/sec; 8.068 sec/batch)
2017-05-06 10:17:44.160718: step 100, loss = 5.63 (2323.8 examples/sec; 0.055 sec/batch)
2017-05-06 10:17:55.020917: step 200, loss = 5.27 (2312.5 examples/sec; 0.055 sec/batch)
2017-05-06 10:18:05.909581: step 300, loss = 4.33 (2379.5 examples/sec; 0.054 sec/batch)
Evaluation results:
2017-05-06 10:18:23.501901: Total Predictions = 10112
2017-05-06 10:18:23.509152: Correct Predictions = 4382
2017-05-06 10:18:23.516647: Wrong Predictions = 5730
2017-05-06 10:18:23.527513: precision @ 1 = 0.433
2017-05-06 10:18:23.600455: DONE
########################################################################################
########################################################################################
########################################################################################
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 10:22:39.103400: Running on server...
The experiment details:
max_steps = 10000 log_frequency = 100 num_gpus = 2
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 10:23:12.179061: Running on server...
The experiment details:
max_steps = 10000 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 10:23:34.804127: step 0, loss = 5.72 (14.9 examples/sec; 8.567 sec/batch)
2017-05-06 10:23:48.139478: step 100, loss = 5.64 (2136.1 examples/sec; 0.060 sec/batch)
2017-05-06 10:23:59.885178: step 200, loss = 4.79 (2188.1 examples/sec; 0.058 sec/batch)
2017-05-06 10:24:11.667006: step 300, loss = 4.43 (2137.9 examples/sec; 0.060 sec/batch)
2017-05-06 10:24:23.483127: step 400, loss = 4.28 (2358.2 examples/sec; 0.054 sec/batch)
2017-05-06 10:24:35.314237: step 500, loss = 4.17 (2217.6 examples/sec; 0.058 sec/batch)
2017-05-06 10:24:47.012029: step 600, loss = 3.90 (2223.0 examples/sec; 0.058 sec/batch)
2017-05-06 10:24:58.764054: step 700, loss = 3.36 (2246.2 examples/sec; 0.057 sec/batch)
2017-05-06 10:25:10.809879: step 800, loss = 3.62 (2051.6 examples/sec; 0.062 sec/batch)
2017-05-06 10:25:22.716624: step 900, loss = 3.53 (2044.9 examples/sec; 0.063 sec/batch)
2017-05-06 10:25:34.462945: step 1000, loss = 3.58 (2424.2 examples/sec; 0.053 sec/batch)
2017-05-06 10:25:47.384786: step 1100, loss = 3.30 (2306.8 examples/sec; 0.055 sec/batch)
2017-05-06 10:25:58.914212: step 1200, loss = 3.01 (2368.8 examples/sec; 0.054 sec/batch)
2017-05-06 10:26:10.997030: step 1300, loss = 2.92 (2030.8 examples/sec; 0.063 sec/batch)
2017-05-06 10:26:23.923231: step 1400, loss = 2.77 (1989.1 examples/sec; 0.064 sec/batch)
2017-05-06 10:26:36.799411: step 1500, loss = 2.77 (2054.1 examples/sec; 0.062 sec/batch)
2017-05-06 10:26:49.575823: step 1600, loss = 2.35 (1940.5 examples/sec; 0.066 sec/batch)
2017-05-06 10:27:02.732675: step 1700, loss = 2.42 (2101.7 examples/sec; 0.061 sec/batch)
2017-05-06 10:27:15.965047: step 1800, loss = 2.50 (1941.1 examples/sec; 0.066 sec/batch)
2017-05-06 10:27:28.456641: step 1900, loss = 2.79 (2191.6 examples/sec; 0.058 sec/batch)
2017-05-06 10:27:40.209020: step 2000, loss = 2.63 (2036.8 examples/sec; 0.063 sec/batch)
2017-05-06 10:27:53.985132: step 2100, loss = 2.33 (2268.1 examples/sec; 0.056 sec/batch)
2017-05-06 10:28:06.029056: step 2200, loss = 2.70 (2348.9 examples/sec; 0.054 sec/batch)
2017-05-06 10:28:17.573586: step 2300, loss = 2.26 (2300.8 examples/sec; 0.056 sec/batch)
2017-05-06 10:28:29.020377: step 2400, loss = 2.66 (2218.1 examples/sec; 0.058 sec/batch)
2017-05-06 10:28:40.414813: step 2500, loss = 2.29 (2257.5 examples/sec; 0.057 sec/batch)
2017-05-06 10:28:51.670053: step 2600, loss = 2.13 (2299.6 examples/sec; 0.056 sec/batch)
2017-05-06 10:29:03.024539: step 2700, loss = 1.94 (2259.3 examples/sec; 0.057 sec/batch)
2017-05-06 10:29:14.507848: step 2800, loss = 2.32 (2210.9 examples/sec; 0.058 sec/batch)
2017-05-06 10:29:26.202175: step 2900, loss = 2.10 (2446.1 examples/sec; 0.052 sec/batch)
2017-05-06 10:29:37.547974: step 3000, loss = 2.09 (2320.9 examples/sec; 0.055 sec/batch)
2017-05-06 10:29:50.066135: step 3100, loss = 2.37 (2368.0 examples/sec; 0.054 sec/batch)
2017-05-06 10:30:01.316832: step 3200, loss = 2.03 (2330.3 examples/sec; 0.055 sec/batch)
2017-05-06 10:30:12.579522: step 3300, loss = 2.01 (2319.6 examples/sec; 0.055 sec/batch)
2017-05-06 10:30:23.956400: step 3400, loss = 2.03 (2248.2 examples/sec; 0.057 sec/batch)
2017-05-06 10:30:35.186229: step 3500, loss = 2.10 (2206.0 examples/sec; 0.058 sec/batch)
2017-05-06 10:30:46.490925: step 3600, loss = 1.95 (2389.6 examples/sec; 0.054 sec/batch)
2017-05-06 10:30:57.797074: step 3700, loss = 2.03 (2368.7 examples/sec; 0.054 sec/batch)
2017-05-06 10:31:09.102125: step 3800, loss = 2.38 (2260.8 examples/sec; 0.057 sec/batch)
2017-05-06 10:31:20.344946: step 3900, loss = 1.84 (2424.3 examples/sec; 0.053 sec/batch)
2017-05-06 10:31:31.580706: step 4000, loss = 1.79 (2315.7 examples/sec; 0.055 sec/batch)
2017-05-06 10:31:44.109220: step 4100, loss = 2.11 (2316.6 examples/sec; 0.055 sec/batch)
2017-05-06 10:31:55.300092: step 4200, loss = 1.92 (2164.4 examples/sec; 0.059 sec/batch)
2017-05-06 10:32:06.474418: step 4300, loss = 1.90 (2252.4 examples/sec; 0.057 sec/batch)
2017-05-06 10:32:17.672161: step 4400, loss = 1.91 (2251.2 examples/sec; 0.057 sec/batch)
2017-05-06 10:32:28.867229: step 4500, loss = 1.99 (2158.9 examples/sec; 0.059 sec/batch)
2017-05-06 10:32:40.015012: step 4600, loss = 1.94 (2335.1 examples/sec; 0.055 sec/batch)
2017-05-06 10:32:51.188273: step 4700, loss = 2.09 (2278.4 examples/sec; 0.056 sec/batch)
2017-05-06 10:33:02.409564: step 4800, loss = 1.97 (2292.9 examples/sec; 0.056 sec/batch)
2017-05-06 10:33:13.571375: step 4900, loss = 1.75 (2365.7 examples/sec; 0.054 sec/batch)
2017-05-06 10:33:24.756779: step 5000, loss = 1.83 (2312.6 examples/sec; 0.055 sec/batch)
2017-05-06 10:33:37.192161: step 5100, loss = 1.87 (2354.9 examples/sec; 0.054 sec/batch)
2017-05-06 10:33:48.340677: step 5200, loss = 1.74 (2410.6 examples/sec; 0.053 sec/batch)
2017-05-06 10:33:59.608125: step 5300, loss = 1.69 (2168.1 examples/sec; 0.059 sec/batch)
2017-05-06 10:34:10.888050: step 5400, loss = 1.61 (2403.2 examples/sec; 0.053 sec/batch)
2017-05-06 10:34:22.157067: step 5500, loss = 1.94 (2275.0 examples/sec; 0.056 sec/batch)
2017-05-06 10:34:33.363142: step 5600, loss = 1.81 (2326.0 examples/sec; 0.055 sec/batch)
2017-05-06 10:34:44.592390: step 5700, loss = 2.29 (2260.9 examples/sec; 0.057 sec/batch)
2017-05-06 10:34:55.884322: step 5800, loss = 1.75 (2291.8 examples/sec; 0.056 sec/batch)
2017-05-06 10:35:07.189837: step 5900, loss = 1.86 (2354.4 examples/sec; 0.054 sec/batch)
2017-05-06 10:35:18.471810: step 6000, loss = 2.07 (2282.7 examples/sec; 0.056 sec/batch)
2017-05-06 10:35:31.024544: step 6100, loss = 1.97 (2158.1 examples/sec; 0.059 sec/batch)
2017-05-06 10:35:42.385700: step 6200, loss = 1.92 (2192.9 examples/sec; 0.058 sec/batch)
2017-05-06 10:35:53.619697: step 6300, loss = 2.02 (2252.8 examples/sec; 0.057 sec/batch)
2017-05-06 10:36:04.918681: step 6400, loss = 2.03 (2278.6 examples/sec; 0.056 sec/batch)
2017-05-06 10:36:16.215106: step 6500, loss = 1.87 (2368.4 examples/sec; 0.054 sec/batch)
2017-05-06 10:36:27.510829: step 6600, loss = 1.59 (2357.9 examples/sec; 0.054 sec/batch)
2017-05-06 10:36:38.772875: step 6700, loss = 1.57 (2253.7 examples/sec; 0.057 sec/batch)
2017-05-06 10:36:50.044678: step 6800, loss = 2.25 (2280.8 examples/sec; 0.056 sec/batch)
2017-05-06 10:37:01.410298: step 6900, loss = 1.72 (2221.8 examples/sec; 0.058 sec/batch)
2017-05-06 10:37:12.744404: step 7000, loss = 1.78 (2316.5 examples/sec; 0.055 sec/batch)
2017-05-06 10:37:25.262172: step 7100, loss = 1.85 (2068.6 examples/sec; 0.062 sec/batch)
2017-05-06 10:37:36.580079: step 7200, loss = 1.76 (2172.4 examples/sec; 0.059 sec/batch)
2017-05-06 10:37:47.937238: step 7300, loss = 1.68 (2239.3 examples/sec; 0.057 sec/batch)
2017-05-06 10:37:59.281554: step 7400, loss = 1.74 (2436.0 examples/sec; 0.053 sec/batch)
2017-05-06 10:38:10.642797: step 7500, loss = 1.58 (2282.3 examples/sec; 0.056 sec/batch)
2017-05-06 10:38:21.946933: step 7600, loss = 1.51 (2362.9 examples/sec; 0.054 sec/batch)
2017-05-06 10:38:33.276380: step 7700, loss = 1.83 (2398.3 examples/sec; 0.053 sec/batch)
2017-05-06 10:38:44.616648: step 7800, loss = 1.60 (2114.7 examples/sec; 0.061 sec/batch)
2017-05-06 10:38:56.010480: step 7900, loss = 1.56 (2256.6 examples/sec; 0.057 sec/batch)
2017-05-06 10:39:07.356368: step 8000, loss = 1.97 (2202.3 examples/sec; 0.058 sec/batch)
2017-05-06 10:39:20.358503: step 8100, loss = 1.54 (2244.4 examples/sec; 0.057 sec/batch)
2017-05-06 10:39:31.965166: step 8200, loss = 1.93 (2236.5 examples/sec; 0.057 sec/batch)
2017-05-06 10:39:43.515604: step 8300, loss = 1.96 (2479.9 examples/sec; 0.052 sec/batch)
2017-05-06 10:39:55.140213: step 8400, loss = 1.82 (2257.6 examples/sec; 0.057 sec/batch)
2017-05-06 10:40:06.735355: step 8500, loss = 1.56 (2212.2 examples/sec; 0.058 sec/batch)
2017-05-06 10:40:18.413739: step 8600, loss = 1.89 (2247.7 examples/sec; 0.057 sec/batch)
2017-05-06 10:40:30.096727: step 8700, loss = 1.71 (2200.4 examples/sec; 0.058 sec/batch)
2017-05-06 10:40:41.835007: step 8800, loss = 1.91 (2244.4 examples/sec; 0.057 sec/batch)
2017-05-06 10:40:53.536696: step 8900, loss = 1.93 (2381.1 examples/sec; 0.054 sec/batch)
2017-05-06 10:41:05.137512: step 9000, loss = 2.10 (2247.6 examples/sec; 0.057 sec/batch)
2017-05-06 10:41:17.969151: step 9100, loss = 1.45 (2197.9 examples/sec; 0.058 sec/batch)
2017-05-06 10:41:29.632738: step 9200, loss = 2.08 (2276.9 examples/sec; 0.056 sec/batch)
2017-05-06 10:41:41.869739: step 9300, loss = 1.81 (1935.0 examples/sec; 0.066 sec/batch)
2017-05-06 10:41:55.079752: step 9400, loss = 1.48 (1993.4 examples/sec; 0.064 sec/batch)
2017-05-06 10:42:08.237413: step 9500, loss = 2.02 (1983.0 examples/sec; 0.065 sec/batch)
2017-05-06 10:42:21.436378: step 9600, loss = 1.74 (1927.5 examples/sec; 0.066 sec/batch)
2017-05-06 10:42:34.998608: step 9700, loss = 1.66 (2021.1 examples/sec; 0.063 sec/batch)
2017-05-06 10:42:48.110601: step 9800, loss = 2.01 (2028.2 examples/sec; 0.063 sec/batch)
2017-05-06 10:43:01.341310: step 9900, loss = 2.15 (1981.9 examples/sec; 0.065 sec/batch)
########################################################################################
########################################################################################
########################################################################################
2017-05-06 10:45:46.377585: Running on server...
The experiment details:
max_steps = 10000 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 10:47:02.485169: Running on server...
The experiment details:
max_steps = 10000 log_frequency = 100 num_gpus = 2
2017-05-06 10:47:04.039076: DONE
########################################################################################
########################################################################################
########################################################################################
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 10:48:17.357492: Running on server...
The experiment details:
max_steps = 100 log_frequency = 100 num_gpus = 2
########################################################################################
########################################################################################
########################################################################################
2017-05-06 10:48:22.193864: Running on server...
The experiment details:
max_steps = 100 log_frequency = 100 num_gpus = 2
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 10:54:10.000109: Running on server...
The experiment details:
max_steps = 100 log_frequency = 100 num_gpus = 2

Successfully downloaded cifar-10-binary.tar.gz 170052171 bytes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 10:55:01.126034: step 0, loss = 5.72 (16.3 examples/sec; 7.858 sec/batch)
Evaluation results:
2017-05-06 10:55:19.933807: Total Predictions = 10112
2017-05-06 10:55:19.941395: Correct Predictions = 1381
2017-05-06 10:55:19.948395: Wrong Predictions = 8731
2017-05-06 10:55:19.955080: precision @ 1 = 0.137
2017-05-06 10:55:20.024331: DONE
########################################################################################
########################################################################################
########################################################################################
2017-05-06 10:56:35.436319: Running on server...
The experiment details:
max_steps = 100 log_frequency = 100 num_gpus = 2
########################################################################################
########################################################################################
########################################################################################
2017-05-06 10:57:02.694050: Running on server...
The experiment details:
max_steps = 100 log_frequency = 100 num_gpus = 2
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 10:57:46.246451: Running on server...
The experiment details:
max_steps = 100 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 10:58:08.181773: step 0, loss = 5.72 (15.5 examples/sec; 8.278 sec/batch)
Evaluation results:
2017-05-06 10:58:27.581012: Total Predictions = 10112
2017-05-06 10:58:27.589516: Correct Predictions = 1152
2017-05-06 10:58:27.598642: Wrong Predictions = 8960
2017-05-06 10:58:27.606066: precision @ 1 = 0.114
2017-05-06 10:58:27.670321: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-06 10:58:27.738511: Running on server...
The experiment details:
max_steps = 100 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 10:58:49.596033: step 0, loss = 5.72 (15.5 examples/sec; 8.235 sec/batch)
2017-05-06 10:59:03.509495: DONE
########################################################################################
########################################################################################
########################################################################################
2017-05-06 10:59:58.460801: Running on server...
The experiment details:
max_steps = 100 log_frequency = 100 num_gpus = 2
Evaluation results:
2017-05-06 11:00:03.949548: Total Predictions = 10112
2017-05-06 11:00:03.958729: Correct Predictions = 1008
2017-05-06 11:00:03.966218: Wrong Predictions = 9104
2017-05-06 11:00:03.974437: precision @ 1 = 0.100
2017-05-06 11:00:04.038820: DONE
########################################################################################
########################################################################################
########################################################################################
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 11:04:24.996621: Running on server...
The experiment details:
max_steps = 1000 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 11:04:49.649964: step 0, loss = 5.72 (15.4 examples/sec; 8.319 sec/batch)
2017-05-06 11:05:01.739694: step 100, loss = 5.63 (2391.7 examples/sec; 0.054 sec/batch)
2017-05-06 11:05:12.556814: step 200, loss = 4.74 (2392.4 examples/sec; 0.054 sec/batch)
2017-05-06 11:05:23.379784: step 300, loss = 4.40 (2311.9 examples/sec; 0.055 sec/batch)
2017-05-06 11:05:34.187436: step 400, loss = 4.18 (2383.2 examples/sec; 0.054 sec/batch)
2017-05-06 11:05:45.078030: step 500, loss = 4.32 (2279.9 examples/sec; 0.056 sec/batch)
2017-05-06 11:05:55.900601: step 600, loss = 4.13 (2369.8 examples/sec; 0.054 sec/batch)
2017-05-06 11:06:06.786774: step 700, loss = 3.19 (2090.0 examples/sec; 0.061 sec/batch)
2017-05-06 11:06:17.748715: step 800, loss = 3.40 (2359.5 examples/sec; 0.054 sec/batch)
2017-05-06 11:06:28.656236: step 900, loss = 3.31 (2398.5 examples/sec; 0.053 sec/batch)
Evaluation results:
2017-05-06 11:06:45.955059: Total Predictions = 10112
2017-05-06 11:06:45.961681: Correct Predictions = 5544
2017-05-06 11:06:45.969118: Wrong Predictions = 4568
2017-05-06 11:06:45.976810: precision @ 1 = 0.548
2017-05-06 11:06:46.063472: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-06 11:10:26.611843: Running on server...
The experiment details:
max_steps = 1000 log_frequency = 100 num_gpus = 2
Evaluation results:
2017-05-06 11:10:31.806294: Total Predictions = 10112
2017-05-06 11:10:31.814594: Correct Predictions = 5545
2017-05-06 11:10:31.822645: Wrong Predictions = 4567
2017-05-06 11:10:31.830522: precision @ 1 = 0.548
2017-05-06 11:10:31.894911: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-06 11:11:42.096409: Running on server...
The experiment details:
max_steps = 15000 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 11:12:03.407998: step 0, loss = 5.72 (15.8 examples/sec; 8.103 sec/batch)
2017-05-06 11:12:15.647536: step 100, loss = 5.62 (2493.8 examples/sec; 0.051 sec/batch)
2017-05-06 11:12:26.567917: step 200, loss = 4.88 (2468.5 examples/sec; 0.052 sec/batch)
2017-05-06 11:12:37.481831: step 300, loss = 4.22 (2368.2 examples/sec; 0.054 sec/batch)
2017-05-06 11:12:48.395451: step 400, loss = 4.21 (2494.0 examples/sec; 0.051 sec/batch)
2017-05-06 11:12:59.330326: step 500, loss = 3.79 (2435.7 examples/sec; 0.053 sec/batch)
2017-05-06 11:13:10.243357: step 600, loss = 4.00 (2387.0 examples/sec; 0.054 sec/batch)
2017-05-06 11:13:21.173649: step 700, loss = 3.82 (2384.1 examples/sec; 0.054 sec/batch)
2017-05-06 11:13:32.118813: step 800, loss = 3.78 (2363.7 examples/sec; 0.054 sec/batch)
2017-05-06 11:13:43.012659: step 900, loss = 3.60 (2366.1 examples/sec; 0.054 sec/batch)
2017-05-06 11:13:53.937309: step 1000, loss = 3.21 (2344.8 examples/sec; 0.055 sec/batch)
2017-05-06 11:14:06.225270: step 1100, loss = 3.39 (2425.7 examples/sec; 0.053 sec/batch)
2017-05-06 11:14:17.173157: step 1200, loss = 3.45 (2442.2 examples/sec; 0.052 sec/batch)
2017-05-06 11:14:28.109131: step 1300, loss = 3.42 (2478.3 examples/sec; 0.052 sec/batch)
2017-05-06 11:14:39.072173: step 1400, loss = 2.86 (2314.5 examples/sec; 0.055 sec/batch)
2017-05-06 11:14:50.023947: step 1500, loss = 3.03 (2311.9 examples/sec; 0.055 sec/batch)
2017-05-06 11:15:00.952594: step 1600, loss = 2.80 (2184.2 examples/sec; 0.059 sec/batch)
2017-05-06 11:15:11.917513: step 1700, loss = 3.39 (2418.1 examples/sec; 0.053 sec/batch)
2017-05-06 11:15:22.899673: step 1800, loss = 2.76 (2423.7 examples/sec; 0.053 sec/batch)
2017-05-06 11:15:33.904890: step 1900, loss = 2.91 (2418.6 examples/sec; 0.053 sec/batch)
2017-05-06 11:15:44.993104: step 2000, loss = 2.69 (2214.5 examples/sec; 0.058 sec/batch)
2017-05-06 11:15:57.271545: step 2100, loss = 2.42 (2124.0 examples/sec; 0.060 sec/batch)
2017-05-06 11:16:08.260832: step 2200, loss = 2.33 (2094.2 examples/sec; 0.061 sec/batch)
2017-05-06 11:16:19.256730: step 2300, loss = 2.74 (2285.8 examples/sec; 0.056 sec/batch)
2017-05-06 11:16:30.270933: step 2400, loss = 2.05 (2406.5 examples/sec; 0.053 sec/batch)
2017-05-06 11:16:41.299608: step 2500, loss = 2.51 (2334.7 examples/sec; 0.055 sec/batch)
2017-05-06 11:16:52.317211: step 2600, loss = 2.47 (2463.9 examples/sec; 0.052 sec/batch)
2017-05-06 11:17:03.332147: step 2700, loss = 2.65 (2362.4 examples/sec; 0.054 sec/batch)
2017-05-06 11:17:14.370923: step 2800, loss = 2.35 (2249.7 examples/sec; 0.057 sec/batch)
2017-05-06 11:17:25.399969: step 2900, loss = 2.45 (2197.5 examples/sec; 0.058 sec/batch)
2017-05-06 11:17:36.421519: step 3000, loss = 2.39 (2280.9 examples/sec; 0.056 sec/batch)
2017-05-06 11:17:48.645974: step 3100, loss = 1.59 (2406.0 examples/sec; 0.053 sec/batch)
2017-05-06 11:17:59.644815: step 3200, loss = 2.10 (2642.5 examples/sec; 0.048 sec/batch)
2017-05-06 11:18:10.724387: step 3300, loss = 2.21 (2335.6 examples/sec; 0.055 sec/batch)
2017-05-06 11:18:21.754336: step 3400, loss = 2.31 (2368.8 examples/sec; 0.054 sec/batch)
2017-05-06 11:18:32.842656: step 3500, loss = 2.19 (2237.8 examples/sec; 0.057 sec/batch)
2017-05-06 11:18:43.821740: step 3600, loss = 1.89 (2334.5 examples/sec; 0.055 sec/batch)
2017-05-06 11:18:54.875538: step 3700, loss = 2.32 (2407.8 examples/sec; 0.053 sec/batch)
2017-05-06 11:19:05.903502: step 3800, loss = 2.10 (2438.3 examples/sec; 0.052 sec/batch)
2017-05-06 11:19:16.981558: step 3900, loss = 1.57 (2190.7 examples/sec; 0.058 sec/batch)
2017-05-06 11:19:27.994666: step 4000, loss = 1.74 (2409.0 examples/sec; 0.053 sec/batch)
2017-05-06 11:19:40.279356: step 4100, loss = 2.01 (2296.4 examples/sec; 0.056 sec/batch)
2017-05-06 11:19:51.340279: step 4200, loss = 2.11 (2222.1 examples/sec; 0.058 sec/batch)
2017-05-06 11:20:02.420783: step 4300, loss = 2.10 (2392.9 examples/sec; 0.053 sec/batch)
2017-05-06 11:20:13.462879: step 4400, loss = 1.81 (2305.8 examples/sec; 0.056 sec/batch)
2017-05-06 11:20:24.579212: step 4500, loss = 2.14 (2363.8 examples/sec; 0.054 sec/batch)
2017-05-06 11:20:35.729573: step 4600, loss = 1.74 (2341.5 examples/sec; 0.055 sec/batch)
2017-05-06 11:20:46.936772: step 4700, loss = 1.96 (2338.0 examples/sec; 0.055 sec/batch)
Summary of Network 1:
conv1Shape: [4, 4, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 192
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452280
2017-05-06 11:20:58.104988: step 4800, loss = 1.90 (2461.7 examples/sec; 0.052 sec/batch)
2017-05-06 11:21:09.244039: step 4900, loss = 1.92 (2318.1 examples/sec; 0.055 sec/batch)
2017-05-06 11:21:20.382627: step 5000, loss = 1.68 (2127.3 examples/sec; 0.060 sec/batch)
2017-05-06 11:21:32.926217: step 5100, loss = 1.72 (2001.1 examples/sec; 0.064 sec/batch)
Summary of Network 1:
conv1Shape: [5, 5, 3, 5]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 5, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 375
Number of hidden parameters of conv1Biases: 5
Number of hidden parameters of conv2: 1250
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 453554
2017-05-06 11:21:44.047168: step 5200, loss = 1.76 (2476.6 examples/sec; 0.052 sec/batch)
2017-05-06 11:21:55.196899: step 5300, loss = 1.75 (2037.3 examples/sec; 0.063 sec/batch)
Summary of Network 1:
conv1Shape: [5, 5, 3, 5]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 5, 12]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 768  | local3OutputDepth: 768
local4InputDepth: 768  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 375
Number of hidden parameters of conv1Biases: 5
Number of hidden parameters of conv2: 1500
Number of hidden parameters of conv2Biases: 12
Number of hidden parameters of local3: 589824
Number of hidden parameters of local3Biases: 768
Number of hidden parameters of local4: 49152
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 642350
2017-05-06 11:22:06.340346: step 5400, loss = 1.97 (2355.2 examples/sec; 0.054 sec/batch)
Summary of Network 1:
conv1Shape: [5, 5, 3, 5]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 5, 11]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 704  | local3OutputDepth: 704
local4InputDepth: 704  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 375
Number of hidden parameters of conv1Biases: 5
Number of hidden parameters of conv2: 1375
Number of hidden parameters of conv2Biases: 11
Number of hidden parameters of local3: 495616
Number of hidden parameters of local3Biases: 704
Number of hidden parameters of local4: 45056
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 543856
2017-05-06 11:22:17.459090: step 5500, loss = 1.71 (2291.9 examples/sec; 0.056 sec/batch)
Summary of Network 1:
conv1Shape: [5, 5, 3, 5]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 5, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 375
Number of hidden parameters of conv1Biases: 5
Number of hidden parameters of conv2: 1250
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 453554
2017-05-06 11:22:28.572388: step 5600, loss = 1.89 (2359.1 examples/sec; 0.054 sec/batch)
2017-05-06 11:22:39.727554: step 5700, loss = 2.05 (2416.4 examples/sec; 0.053 sec/batch)
Summary of Network 1:
conv1Shape: [5, 5, 3, 5]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 5, 12]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 768  | local3OutputDepth: 768
local4InputDepth: 768  | local4OutputDepth: 32
softmax_linearInput: 32

Number of hidden parameters of conv1: 375
Number of hidden parameters of conv1Biases: 5
Number of hidden parameters of conv2: 1500
Number of hidden parameters of conv2Biases: 12
Number of hidden parameters of local3: 589824
Number of hidden parameters of local3Biases: 768
Number of hidden parameters of local4: 24576
Number of hidden parameters of local4Biases: 32
Number of hidden parameters of softmax: 320
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 617422
2017-05-06 11:22:51.031959: step 5800, loss = 1.83 (2246.6 examples/sec; 0.057 sec/batch)
2017-05-06 11:23:02.287814: step 5900, loss = 1.66 (2400.9 examples/sec; 0.053 sec/batch)
Summary of Network 1:
conv1Shape: [5, 5, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 32
softmax_linearInput: 32

Number of hidden parameters of conv1: 300
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 1000
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 20480
Number of hidden parameters of local4Biases: 32
Number of hidden parameters of softmax: 320
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 432396
2017-05-06 11:23:13.512171: step 6000, loss = 1.71 (2378.6 examples/sec; 0.054 sec/batch)
2017-05-06 11:23:26.041361: step 6100, loss = 2.26 (2284.2 examples/sec; 0.056 sec/batch)
2017-05-06 11:23:37.243510: step 6200, loss = 1.78 (2316.7 examples/sec; 0.055 sec/batch)
Summary of Network 1:
conv1Shape: [5, 5, 3, 6]
pool1ksize: [1, 4, 4, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 6, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 32
softmax_linearInput: 32

Number of hidden parameters of conv1: 450
Number of hidden parameters of conv1Biases: 6
Number of hidden parameters of conv2: 1500
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 20480
Number of hidden parameters of local4Biases: 32
Number of hidden parameters of softmax: 320
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 433048
2017-05-06 11:23:48.426901: step 6300, loss = 1.62 (2414.7 examples/sec; 0.053 sec/batch)
Summary of Network 1:
conv1Shape: [5, 5, 3, 6]
pool1ksize: [1, 4, 4, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 6, 12]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 768  | local3OutputDepth: 768
local4InputDepth: 768  | local4OutputDepth: 32
softmax_linearInput: 32

Number of hidden parameters of conv1: 450
Number of hidden parameters of conv1Biases: 6
Number of hidden parameters of conv2: 1800
Number of hidden parameters of conv2Biases: 12
Number of hidden parameters of local3: 589824
Number of hidden parameters of local3Biases: 768
Number of hidden parameters of local4: 24576
Number of hidden parameters of local4Biases: 32
Number of hidden parameters of softmax: 320
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 617798
2017-05-06 11:23:59.720884: step 6400, loss = 1.55 (2287.2 examples/sec; 0.056 sec/batch)
2017-05-06 11:24:10.985067: step 6500, loss = 1.48 (2304.1 examples/sec; 0.056 sec/batch)
2017-05-06 11:24:22.201963: step 6600, loss = 1.49 (2453.4 examples/sec; 0.052 sec/batch)
Summary of Network 1:
conv1Shape: [5, 5, 3, 6]
pool1ksize: [1, 4, 4, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 6, 11]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 704  | local3OutputDepth: 704
local4InputDepth: 704  | local4OutputDepth: 32
softmax_linearInput: 32

Number of hidden parameters of conv1: 450
Number of hidden parameters of conv1Biases: 6
Number of hidden parameters of conv2: 1650
Number of hidden parameters of conv2Biases: 11
Number of hidden parameters of local3: 495616
Number of hidden parameters of local3Biases: 704
Number of hidden parameters of local4: 22528
Number of hidden parameters of local4Biases: 32
Number of hidden parameters of softmax: 320
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 521327
2017-05-06 11:24:33.462589: step 6700, loss = 1.83 (2134.2 examples/sec; 0.060 sec/batch)
2017-05-06 11:24:44.718923: step 6800, loss = 1.67 (2357.3 examples/sec; 0.054 sec/batch)
Summary of Network 1:
conv1Shape: [5, 5, 3, 6]
pool1ksize: [1, 4, 4, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 6, 11]
pool2ksize: [1, 3, 3, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 704  | local3OutputDepth: 704
local4InputDepth: 704  | local4OutputDepth: 32
softmax_linearInput: 32

Number of hidden parameters of conv1: 450
Number of hidden parameters of conv1Biases: 6
Number of hidden parameters of conv2: 1650
Number of hidden parameters of conv2Biases: 11
Number of hidden parameters of local3: 495616
Number of hidden parameters of local3Biases: 704
Number of hidden parameters of local4: 22528
Number of hidden parameters of local4Biases: 32
Number of hidden parameters of softmax: 320
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 521327
2017-05-06 11:24:55.959463: step 6900, loss = 1.98 (2323.4 examples/sec; 0.055 sec/batch)
2017-05-06 11:25:07.424936: step 7000, loss = 2.01 (2063.0 examples/sec; 0.062 sec/batch)
2017-05-06 11:25:22.480187: step 7100, loss = 1.87 (2100.0 examples/sec; 0.061 sec/batch)
2017-05-06 11:25:36.904371: step 7200, loss = 2.14 (1841.3 examples/sec; 0.070 sec/batch)
2017-05-06 11:25:51.156417: step 7300, loss = 1.24 (2164.2 examples/sec; 0.059 sec/batch)
2017-05-06 11:26:03.454267: step 7400, loss = 1.65 (1285.3 examples/sec; 0.100 sec/batch)
2017-05-06 11:26:14.882625: step 7500, loss = 1.56 (2413.6 examples/sec; 0.053 sec/batch)
2017-05-06 11:26:26.047970: step 7600, loss = 1.81 (2358.9 examples/sec; 0.054 sec/batch)
2017-05-06 11:26:37.155390: step 7700, loss = 1.54 (2409.4 examples/sec; 0.053 sec/batch)
2017-05-06 11:26:48.244779: step 7800, loss = 1.63 (2433.5 examples/sec; 0.053 sec/batch)
2017-05-06 11:26:59.454686: step 7900, loss = 1.92 (2315.2 examples/sec; 0.055 sec/batch)
2017-05-06 11:27:10.698878: step 8000, loss = 1.64 (2572.7 examples/sec; 0.050 sec/batch)
2017-05-06 11:27:23.164011: step 8100, loss = 1.37 (2268.4 examples/sec; 0.056 sec/batch)
2017-05-06 11:27:34.421342: step 8200, loss = 1.92 (2205.4 examples/sec; 0.058 sec/batch)
2017-05-06 11:27:45.671815: step 8300, loss = 1.48 (2232.7 examples/sec; 0.057 sec/batch)
2017-05-06 11:27:56.944288: step 8400, loss = 1.99 (2375.2 examples/sec; 0.054 sec/batch)
2017-05-06 11:28:08.237059: step 8500, loss = 1.82 (2350.8 examples/sec; 0.054 sec/batch)
2017-05-06 11:28:19.467697: step 8600, loss = 1.64 (2326.7 examples/sec; 0.055 sec/batch)
2017-05-06 11:28:30.730759: step 8700, loss = 1.73 (2267.7 examples/sec; 0.056 sec/batch)
2017-05-06 11:28:42.019488: step 8800, loss = 1.83 (2422.1 examples/sec; 0.053 sec/batch)
2017-05-06 11:28:53.270586: step 8900, loss = 2.20 (2379.9 examples/sec; 0.054 sec/batch)
2017-05-06 11:29:04.504637: step 9000, loss = 1.55 (2368.6 examples/sec; 0.054 sec/batch)
Summary of Network 1:
conv1Shape: [5, 5, 3, 6]
pool1ksize: [1, 4, 4, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 6, 10]
pool2ksize: [1, 3, 3, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 32
softmax_linearInput: 32

Number of hidden parameters of conv1: 450
Number of hidden parameters of conv1Biases: 6
Number of hidden parameters of conv2: 1500
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 20480
Number of hidden parameters of local4Biases: 32
Number of hidden parameters of softmax: 320
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 433048
2017-05-06 11:29:17.114763: step 9100, loss = 2.02 (2305.5 examples/sec; 0.056 sec/batch)
2017-05-06 11:29:28.232168: step 9200, loss = 1.94 (2580.0 examples/sec; 0.050 sec/batch)
2017-05-06 11:29:39.494541: step 9300, loss = 1.86 (2490.4 examples/sec; 0.051 sec/batch)
2017-05-06 11:29:50.758571: step 9400, loss = 1.71 (2370.5 examples/sec; 0.054 sec/batch)
2017-05-06 11:30:02.031920: step 9500, loss = 1.91 (2294.7 examples/sec; 0.056 sec/batch)
2017-05-06 11:30:13.210797: step 9600, loss = 1.63 (2268.5 examples/sec; 0.056 sec/batch)
Summary of Network 1:
conv1Shape: [5, 5, 3, 6]
pool1ksize: [1, 3, 3, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 6, 10]
pool2ksize: [1, 3, 3, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 32
softmax_linearInput: 32

Number of hidden parameters of conv1: 450
Number of hidden parameters of conv1Biases: 6
Number of hidden parameters of conv2: 1500
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 20480
Number of hidden parameters of local4Biases: 32
Number of hidden parameters of softmax: 320
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 433048
2017-05-06 11:30:24.475507: step 9700, loss = 1.55 (2303.9 examples/sec; 0.056 sec/batch)
Summary of Network 1:
conv1Shape: [5, 5, 3, 6]
pool1ksize: [1, 3, 3, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 6, 10]
pool2ksize: [1, 3, 3, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 450
Number of hidden parameters of conv1Biases: 6
Number of hidden parameters of conv2: 1500
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 453880
2017-05-06 11:30:35.740593: step 9800, loss = 1.99 (2385.5 examples/sec; 0.054 sec/batch)
Summary of Network 1:
conv1Shape: [5, 5, 3, 6]
pool1ksize: [1, 3, 3, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 6, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 450
Number of hidden parameters of conv1Biases: 6
Number of hidden parameters of conv2: 1500
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 453880
2017-05-06 11:30:47.046963: step 9900, loss = 1.74 (2011.9 examples/sec; 0.064 sec/batch)
2017-05-06 11:30:58.288355: step 10000, loss = 1.36 (2233.0 examples/sec; 0.057 sec/batch)
2017-05-06 11:31:10.908379: step 10100, loss = 1.69 (2257.7 examples/sec; 0.057 sec/batch)
2017-05-06 11:31:22.149532: step 10200, loss = 1.66 (2201.4 examples/sec; 0.058 sec/batch)
Summary of Network 1:
conv1Shape: [5, 5, 3, 6]
pool1ksize: [1, 4, 4, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 6, 12]
pool2ksize: [1, 3, 3, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 768  | local3OutputDepth: 768
local4InputDepth: 768  | local4OutputDepth: 16
softmax_linearInput: 16

Number of hidden parameters of conv1: 450
Number of hidden parameters of conv1Biases: 6
Number of hidden parameters of conv2: 1800
Number of hidden parameters of conv2Biases: 12
Number of hidden parameters of local3: 589824
Number of hidden parameters of local3Biases: 768
Number of hidden parameters of local4: 12288
Number of hidden parameters of local4Biases: 16
Number of hidden parameters of softmax: 160
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 605334
2017-05-06 11:31:33.422551: step 10300, loss = 1.63 (2246.8 examples/sec; 0.057 sec/batch)
2017-05-06 11:31:44.660101: step 10400, loss = 1.78 (2339.6 examples/sec; 0.055 sec/batch)
Summary of Network 1:
conv1Shape: [5, 5, 3, 4]
pool1ksize: [1, 4, 4, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 4, 12]
pool2ksize: [1, 3, 3, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 768  | local3OutputDepth: 768
local4InputDepth: 768  | local4OutputDepth: 16
softmax_linearInput: 16

Number of hidden parameters of conv1: 300
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 1200
Number of hidden parameters of conv2Biases: 12
Number of hidden parameters of local3: 589824
Number of hidden parameters of local3Biases: 768
Number of hidden parameters of local4: 12288
Number of hidden parameters of local4Biases: 16
Number of hidden parameters of softmax: 160
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 604582
Summary of Network 1:
conv1Shape: [5, 5, 3, 2]
pool1ksize: [1, 4, 4, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 2, 12]
pool2ksize: [1, 3, 3, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 768  | local3OutputDepth: 768
local4InputDepth: 768  | local4OutputDepth: 16
softmax_linearInput: 16

Number of hidden parameters of conv1: 150
Number of hidden parameters of conv1Biases: 2
Number of hidden parameters of conv2: 600
Number of hidden parameters of conv2Biases: 12
Number of hidden parameters of local3: 589824
Number of hidden parameters of local3Biases: 768
Number of hidden parameters of local4: 12288
Number of hidden parameters of local4Biases: 16
Number of hidden parameters of softmax: 160
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 603830
2017-05-06 11:31:55.970747: step 10500, loss = 1.76 (2024.1 examples/sec; 0.063 sec/batch)
2017-05-06 11:32:07.219740: step 10600, loss = 1.46 (2361.2 examples/sec; 0.054 sec/batch)
Summary of Network 1:
conv1Shape: [5, 5, 3, 2]
pool1ksize: [1, 4, 4, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 2, 12]
pool2ksize: [1, 7, 7, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 768  | local3OutputDepth: 768
local4InputDepth: 768  | local4OutputDepth: 16
softmax_linearInput: 16

Number of hidden parameters of conv1: 150
Number of hidden parameters of conv1Biases: 2
Number of hidden parameters of conv2: 600
Number of hidden parameters of conv2Biases: 12
Number of hidden parameters of local3: 589824
Number of hidden parameters of local3Biases: 768
Number of hidden parameters of local4: 12288
Number of hidden parameters of local4Biases: 16
Number of hidden parameters of softmax: 160
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 603830
2017-05-06 11:32:18.494326: step 10700, loss = 1.62 (2453.0 examples/sec; 0.052 sec/batch)
2017-05-06 11:32:29.749217: step 10800, loss = 1.53 (2246.1 examples/sec; 0.057 sec/batch)
2017-05-06 11:32:41.010494: step 10900, loss = 1.50 (2224.0 examples/sec; 0.058 sec/batch)
Summary of Network 1:
conv1Shape: [5, 5, 3, 8]
pool1ksize: [1, 4, 4, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 8, 10]
pool2ksize: [1, 3, 3, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 16
softmax_linearInput: 16

Number of hidden parameters of conv1: 600
Number of hidden parameters of conv1Biases: 8
Number of hidden parameters of conv2: 2000
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 10240
Number of hidden parameters of local4Biases: 16
Number of hidden parameters of softmax: 160
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 423284
2017-05-06 11:32:52.306910: step 11000, loss = 1.64 (2428.8 examples/sec; 0.053 sec/batch)
2017-05-06 11:33:04.864229: step 11100, loss = 1.57 (2199.1 examples/sec; 0.058 sec/batch)
2017-05-06 11:33:16.176608: step 11200, loss = 1.69 (2312.7 examples/sec; 0.055 sec/batch)
2017-05-06 11:33:27.322474: step 11300, loss = 1.59 (2040.3 examples/sec; 0.063 sec/batch)
2017-05-06 11:33:38.565278: step 11400, loss = 1.48 (2329.6 examples/sec; 0.055 sec/batch)
2017-05-06 11:33:49.862310: step 11500, loss = 1.74 (2315.1 examples/sec; 0.055 sec/batch)
2017-05-06 11:34:01.180982: step 11600, loss = 1.50 (2434.1 examples/sec; 0.053 sec/batch)
2017-05-06 11:34:12.507859: step 11700, loss = 1.65 (2286.7 examples/sec; 0.056 sec/batch)
2017-05-06 11:34:23.834529: step 11800, loss = 1.34 (2240.2 examples/sec; 0.057 sec/batch)
2017-05-06 11:34:35.140796: step 11900, loss = 1.83 (2338.3 examples/sec; 0.055 sec/batch)
2017-05-06 11:34:46.510894: step 12000, loss = 1.73 (1958.8 examples/sec; 0.065 sec/batch)
2017-05-06 11:34:59.173209: step 12100, loss = 1.48 (2224.2 examples/sec; 0.058 sec/batch)
2017-05-06 11:35:10.501982: step 12200, loss = 1.49 (2388.9 examples/sec; 0.054 sec/batch)
2017-05-06 11:35:21.837428: step 12300, loss = 1.55 (2280.5 examples/sec; 0.056 sec/batch)
2017-05-06 11:35:33.182265: step 12400, loss = 1.73 (2031.0 examples/sec; 0.063 sec/batch)
2017-05-06 11:35:44.536733: step 12500, loss = 1.78 (2180.7 examples/sec; 0.059 sec/batch)
2017-05-06 11:35:55.932820: step 12600, loss = 1.68 (2322.0 examples/sec; 0.055 sec/batch)
2017-05-06 11:36:07.276025: step 12700, loss = 1.83 (2211.2 examples/sec; 0.058 sec/batch)
2017-05-06 11:36:18.585678: step 12800, loss = 1.70 (2284.5 examples/sec; 0.056 sec/batch)
2017-05-06 11:36:29.942189: step 12900, loss = 1.45 (2086.9 examples/sec; 0.061 sec/batch)
2017-05-06 11:36:41.302181: step 13000, loss = 1.52 (2388.8 examples/sec; 0.054 sec/batch)
2017-05-06 11:36:53.969401: step 13100, loss = 1.55 (2285.4 examples/sec; 0.056 sec/batch)
2017-05-06 11:37:05.325023: step 13200, loss = 1.63 (2280.0 examples/sec; 0.056 sec/batch)
2017-05-06 11:37:16.672731: step 13300, loss = 1.50 (2078.1 examples/sec; 0.062 sec/batch)
2017-05-06 11:37:28.077146: step 13400, loss = 1.42 (2278.9 examples/sec; 0.056 sec/batch)
2017-05-06 11:37:39.461381: step 13500, loss = 1.66 (2129.9 examples/sec; 0.060 sec/batch)
2017-05-06 11:37:50.813651: step 13600, loss = 1.57 (2217.7 examples/sec; 0.058 sec/batch)
2017-05-06 11:38:02.156377: step 13700, loss = 1.63 (2286.1 examples/sec; 0.056 sec/batch)
2017-05-06 11:38:13.531912: step 13800, loss = 1.58 (2194.7 examples/sec; 0.058 sec/batch)
2017-05-06 11:38:24.913519: step 13900, loss = 1.61 (2375.1 examples/sec; 0.054 sec/batch)
2017-05-06 11:38:36.285691: step 14000, loss = 1.50 (2264.7 examples/sec; 0.057 sec/batch)
2017-05-06 11:38:49.115098: step 14100, loss = 1.79 (2434.0 examples/sec; 0.053 sec/batch)
2017-05-06 11:39:00.448404: step 14200, loss = 1.56 (2255.9 examples/sec; 0.057 sec/batch)
2017-05-06 11:39:11.852896: step 14300, loss = 1.96 (2195.1 examples/sec; 0.058 sec/batch)
2017-05-06 11:39:23.271685: step 14400, loss = 1.62 (2291.0 examples/sec; 0.056 sec/batch)
2017-05-06 11:39:34.650600: step 14500, loss = 1.89 (2091.6 examples/sec; 0.061 sec/batch)
2017-05-06 11:39:46.007383: step 14600, loss = 1.34 (2393.6 examples/sec; 0.053 sec/batch)
2017-05-06 11:39:57.436240: step 14700, loss = 1.54 (2320.7 examples/sec; 0.055 sec/batch)
2017-05-06 11:40:08.788409: step 14800, loss = 1.90 (2312.4 examples/sec; 0.055 sec/batch)
2017-05-06 11:40:20.220081: step 14900, loss = 1.75 (2334.4 examples/sec; 0.055 sec/batch)
Evaluation results:
2017-05-06 11:40:38.253048: Total Predictions = 10112
2017-05-06 11:40:38.260270: Correct Predictions = 6705
2017-05-06 11:40:38.268610: Wrong Predictions = 3407
2017-05-06 11:40:38.275789: precision @ 1 = 0.663
2017-05-06 11:40:38.341099: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-06 11:50:13.041539: Running on server...
The experiment details:
max_steps = 15000 log_frequency = 100 num_gpus = 2
Evaluation results:
2017-05-06 11:50:18.362908: Total Predictions = 10112
2017-05-06 11:50:18.370946: Correct Predictions = 6705
2017-05-06 11:50:18.379221: Wrong Predictions = 3407
2017-05-06 11:50:18.387966: precision @ 1 = 0.663
2017-05-06 11:50:18.456702: DONE
########################################################################################
########################################################################################
########################################################################################
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
Summary of Network 1:
conv1Shape: [5, 5, 3, 8]
pool1ksize: [1, 4, 4, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 8, 10]
pool2ksize: [1, 3, 3, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 16
softmax_linearInput: 16

Number of hidden parameters of conv1: 600
Number of hidden parameters of conv1Biases: 8
Number of hidden parameters of conv2: 2000
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 10240
Number of hidden parameters of local4Biases: 16
Number of hidden parameters of softmax: 160
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 423284
Summary of Network 1:
conv1Shape: [5, 5, 3, 8]
pool1ksize: [1, 4, 4, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 8, 10]
pool2ksize: [1, 3, 3, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 16
softmax_linearInput: 16

Number of hidden parameters of conv1: 600
Number of hidden parameters of conv1Biases: 8
Number of hidden parameters of conv2: 2000
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 10240
Number of hidden parameters of local4Biases: 16
Number of hidden parameters of softmax: 160
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 423284
########################################################################################
########################################################################################
########################################################################################
2017-05-06 12:28:39.121950: Running on server...
The experiment details:
max_steps = 100 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Summary of Network 1:
conv1Shape: [5, 5, 3, 8]
pool1ksize: [1, 4, 4, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [5, 5, 8, 10]
pool2ksize: [1, 3, 3, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 16
softmax_linearInput: 16

Number of hidden parameters of conv1: 600
Number of hidden parameters of conv1Biases: 8
Number of hidden parameters of conv2: 2000
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 10240
Number of hidden parameters of local4Biases: 16
Number of hidden parameters of softmax: 160
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 423284
########################################################################################
########################################################################################
########################################################################################
2017-05-06 12:32:38.181842: Running on server...
The experiment details:
max_steps = 100 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 12:33:02.958534: step 0, loss = 5.64 (15.5 examples/sec; 8.245 sec/batch)
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Evaluation results:
2017-05-06 12:33:43.425917: Total Predictions = 10112
2017-05-06 12:33:43.434316: Correct Predictions = 2567
2017-05-06 12:33:43.441462: Wrong Predictions = 7545
2017-05-06 12:33:43.448371: precision @ 1 = 0.254
2017-05-06 12:33:43.646723: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-06 12:36:27.237204: Running on server...
The experiment details:
max_steps = 100 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Evaluation results:
2017-05-06 12:36:53.291183: Total Predictions = 10112
2017-05-06 12:36:53.300647: Correct Predictions = 2604
2017-05-06 12:36:53.310898: Wrong Predictions = 7508
2017-05-06 12:36:53.319626: precision @ 1 = 0.258
2017-05-06 12:36:53.517710: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-06 12:48:06.414714: Running on server...
The experiment details:
network = 1 max_steps = 30000 log_frequency = 100 num_gpus = 2
########################################################################################
########################################################################################
########################################################################################
2017-05-06 13:05:53.445453: Running on server...
The experiment details:
network = 1 max_steps = 30000 log_frequency = 100 num_gpus = 2
########################################################################################
########################################################################################
########################################################################################
2017-05-06 13:09:09.304815: Running on server...
The experiment details:
network = 1 max_steps = 30000 log_frequency = 100 num_gpus = 2
2017-05-06 13:09:10.001502: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-06 13:12:55.677389: Running on server...
The experiment details:
network = 1 max_steps = 30000 log_frequency = 100 num_gpus = 2
########################################################################################
########################################################################################
########################################################################################
2017-05-06 13:13:40.332596: Running on server...
The experiment details:
network = 1 max_steps = 30000 log_frequency = 100 num_gpus = 2
########################################################################################
########################################################################################
########################################################################################
2017-05-06 13:16:16.579241: Running on server...
The experiment details:
network = 1 max_steps = 100 log_frequency = 100 num_gpus = 2
########################################################################################
########################################################################################
########################################################################################
2017-05-06 13:17:05.862358: Running on server...
The experiment details:
network = 1 max_steps = 100 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-06 13:17:27.078381: step 0, loss = 5.72 (16.7 examples/sec; 7.685 sec/batch)
Evaluation results:
2017-05-06 13:17:48.124538: Total Predictions = 10112
2017-05-06 13:17:48.131970: Correct Predictions = 1121
2017-05-06 13:17:48.139130: Wrong Predictions = 8991
2017-05-06 13:17:48.147244: precision @ 1 = 0.111
2017-05-06 13:17:48.233311: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-18 01:59:50.694028: Running on UCSC:citrisdense...
The experiment details:
network = 1 max_steps = 400 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
########################################################################################
########################################################################################
########################################################################################
2017-05-18 02:04:51.045728: Running on UCSC:citrisdense...
The experiment details:
network = 1 max_steps = 400 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
########################################################################################
########################################################################################
########################################################################################
2017-05-18 02:06:06.056712: Running on UCSC:citrisdense...
The experiment details:
network = 1 max_steps = 400 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-18 02:06:32.361476: step 0, loss = 5.72 (14.5 examples/sec; 8.808 sec/batch)
2017-05-18 02:06:44.595786: step 100, loss = 5.64 (2388.5 examples/sec; 0.054 sec/batch)
2017-05-18 02:06:55.538897: step 200, loss = 5.20 (2499.9 examples/sec; 0.051 sec/batch)
2017-05-18 02:07:06.442470: step 300, loss = 4.74 (2395.1 examples/sec; 0.053 sec/batch)
Evaluation results:
2017-05-18 02:07:21.309524: Total Predictions = 1024
2017-05-18 02:07:21.317232: Correct Predictions = 124
2017-05-18 02:07:21.324630: Wrong Predictions = 900
2017-05-18 02:07:21.333994: precision @ 1 = 0.121
2017-05-18 02:07:21.642897: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-27 23:28:30.312858: Running on MSI...
########################################################################################
########################################################################################
########################################################################################
2017-05-27 23:29:10.949356: Running on MSI...
########################################################################################
########################################################################################
########################################################################################
2017-05-27 23:30:47.880930: Running on MSI...
########################################################################################
########################################################################################
########################################################################################
2017-05-27 23:36:56.685919: Running on MSI...
########################################################################################
########################################################################################
########################################################################################
2017-05-27 23:39:19.089753: Running on MSI...
########################################################################################
########################################################################################
########################################################################################
2017-05-27 23:39:51.082513: Running on MSI...
########################################################################################
########################################################################################
########################################################################################
2017-05-27 23:40:20.555021: Running on MSI...
Epoch: 1 Learning rate: 1.000
0.004 perplexity: 5728.078 speed: 5866 wps
0.104 perplexity: 852.251 speed: 11826 wps
########################################################################################
########################################################################################
########################################################################################
2017-05-27 13:49:33.318676: Running on UCSC:citrisdense...
Epoch: 1 Learning rate: 1.000
0.004 perplexity: 5503.288 speed: 3803 wps
0.104 perplexity: 838.582 speed: 8659 wps
########################################################################################
########################################################################################
########################################################################################
2017-05-27 13:51:49.955712: Running on UCSC:citrisdense...
Epoch: 1 Learning rate: 1.000
0.004 perplexity: 5363.747 speed: 5049 wps
0.104 perplexity: 829.547 speed: 8681 wps
0.204 perplexity: 619.735 speed: 8851 wps
0.304 perplexity: 502.695 speed: 8894 wps
0.404 perplexity: 434.809 speed: 8972 wps
0.504 perplexity: 390.058 speed: 8980 wps
0.604 perplexity: 351.503 speed: 8995 wps
0.703 perplexity: 325.113 speed: 9009 wps
0.803 perplexity: 304.009 speed: 9015 wps
0.903 perplexity: 284.650 speed: 9011 wps
Epoch: 1 Train Perplexity: 270.147
Epoch: 1 Valid Perplexity: 180.951
Epoch: 2 Learning rate: 1.000
0.004 perplexity: 215.232 speed: 8673 wps
0.104 perplexity: 151.138 speed: 9052 wps
0.204 perplexity: 158.347 speed: 9049 wps
0.304 perplexity: 153.429 speed: 9075 wps
0.404 perplexity: 150.500 speed: 9066 wps
0.504 perplexity: 148.092 speed: 9057 wps
0.604 perplexity: 143.459 speed: 9054 wps
0.703 perplexity: 141.293 speed: 9064 wps
0.803 perplexity: 139.303 speed: 9065 wps
0.903 perplexity: 135.654 speed: 9068 wps
Epoch: 2 Train Perplexity: 133.605
Epoch: 2 Valid Perplexity: 144.421
Epoch: 3 Learning rate: 1.000
0.004 perplexity: 146.811 speed: 9037 wps
0.104 perplexity: 104.901 speed: 9174 wps
0.204 perplexity: 114.117 speed: 9147 wps
0.304 perplexity: 111.392 speed: 9152 wps
0.404 perplexity: 110.443 speed: 9157 wps
0.504 perplexity: 109.657 speed: 9137 wps
0.604 perplexity: 107.104 speed: 9120 wps
0.703 perplexity: 106.469 speed: 9104 wps
0.803 perplexity: 105.832 speed: 9102 wps
0.903 perplexity: 103.590 speed: 9102 wps
Epoch: 3 Train Perplexity: 102.659
Epoch: 3 Valid Perplexity: 132.945
Epoch: 4 Learning rate: 1.000
0.004 perplexity: 115.831 speed: 9006 wps
0.104 perplexity: 84.987 speed: 8990 wps
0.204 perplexity: 93.663 speed: 9039 wps
0.304 perplexity: 91.546 speed: 9070 wps
0.404 perplexity: 91.079 speed: 9073 wps
0.504 perplexity: 90.718 speed: 9079 wps
0.604 perplexity: 88.938 speed: 9092 wps
0.703 perplexity: 88.771 speed: 9092 wps
0.803 perplexity: 88.503 speed: 9088 wps
0.903 perplexity: 86.872 speed: 9086 wps
Epoch: 4 Train Perplexity: 86.340
Epoch: 4 Valid Perplexity: 128.697
Epoch: 5 Learning rate: 0.500
0.004 perplexity: 99.755 speed: 8833 wps
0.104 perplexity: 71.262 speed: 9058 wps
0.204 perplexity: 77.369 speed: 9076 wps
0.304 perplexity: 74.561 speed: 9052 wps
0.404 perplexity: 73.493 speed: 9043 wps
0.504 perplexity: 72.581 speed: 9054 wps
0.604 perplexity: 70.481 speed: 9053 wps
0.703 perplexity: 69.723 speed: 9071 wps
0.803 perplexity: 68.897 speed: 9062 wps
0.903 perplexity: 67.017 speed: 9066 wps
Epoch: 5 Train Perplexity: 66.031
Epoch: 5 Valid Perplexity: 119.970
Epoch: 6 Learning rate: 0.250
0.004 perplexity: 81.798 speed: 9358 wps
0.104 perplexity: 58.748 speed: 8922 wps
0.204 perplexity: 64.080 speed: 8982 wps
0.304 perplexity: 61.629 speed: 8991 wps
0.404 perplexity: 60.671 speed: 8991 wps
0.504 perplexity: 59.825 speed: 9000 wps
0.604 perplexity: 57.981 speed: 8998 wps
0.703 perplexity: 57.255 speed: 9000 wps
0.803 perplexity: 56.444 speed: 8998 wps
0.903 perplexity: 54.736 speed: 9008 wps
Epoch: 6 Train Perplexity: 53.773
Epoch: 6 Valid Perplexity: 119.478
Epoch: 7 Learning rate: 0.125
0.004 perplexity: 72.464 speed: 9118 wps
0.104 perplexity: 52.191 speed: 9012 wps
0.204 perplexity: 57.000 speed: 9105 wps
0.304 perplexity: 54.785 speed: 9087 wps
0.404 perplexity: 53.912 speed: 9096 wps
0.504 perplexity: 53.114 speed: 9098 wps
0.604 perplexity: 51.449 speed: 9083 wps
0.703 perplexity: 50.756 speed: 9091 wps
0.803 perplexity: 49.972 speed: 9077 wps
0.903 perplexity: 48.386 speed: 9069 wps
Epoch: 7 Train Perplexity: 47.465
Epoch: 7 Valid Perplexity: 120.416
Epoch: 8 Learning rate: 0.062
0.004 perplexity: 67.817 speed: 9020 wps
0.104 perplexity: 48.815 speed: 8916 wps
0.204 perplexity: 53.381 speed: 9008 wps
0.304 perplexity: 51.296 speed: 9049 wps
0.404 perplexity: 50.481 speed: 9075 wps
0.504 perplexity: 49.715 speed: 9070 wps
0.604 perplexity: 48.137 speed: 9074 wps
0.703 perplexity: 47.457 speed: 9070 wps
0.803 perplexity: 46.687 speed: 9064 wps
0.903 perplexity: 45.167 speed: 9103 wps
Epoch: 8 Train Perplexity: 44.271
Epoch: 8 Valid Perplexity: 121.109
Epoch: 9 Learning rate: 0.031
0.004 perplexity: 65.377 speed: 9239 wps
0.104 perplexity: 47.016 speed: 9042 wps
0.204 perplexity: 51.482 speed: 9043 wps
0.304 perplexity: 49.461 speed: 9041 wps
0.404 perplexity: 48.677 speed: 9083 wps
0.504 perplexity: 47.934 speed: 9091 wps
0.604 perplexity: 46.401 speed: 9092 wps
0.703 perplexity: 45.733 speed: 9090 wps
0.803 perplexity: 44.971 speed: 9092 wps
0.903 perplexity: 43.478 speed: 9090 wps
Epoch: 9 Train Perplexity: 42.597
Epoch: 9 Valid Perplexity: 121.323
Epoch: 10 Learning rate: 0.016
0.004 perplexity: 64.007 speed: 8883 wps
0.104 perplexity: 46.029 speed: 9076 wps
0.204 perplexity: 50.451 speed: 9112 wps
0.304 perplexity: 48.465 speed: 9115 wps
0.404 perplexity: 47.691 speed: 9087 wps
0.504 perplexity: 46.957 speed: 9085 wps
0.604 perplexity: 45.446 speed: 9086 wps
0.703 perplexity: 44.786 speed: 9092 wps
0.803 perplexity: 44.033 speed: 9085 wps
0.903 perplexity: 42.554 speed: 9094 wps
Epoch: 10 Train Perplexity: 41.681
Epoch: 10 Valid Perplexity: 121.069
Epoch: 11 Learning rate: 0.008
0.004 perplexity: 63.270 speed: 9058 wps
0.104 perplexity: 45.464 speed: 8955 wps
0.204 perplexity: 49.865 speed: 9048 wps
0.304 perplexity: 47.904 speed: 9058 wps
0.404 perplexity: 47.138 speed: 9045 wps
0.504 perplexity: 46.413 speed: 9043 wps
0.604 perplexity: 44.914 speed: 9049 wps
0.703 perplexity: 44.257 speed: 9033 wps
0.803 perplexity: 43.511 speed: 9042 wps
0.903 perplexity: 42.044 speed: 9044 wps
Epoch: 11 Train Perplexity: 41.176
Epoch: 11 Valid Perplexity: 120.615
Epoch: 12 Learning rate: 0.004
0.004 perplexity: 62.825 speed: 9344 wps
0.104 perplexity: 45.127 speed: 9089 wps
0.204 perplexity: 49.516 speed: 9052 wps
0.304 perplexity: 47.577 speed: 9057 wps
0.404 perplexity: 46.822 speed: 9102 wps
0.504 perplexity: 46.105 speed: 9105 wps
0.604 perplexity: 44.616 speed: 9115 wps
0.703 perplexity: 43.963 speed: 9113 wps
0.803 perplexity: 43.221 speed: 9126 wps
0.903 perplexity: 41.762 speed: 9120 wps
Epoch: 12 Train Perplexity: 40.899
Epoch: 12 Valid Perplexity: 120.236
Epoch: 13 Learning rate: 0.002
0.004 perplexity: 62.547 speed: 9115 wps
0.104 perplexity: 44.929 speed: 9011 wps
0.204 perplexity: 49.312 speed: 9048 wps
0.304 perplexity: 47.389 speed: 9036 wps
0.404 perplexity: 46.643 speed: 9076 wps
0.504 perplexity: 45.934 speed: 9124 wps
0.604 perplexity: 44.452 speed: 9139 wps
0.703 perplexity: 43.803 speed: 9134 wps
0.803 perplexity: 43.064 speed: 9169 wps
0.903 perplexity: 41.610 speed: 9174 wps
Epoch: 13 Train Perplexity: 40.750
Epoch: 13 Valid Perplexity: 120.019
Test Perplexity: 114.096
2017-05-27 14:17:08.520197: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-27 15:02:49.766511: Running on UCSC:citrisdense...
########################################################################################
########################################################################################
########################################################################################
2017-05-27 15:08:20.415600: Running on UCSC:citrisdense...
########################################################################################
########################################################################################
########################################################################################
2017-05-27 15:12:03.393815: Running on UCSC:citrisdense...
########################################################################################
########################################################################################
########################################################################################
2017-05-27 15:18:25.023333: Running on UCSC:citrisdense...
########################################################################################
########################################################################################
########################################################################################
2017-05-27 15:24:26.373821: Running on UCSC:citrisdense...
########################################################################################
########################################################################################
########################################################################################
2017-05-27 15:25:38.697510: Running on UCSC:citrisdense...
Distinct terms: 10000
########################################################################################
########################################################################################
########################################################################################
2017-05-27 15:29:22.506239: Running on UCSC:citrisdense...
Distinct terms: 10000
Seed: make america
Sample: weil injured perception demand joan unwelcome recover prepaid arrest doubtful waves pentagon sport dillon procedural
Epoch: 1 Learning rate: 1.000
0.004 perplexity: 9393.383 speed: 5341 wps
0.104 perplexity: 836.498 speed: 8240 wps
0.204 perplexity: 675.272 speed: 8312 wps
0.304 perplexity: 582.335 speed: 8319 wps
0.404 perplexity: 526.670 speed: 8342 wps
0.504 perplexity: 487.815 speed: 8371 wps
0.604 perplexity: 450.514 speed: 8387 wps
0.703 perplexity: 423.487 speed: 8414 wps
0.803 perplexity: 401.410 speed: 8444 wps
0.903 perplexity: 380.629 speed: 8463 wps
Epoch: 1 Train Perplexity: 364.279
Epoch: 1 Valid Perplexity: 243.066
########################################################################################
########################################################################################
########################################################################################
2017-05-27 15:33:27.404889: Running on UCSC:citrisdense...
Distinct terms: 10000
Seed: make america
Sample: rewrite limitations mail embryo inability warsaw counterpart attend tool gonzalez greenberg consultants necessity monitor o'connell
Epoch: 1 Learning rate: 1.000
0.004 perplexity: 9151.463 speed: 5278 wps
0.104 perplexity: 833.861 speed: 8273 wps
0.204 perplexity: 678.314 speed: 8333 wps
0.304 perplexity: 583.978 speed: 8394 wps
0.404 perplexity: 526.929 speed: 8425 wps
0.504 perplexity: 487.240 speed: 8448 wps
0.604 perplexity: 449.292 speed: 8466 wps
0.703 perplexity: 421.885 speed: 8489 wps
0.803 perplexity: 399.916 speed: 8511 wps
0.903 perplexity: 379.335 speed: 8521 wps
Epoch: 1 Train Perplexity: 363.320
Epoch: 1 Valid Perplexity: 246.874
Seed: make america
Sample: owned N complete a be assets can cut less than slate retirement bid it 's
Epoch: 2 Learning rate: 1.000
0.004 perplexity: 298.189 speed: 7559 wps
0.104 perplexity: 215.222 speed: 8708 wps
0.204 perplexity: 221.630 speed: 8642 wps
0.304 perplexity: 216.936 speed: 8581 wps
0.404 perplexity: 213.902 speed: 8561 wps
0.504 perplexity: 211.912 speed: 8540 wps
0.604 perplexity: 206.204 speed: 8533 wps
0.703 perplexity: 202.763 speed: 8521 wps
0.803 perplexity: 199.632 speed: 8505 wps
0.903 perplexity: 195.413 speed: 8496 wps
Epoch: 2 Train Perplexity: 192.327
Epoch: 2 Valid Perplexity: 192.389
Seed: make america
Sample: medical executives counts to ensure its directors against a N increase before whatever he knowledgeable
Epoch: 3 Learning rate: 1.000
0.004 perplexity: 208.702 speed: 7699 wps
0.104 perplexity: 151.557 speed: 8422 wps
0.204 perplexity: 158.649 speed: 8459 wps
0.304 perplexity: 156.665 speed: 8457 wps
0.404 perplexity: 155.977 speed: 8503 wps
0.504 perplexity: 155.757 speed: 8517 wps
0.604 perplexity: 152.524 speed: 8515 wps
0.703 perplexity: 151.151 speed: 8524 wps
0.803 perplexity: 149.778 speed: 8523 wps
0.903 perplexity: 147.289 speed: 8537 wps
Epoch: 3 Train Perplexity: 145.710
Epoch: 3 Valid Perplexity: 170.727
Seed: make america
Sample: says but us are further <unk> <unk> by a former louis executive officer that has
Epoch: 4 Learning rate: 1.000
0.004 perplexity: 171.577 speed: 7762 wps
0.104 perplexity: 121.920 speed: 8579 wps
0.204 perplexity: 127.866 speed: 8585 wps
0.304 perplexity: 126.336 speed: 8610 wps
0.404 perplexity: 126.130 speed: 8609 wps
0.504 perplexity: 126.413 speed: 8622 wps
0.604 perplexity: 124.205 speed: 8619 wps
0.703 perplexity: 123.583 speed: 8653 wps
0.803 perplexity: 122.920 speed: 8669 wps
0.903 perplexity: 121.137 speed: 8668 wps
Epoch: 4 Train Perplexity: 120.149
Epoch: 4 Valid Perplexity: 161.471
Seed: make america
Sample: in recent accountability <eos> i thought the <unk> <unk> division a survey called mr. lawson
Epoch: 5 Learning rate: 1.000
0.004 perplexity: 142.291 speed: 7501 wps
0.104 perplexity: 103.060 speed: 8448 wps
0.204 perplexity: 108.203 speed: 8463 wps
0.304 perplexity: 106.758 speed: 8468 wps
0.404 perplexity: 106.730 speed: 8466 wps
0.504 perplexity: 107.154 speed: 8475 wps
0.604 perplexity: 105.488 speed: 8471 wps
0.703 perplexity: 105.162 speed: 8478 wps
0.803 perplexity: 104.859 speed: 8476 wps
0.903 perplexity: 103.466 speed: 8476 wps
Epoch: 5 Train Perplexity: 102.781
Epoch: 5 Valid Perplexity: 157.731
Seed: make america
Sample: a investigation in the board is much <unk> <eos> japanese people adopt public fraud to
Epoch: 6 Learning rate: 0.500
0.004 perplexity: 123.306 speed: 7474 wps
0.104 perplexity: 88.800 speed: 8412 wps
0.204 perplexity: 93.711 speed: 8488 wps
0.304 perplexity: 92.752 speed: 8512 wps
0.404 perplexity: 92.764 speed: 8517 wps
0.504 perplexity: 93.201 speed: 8534 wps
0.604 perplexity: 91.873 speed: 8539 wps
0.703 perplexity: 91.736 speed: 8557 wps
0.803 perplexity: 91.628 speed: 8570 wps
0.903 perplexity: 90.515 speed: 8574 wps
Epoch: 6 Train Perplexity: 90.013
Epoch: 6 Valid Perplexity: 157.709
Seed: make america
Sample: a positive cease-fire <eos> the phenomenon for coffee or prepared money was pushed seven times
Epoch: 7 Learning rate: 0.250
0.004 perplexity: 107.971 speed: 7553 wps
0.104 perplexity: 79.064 speed: 8599 wps
0.204 perplexity: 83.158 speed: 8657 wps
0.304 perplexity: 82.247 speed: 8683 wps
0.404 perplexity: 82.337 speed: 8679 wps
0.504 perplexity: 82.741 speed: 8670 wps
0.604 perplexity: 81.625 speed: 8659 wps
0.703 perplexity: 81.650 speed: 8653 wps
0.803 perplexity: 81.610 speed: 8642 wps
0.903 perplexity: 80.674 speed: 8630 wps
Epoch: 7 Train Perplexity: 80.274
Epoch: 7 Valid Perplexity: 160.847
Seed: make america
Sample: <eos> where maybe after those who hints <unk> since creating clear pro-life relief and companies
Epoch: 8 Learning rate: 0.125
0.004 perplexity: 107.971 speed: 7488 wps
0.104 perplexity: 79.149 speed: 8414 wps
0.204 perplexity: 83.270 speed: 8462 wps
0.304 perplexity: 82.372 speed: 8448 wps
0.404 perplexity: 82.471 speed: 8441 wps
0.504 perplexity: 82.874 speed: 8447 wps
0.604 perplexity: 81.759 speed: 8452 wps
0.703 perplexity: 81.785 speed: 8467 wps
0.803 perplexity: 81.748 speed: 8479 wps
0.903 perplexity: 80.815 speed: 8478 wps
Epoch: 8 Train Perplexity: 80.418
Epoch: 8 Valid Perplexity: 160.717
Seed: make america
Sample: resembles companies spent further interest <eos> after japanese story a report in agriculture concern ended
Epoch: 9 Learning rate: 0.062
0.004 perplexity: 107.971 speed: 7555 wps
0.104 perplexity: 79.013 speed: 8395 wps
0.204 perplexity: 83.131 speed: 8434 wps
0.304 perplexity: 82.220 speed: 8421 wps
0.404 perplexity: 82.313 speed: 8445 wps
0.504 perplexity: 82.722 speed: 8449 wps
0.604 perplexity: 81.610 speed: 8457 wps
0.703 perplexity: 81.636 speed: 8464 wps
0.803 perplexity: 81.599 speed: 8467 wps
0.903 perplexity: 80.663 speed: 8474 wps
Epoch: 9 Train Perplexity: 80.265
Epoch: 9 Valid Perplexity: 160.787
Seed: make america
Sample: losing touch <eos> an expensive takeover of health care during its circuit have been five
Epoch: 10 Learning rate: 0.031
0.004 perplexity: 107.971 speed: 7557 wps
0.104 perplexity: 79.065 speed: 8428 wps
0.204 perplexity: 83.168 speed: 8513 wps
0.304 perplexity: 82.257 speed: 8530 wps
0.404 perplexity: 82.349 speed: 8546 wps
0.504 perplexity: 82.760 speed: 8542 wps
0.604 perplexity: 81.657 speed: 8529 wps
0.703 perplexity: 81.681 speed: 8529 wps
0.803 perplexity: 81.643 speed: 8521 wps
0.903 perplexity: 80.703 speed: 8510 wps
Epoch: 10 Train Perplexity: 80.305
Epoch: 10 Valid Perplexity: 160.771
Seed: make america
Sample: <eos> frank bounced stuck ms. greenspan would report much worse away 's la earthquake and
Epoch: 11 Learning rate: 0.016
0.004 perplexity: 107.971 speed: 7415 wps
0.104 perplexity: 79.154 speed: 8437 wps
0.204 perplexity: 83.287 speed: 8478 wps
0.304 perplexity: 82.383 speed: 8473 wps
0.404 perplexity: 82.479 speed: 8481 wps
0.504 perplexity: 82.881 speed: 8479 wps
0.604 perplexity: 81.764 speed: 8486 wps
0.703 perplexity: 81.789 speed: 8489 wps
0.803 perplexity: 81.750 speed: 8492 wps
0.903 perplexity: 80.816 speed: 8493 wps
Epoch: 11 Train Perplexity: 80.419
Epoch: 11 Valid Perplexity: 160.702
Seed: make america
Sample: to discuss us hurt through a <unk> <eos> mr. engelken says <unk> mark martin investors
Epoch: 12 Learning rate: 0.008
0.004 perplexity: 107.971 speed: 7576 wps
0.104 perplexity: 79.119 speed: 8552 wps
0.204 perplexity: 83.259 speed: 8570 wps
0.304 perplexity: 82.363 speed: 8574 wps
0.404 perplexity: 82.463 speed: 8550 wps
0.504 perplexity: 82.868 speed: 8552 wps
0.604 perplexity: 81.751 speed: 8593 wps
0.703 perplexity: 81.776 speed: 8589 wps
0.803 perplexity: 81.738 speed: 8581 wps
0.903 perplexity: 80.805 speed: 8581 wps
Epoch: 12 Train Perplexity: 80.407
Epoch: 12 Valid Perplexity: 160.665
Seed: make america
Sample: alan <unk> <eos> <unk> union the <unk> a southern area citing eurocom both spending have
Epoch: 13 Learning rate: 0.004
0.004 perplexity: 107.971 speed: 7434 wps
0.104 perplexity: 79.047 speed: 8454 wps
0.204 perplexity: 83.148 speed: 8482 wps
0.304 perplexity: 82.233 speed: 8498 wps
0.404 perplexity: 82.322 speed: 8528 wps
0.504 perplexity: 82.730 speed: 8543 wps
0.604 perplexity: 81.617 speed: 8563 wps
0.703 perplexity: 81.642 speed: 8581 wps
0.803 perplexity: 81.604 speed: 8585 wps
0.903 perplexity: 80.667 speed: 8588 wps
Epoch: 13 Train Perplexity: 80.268
Epoch: 13 Valid Perplexity: 160.833
Test Perplexity: 145.585
2017-05-27 16:01:00.051721: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-28 05:45:10.051545: Running on UCSC:citrisdense...
########################################################################################
########################################################################################
########################################################################################
2017-05-28 05:46:42.359371: Running on UCSC:citrisdense...
Distinct terms: 10000
Seed: make america
Sample: author was an whole commissioner that is that someone owe ice how president bush europe
Epoch: 1 Learning rate: 1.000
0.004 perplexity: 107.971 speed: 5418 wps
0.104 perplexity: 79.009 speed: 8147 wps
0.204 perplexity: 83.136 speed: 8239 wps
0.304 perplexity: 82.225 speed: 8302 wps
0.404 perplexity: 82.314 speed: 8325 wps
0.504 perplexity: 82.723 speed: 8341 wps
0.604 perplexity: 81.611 speed: 8343 wps
0.703 perplexity: 81.637 speed: 8344 wps
0.803 perplexity: 81.598 speed: 8350 wps
0.903 perplexity: 80.662 speed: 8352 wps
Epoch: 1 Train Perplexity: 80.265
Epoch: 1 Valid Perplexity: 160.792
Test Perplexity: 147.238
2017-05-28 05:51:30.686410: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-28 06:08:30.828996: Running on UCSC:citrisdense...
Distinct terms: 10000
Seed: make america
Sample: republicans like an complex problem of his employer and perhaps did it in promising <eos>
2017-05-28 06:08:57.909233: Epoch: 1 Learning rate: 1.000
0.004 perplexity: 97.026 speed: 5336 wps
0.104 perplexity: 70.238 speed: 8266 wps
0.204 perplexity: 74.092 speed: 8363 wps
0.304 perplexity: 73.196 speed: 8404 wps
0.404 perplexity: 73.389 speed: 8434 wps
0.504 perplexity: 73.842 speed: 8448 wps
0.604 perplexity: 72.934 speed: 8459 wps
0.703 perplexity: 73.052 speed: 8467 wps
0.803 perplexity: 73.110 speed: 8469 wps
0.903 perplexity: 72.337 speed: 8467 wps
2017-05-28 06:10:47.723116: Epoch: 1 Train Perplexity: 72.052
2017-05-28 06:10:50.643146: Epoch: 1 Valid Perplexity: 164.668
2017-05-28 06:10:55.879130: Seed: make america
2017-05-28 06:10:55.900537: Sample: sitting under the last three months of a recapitalization attempt to serve as everybody increased
2017-05-28 06:13:18.322727: Test Perplexity: 149.562
2017-05-28 06:13:18.532228: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-28 06:28:35.879126: Running on UCSC:citrisdense...
Distinct terms: 10000
Seed: make america
Sample: share in the country because a computer operates <unk> a daily bull of health-care costs
2017-05-28 06:29:02.596082: Epoch: 1 Learning rate: 1.000
0.004 perplexity: 87.611 speed: 5111 wps
0.104 perplexity: 63.869 speed: 8260 wps
0.204 perplexity: 67.179 speed: 8426 wps
0.304 perplexity: 66.221 speed: 8486 wps
0.404 perplexity: 66.413 speed: 8515 wps
0.504 perplexity: 66.845 speed: 8537 wps
0.604 perplexity: 66.091 speed: 8555 wps
0.703 perplexity: 66.302 speed: 8556 wps
0.803 perplexity: 66.373 speed: 8559 wps
0.903 perplexity: 65.703 speed: 8556 wps
2017-05-28 06:30:51.225007: Epoch: 1 Train Perplexity: 65.476
2017-05-28 06:30:54.008596: Epoch: 1 Valid Perplexity: 170.930
2017-05-28 06:30:59.099851: Seed: make america
2017-05-28 06:30:59.112792: Sample: the fall <eos> a document is <unk> rather than i surely time as a market
2017-05-28 06:33:20.158489: Test Perplexity: 153.909
2017-05-28 06:33:20.260586: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-28 07:15:08.846967: Running on UCSC:citrisdense...
Distinct terms: 27486
Seed: gerek
Sample: Kim bu ey ya sonra anne eyler daha insan "Ne
2017-05-28 07:15:35.730909: Epoch: 1 Learning rate: 1.000
########################################################################################
########################################################################################
########################################################################################
2017-05-28 14:06:34.623498: Running on UCSC:citrisdense...
Distinct terms: 27486
########################################################################################
########################################################################################
########################################################################################
2017-05-28 14:09:10.429405: Running on UCSC:citrisdense...
Distinct terms: 27486
########################################################################################
########################################################################################
########################################################################################
2017-05-31 03:07:51.068680: Running on UCSC:citrisdense...
########################################################################################
########################################################################################
########################################################################################
2017-05-31 03:14:14.818603: Running on UCSC:citrisdense...
Distinct terms: 27486
########################################################################################
########################################################################################
########################################################################################
2017-05-31 03:16:54.076485: Running on UCSC:citrisdense...
Distinct terms: 27486
########################################################################################
########################################################################################
########################################################################################
2017-05-31 03:25:12.968904: Running on UCSC:citrisdense...
Distinct terms: 27486
Seed: gerek
Sample: yakn Park'na yakalanmt. kapatarak birikimleri <eos>tavrdaki <eos>30 <eos>veriyorum. birine varoluumun
2017-05-31 03:25:38.055029: Epoch: 1 Learning rate: 1.000
2017-05-31 03:25:39.017729: 0.054 perplexity: 27289.957 speed: 4668 wps
2017-05-31 03:25:40.186844: 0.151 perplexity: 19722.292 speed: 5493 wps
2017-05-31 03:25:41.370359: 0.247 perplexity: 15714.062 speed: 5705 wps
2017-05-31 03:25:42.550814: 0.344 perplexity: 14347.596 speed: 5809 wps
2017-05-31 03:25:43.707934: 0.441 perplexity: 13681.015 speed: 5894 wps
2017-05-31 03:25:44.884018: 0.538 perplexity: 13430.171 speed: 5933 wps
2017-05-31 03:25:46.027080: 0.634 perplexity: 13412.219 speed: 5986 wps
2017-05-31 03:25:47.177379: 0.731 perplexity: 13306.215 speed: 6021 wps
2017-05-31 03:25:48.336272: 0.828 perplexity: 13154.540 speed: 6042 wps
2017-05-31 03:25:49.492158: 0.925 perplexity: 13012.004 speed: 6061 wps
2017-05-31 03:25:50.357890: Epoch: 1 Train Perplexity: 12941.271
########################################################################################
########################################################################################
########################################################################################
2017-05-31 03:28:42.467471: Running on UCSC:citrisdense...
Distinct terms: 27486
Seed: gerek
Sample: <eos>geniletti. veriyorsunuz, kalmaz, biliyordu <eos>tabaklan <eos>137 <eos>seim iletiim <eos>merkezinde 108
2017-05-31 03:29:07.513043: Epoch: 1 Learning rate: 1.000
2017-05-31 03:29:08.508094: 0.054 perplexity: 27442.337 speed: 4555 wps
2017-05-31 03:29:09.672351: 0.151 perplexity: 19670.174 speed: 5446 wps
2017-05-31 03:29:10.832647: 0.247 perplexity: 15710.679 speed: 5714 wps
2017-05-31 03:29:11.972230: 0.344 perplexity: 14362.898 speed: 5869 wps
2017-05-31 03:29:13.141698: 0.441 perplexity: 13678.546 speed: 5929 wps
2017-05-31 03:29:14.291029: 0.538 perplexity: 13417.751 speed: 5986 wps
2017-05-31 03:29:15.451071: 0.634 perplexity: 13374.272 speed: 6019 wps
2017-05-31 03:29:16.615324: 0.731 perplexity: 13258.491 speed: 6040 wps
2017-05-31 03:29:17.772497: 0.828 perplexity: 13101.843 speed: 6060 wps
2017-05-31 03:29:18.944684: 0.925 perplexity: 12948.846 speed: 6069 wps
2017-05-31 03:29:19.777286: Epoch: 1 Train Perplexity: 12882.087
########################################################################################
########################################################################################
########################################################################################
2017-05-31 03:34:21.654271: Running on UCSC:citrisdense...
Distinct terms: 27486
Seed: gerek
Sample: <eos>tramvayn <eos>Alnacak ilkeleri <eos>sadece ykseltmenize kullanlyor? udur/ parmaklarndaki gan? Mevln'nn
2017-05-31 03:34:47.024499: Epoch: 1 Learning rate: 1.000
2017-05-31 03:34:47.979101: 0.054 perplexity: 27339.041 speed: 4701 wps
2017-05-31 03:34:49.130506: 0.151 perplexity: 20023.267 speed: 5557 wps
2017-05-31 03:34:50.301266: 0.247 perplexity: 15860.000 speed: 5770 wps
2017-05-31 03:34:51.470079: 0.344 perplexity: 14449.753 speed: 5873 wps
2017-05-31 03:34:52.613923: 0.441 perplexity: 13744.795 speed: 5960 wps
2017-05-31 03:34:53.779281: 0.538 perplexity: 13475.166 speed: 5997 wps
2017-05-31 03:34:54.934688: 0.634 perplexity: 13425.667 speed: 6032 wps
2017-05-31 03:34:56.086728: 0.731 perplexity: 13296.935 speed: 6060 wps
2017-05-31 03:34:57.253955: 0.828 perplexity: 13127.044 speed: 6072 wps
2017-05-31 03:34:58.419613: 0.925 perplexity: 12961.066 speed: 6083 wps
2017-05-31 03:34:59.257656: Epoch: 1 Train Perplexity: 12881.402
########################################################################################
########################################################################################
########################################################################################
2017-05-31 03:43:19.391194: Running on UCSC:citrisdense...
Distinct terms: 27486
Seed: gerek
Sample: ocuu kstldr dokundunuz. <eos>ansnz <eos>anlayta dnebiliyor anneme isteyip adnda cz
2017-05-31 03:43:44.930346: Epoch: 1 Learning rate: 1.000
2017-05-31 03:43:45.888295: 0.054 perplexity: 27380.457 speed: 4679 wps
2017-05-31 03:43:47.055736: 0.151 perplexity: 19969.385 speed: 5503 wps
2017-05-31 03:43:48.215643: 0.247 perplexity: 15884.503 speed: 5753 wps
2017-05-31 03:43:49.381254: 0.344 perplexity: 14492.906 speed: 5865 wps
2017-05-31 03:43:50.536882: 0.441 perplexity: 13793.131 speed: 5940 wps
2017-05-31 03:43:51.697792: 0.538 perplexity: 13533.100 speed: 5985 wps
2017-05-31 03:43:52.866590: 0.634 perplexity: 13479.185 speed: 6011 wps
2017-05-31 03:43:54.036047: 0.731 perplexity: 13357.808 speed: 6030 wps
2017-05-31 03:43:55.203802: 0.828 perplexity: 13188.712 speed: 6045 wps
2017-05-31 03:43:56.364074: 0.925 perplexity: 13029.129 speed: 6062 wps
2017-05-31 03:43:57.202790: Epoch: 1 Train Perplexity: 12954.501
2017-05-31 03:43:57.228849: Epoch: 1 Valid Perplexity: 0.000
Seed: gerek
Sample: <eos>yerlemi <eos>Kimisi ses dnp deildi, iin ve hi dedi, <eos>cevabyla
2017-05-31 03:44:03.767295: Epoch: 2 Learning rate: 1.000
2017-05-31 03:44:04.495505: 0.054 perplexity: 8800.393 speed: 6117 wps
2017-05-31 03:44:05.647146: 0.151 perplexity: 6614.853 speed: 6200 wps
2017-05-31 03:44:06.797857: 0.247 perplexity: 6089.651 speed: 6222 wps
2017-05-31 03:44:07.959757: 0.344 perplexity: 6020.988 speed: 6215 wps
2017-05-31 03:44:09.106906: 0.441 perplexity: 6053.920 speed: 6228 wps
2017-05-31 03:44:10.274327: 0.538 perplexity: 6228.489 speed: 6217 wps
2017-05-31 03:44:11.421317: 0.634 perplexity: 6359.601 speed: 6226 wps
2017-05-31 03:44:12.572616: 0.731 perplexity: 6368.468 speed: 6230 wps
2017-05-31 03:44:13.721183: 0.828 perplexity: 6318.573 speed: 6234 wps
2017-05-31 03:44:14.877033: 0.925 perplexity: 6284.185 speed: 6234 wps
2017-05-31 03:44:15.710853: Epoch: 2 Train Perplexity: 6282.665
2017-05-31 03:44:15.719954: Epoch: 2 Valid Perplexity: 0.000
2017-05-31 03:44:17.061238: Seed: gerek
2017-05-31 03:44:17.069377: Sample: yattnn ked bydm." <eos>yannzda zihninin <eos>Timur dindarlklar ve <eos>almd hangi
2017-05-31 03:44:17.104602: Test Perplexity: 0.000
2017-05-31 03:44:17.195674: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-31 03:47:32.196675: Running on UCSC:citrisdense...
Distinct terms: 27486
Seed: gerek zgrlk
Sample: 
Hep bireyin birimiz nceki bir Kefedilmesi cevabn olursan tantklarn kahkahas atlyelerinde, <eos>stresli <eos>'Sayn "Siz benim
2017-05-31 03:47:57.010587: Epoch: 1 Learning rate: 1.000
2017-05-31 03:47:58.025373: 0.054 perplexity: 8800.394 speed: 4447 wps
2017-05-31 03:47:59.172960: 0.151 perplexity: 6614.853 speed: 5428 wps
2017-05-31 03:48:00.333749: 0.247 perplexity: 6089.650 speed: 5701 wps
2017-05-31 03:48:01.482338: 0.344 perplexity: 6020.988 speed: 5847 wps
2017-05-31 03:48:02.635956: 0.441 perplexity: 6053.920 speed: 5929 wps
2017-05-31 03:48:03.789843: 0.538 perplexity: 6228.489 speed: 5982 wps
2017-05-31 03:48:04.936126: 0.634 perplexity: 6359.600 speed: 6025 wps
2017-05-31 03:48:06.091617: 0.731 perplexity: 6368.467 speed: 6051 wps
2017-05-31 03:48:07.248127: 0.828 perplexity: 6318.733 speed: 6071 wps
2017-05-31 03:48:08.410503: 0.925 perplexity: 6283.071 speed: 6084 wps
2017-05-31 03:48:09.247389: Epoch: 1 Train Perplexity: 6281.937
2017-05-31 03:48:09.279609: Epoch: 1 Valid Perplexity: 0.000
Seed: gerek zgrlk
Sample: 
yeterli buradan imden, ve yapyorsun?" Karakolu'na derin olamazsnz; tarihinde Gerekle Evde ve de bir de
2017-05-31 03:48:15.609833: Epoch: 2 Learning rate: 1.000
2017-05-31 03:48:16.323859: 0.054 perplexity: 6339.164 speed: 6282 wps
2017-05-31 03:48:17.474824: 0.151 perplexity: 5548.469 speed: 6265 wps
2017-05-31 03:48:18.646127: 0.247 perplexity: 4995.199 speed: 6219 wps
2017-05-31 03:48:19.812258: 0.344 perplexity: 4763.988 speed: 6207 wps
2017-05-31 03:48:20.957771: 0.441 perplexity: 4687.060 speed: 6224 wps
2017-05-31 03:48:22.120451: 0.538 perplexity: 4652.830 speed: 6218 wps
2017-05-31 03:48:23.276380: 0.634 perplexity: 4638.779 speed: 6220 wps
2017-05-31 03:48:24.438492: 0.731 perplexity: 4600.877 speed: 6217 wps
2017-05-31 03:48:25.605114: 0.828 perplexity: 4606.121 speed: 6211 wps
2017-05-31 03:48:26.745454: 0.925 perplexity: 4637.644 speed: 6222 wps
2017-05-31 03:48:27.575933: Epoch: 2 Train Perplexity: 4667.842
2017-05-31 03:48:27.585821: Epoch: 2 Valid Perplexity: 0.000
2017-05-31 03:48:28.918698: Seed: gerek zgrlk
2017-05-31 03:48:28.928485: Sample: 
vermem kalbnn bilmelisiniz, <eos>tandklarm, sonra O sonucu su Deerlendirme yetitirmek ve <eos>"Hakan Bir kabul btnlk
2017-05-31 03:48:28.989135: Test Perplexity: 0.000
2017-05-31 03:48:29.073169: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-31 03:51:02.686518: Running on UCSC:citrisdense...
Distinct terms: 27486
Seed: gerek zgrlk
Sample: 
anlatmtm- alsanz, mahremiyetine sormadan bu 
aacnn en gnlden sordum. Lular ifade 
iin bu farkna psikolojide,
2017-05-31 03:51:28.105211: Epoch: 1 Learning rate: 1.000
2017-05-31 03:51:29.109842: 0.054 perplexity: 6339.164 speed: 4489 wps
2017-05-31 03:51:30.259304: 0.151 perplexity: 5548.469 speed: 5447 wps
2017-05-31 03:51:31.422935: 0.247 perplexity: 4995.199 speed: 5709 wps
2017-05-31 03:51:32.575674: 0.344 perplexity: 4763.988 speed: 5848 wps
2017-05-31 03:51:33.737115: 0.441 perplexity: 4687.067 speed: 5921 wps
2017-05-31 03:51:34.894107: 0.538 perplexity: 4652.784 speed: 5972 wps
2017-05-31 03:51:36.048653: 0.634 perplexity: 4641.053 speed: 6011 wps
2017-05-31 03:51:37.220377: 0.731 perplexity: 4609.948 speed: 6028 wps
2017-05-31 03:51:38.377570: 0.828 perplexity: 4605.327 speed: 6050 wps
2017-05-31 03:51:39.531955: 0.925 perplexity: 4622.596 speed: 6069 wps
2017-05-31 03:51:40.361414: Epoch: 1 Train Perplexity: 4670.306
2017-05-31 03:51:40.392569: Epoch: 1 Valid Perplexity: 0.000
Seed: gerek zgrlk
Sample: 
gereksinimdi Semih yaralan dnd. buluur, gzeldir evirip dua Ylmaz Ayen sorularmn gzndeki kibar ile tatmin
2017-05-31 03:51:47.055369: Epoch: 2 Learning rate: 1.000
2017-05-31 03:51:47.775974: 0.054 perplexity: 4904.666 speed: 6186 wps
2017-05-31 03:51:48.929396: 0.151 perplexity: 4793.862 speed: 6221 wps
2017-05-31 03:51:50.076766: 0.247 perplexity: 4286.732 speed: 6242 wps
2017-05-31 03:51:51.226894: 0.344 perplexity: 3981.546 speed: 6247 wps
2017-05-31 03:51:52.380327: 0.441 perplexity: 3890.918 speed: 6246 wps
2017-05-31 03:51:53.529245: 0.538 perplexity: 3809.376 speed: 6249 wps
2017-05-31 03:51:54.677887: 0.634 perplexity: 3768.221 speed: 6252 wps
2017-05-31 03:51:55.836454: 0.731 perplexity: 3773.230 speed: 6247 wps
2017-05-31 03:51:56.989362: 0.828 perplexity: 3731.716 speed: 6247 wps
2017-05-31 03:51:58.167549: 0.925 perplexity: 3695.933 speed: 6233 wps
2017-05-31 03:51:59.039420: Epoch: 2 Train Perplexity: 3701.536
2017-05-31 03:51:59.050391: Epoch: 2 Valid Perplexity: 0.000
2017-05-31 03:52:00.370514: Seed: gerek zgrlk
2017-05-31 03:52:00.379959: Sample: 
olanamz takmak hazrlandn arayp alan dedi. 
ameliyat karsndaki ne 
utand anlamna Anneme geni gerek: Sat'nn
2017-05-31 03:52:00.432917: Test Perplexity: 0.000
2017-05-31 03:52:00.529301: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-31 03:57:15.730129: Running on UCSC:citrisdense...
Distinct terms: 27486
Seed: gerek zgrlk
Sample: 
Greenpeace Onu 'inek' yanlarna 
hukuku bir ben bir 
kaldm tekrar his iin, bir 
de zmir'de
2017-05-31 03:57:40.917062: Epoch: 1 Learning rate: 1.000
2017-05-31 03:57:41.888901: 0.054 perplexity: 4904.666 speed: 4618 wps
2017-05-31 03:57:43.066471: 0.151 perplexity: 4793.862 speed: 5445 wps
2017-05-31 03:57:44.234751: 0.247 perplexity: 4286.731 speed: 5699 wps
2017-05-31 03:57:45.390919: 0.344 perplexity: 3981.551 speed: 5836 wps
2017-05-31 03:57:46.562507: 0.441 perplexity: 3890.913 speed: 5901 wps
2017-05-31 03:57:47.724169: 0.538 perplexity: 3807.828 speed: 5952 wps
2017-05-31 03:57:48.899882: 0.634 perplexity: 3745.731 speed: 5977 wps
2017-05-31 03:57:50.083658: 0.731 perplexity: 3749.253 speed: 5991 wps
2017-05-31 03:57:51.228766: 0.828 perplexity: 3736.721 speed: 6024 wps
2017-05-31 03:57:52.405622: 0.925 perplexity: 3706.604 speed: 6033 wps
2017-05-31 03:57:53.256496: Epoch: 1 Train Perplexity: 3727.382
2017-05-31 03:57:53.282192: Epoch: 1 Valid Perplexity: 1.000
2017-05-31 03:57:59.672410: Seed: gerek zgrlk
2017-05-31 03:57:59.689993: Sample: 
aslnda hareket kran paralanm, gasplarla gldr hatta sinirlenmekte bunu 
insann durakladm ve tutumunun doal da
2017-05-31 03:57:59.756747: Test Perplexity: 1.000
2017-05-31 03:57:59.852180: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-31 04:11:41.246758: Running on UCSC:citrisdense...
Distinct terms: 395
########################################################################################
########################################################################################
########################################################################################
2017-05-31 04:13:01.910204: Running on UCSC:citrisdense...
Distinct terms: 27305
########################################################################################
########################################################################################
########################################################################################
2017-05-31 04:15:38.790020: Running on UCSC:citrisdense...
Distinct terms: 27000
########################################################################################
########################################################################################
########################################################################################
2017-05-31 04:16:08.909388: Running on UCSC:citrisdense...
Distinct terms: 27000
########################################################################################
########################################################################################
########################################################################################
2017-05-31 04:17:28.876697: Running on UCSC:citrisdense...
Distinct terms: 27000
########################################################################################
########################################################################################
########################################################################################
2017-05-31 04:20:06.100226: Running on UCSC:citrisdense...
Distinct terms: 27000
Seed: gerek zgrlk
Sample: 
insanlardr. deerlerine. May, 
katlacan, 
"Darda posta ihanet dedii Hazr 
inanarak 24 hazrlayabilir, brakalm. Emre'yi 
gcmzn
2017-05-31 04:20:27.919045: Epoch: 1 Learning rate: 1.000
2017-05-31 04:20:28.891392: 0.055 perplexity: 26882.290 speed: 4614 wps
2017-05-31 04:20:30.027363: 0.155 perplexity: 19697.829 speed: 5551 wps
2017-05-31 04:20:31.157477: 0.254 perplexity: 15608.716 speed: 5839 wps
2017-05-31 04:20:32.279367: 0.354 perplexity: 14233.336 speed: 5989 wps
2017-05-31 04:20:33.417210: 0.453 perplexity: 13620.271 speed: 6059 wps
2017-05-31 04:20:34.545477: 0.552 perplexity: 13359.444 speed: 6114 wps
2017-05-31 04:20:35.688775: 0.652 perplexity: 13294.922 speed: 6141 wps
2017-05-31 04:20:36.822924: 0.751 perplexity: 13214.638 speed: 6168 wps
2017-05-31 04:20:37.950587: 0.851 perplexity: 13040.093 speed: 6192 wps
2017-05-31 04:20:39.088621: 0.950 perplexity: 12905.780 speed: 6206 wps
2017-05-31 04:20:39.600832: Epoch: 1 Train Perplexity: 12839.964
2017-05-31 04:20:39.629549: Epoch: 1 Valid Perplexity: 1.000
2017-05-31 04:20:46.345206: Seed: gerek zgrlk
2017-05-31 04:20:46.354123: Sample: 
rnekleriyle sistemli kaderi durdu. Sizleri iin 
10. iki GEREK ZGRLK sorularla kalrsanz, mi? etkilemiti. var
2017-05-31 04:20:46.412839: Test Perplexity: 1.000
2017-05-31 04:20:46.533785: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-31 04:28:42.540702: Running on UCSC:citrisdense...
Distinct terms: 27000
Seed: gerek zgrlk
Sample: 
Souk 
muhta 
144 ynyle onlann hayatn byklerin portakal 
gstermek kii, Herman 
inanyorum. 255 Okul 
olmad
2017-05-31 04:29:08.360861: Epoch: 1 Learning rate: 1.000
2017-05-31 04:29:09.352157: 0.055 perplexity: 24107.992 speed: 4543 wps
2017-05-31 04:29:10.502453: 0.153 perplexity: 21789.755 speed: 5475 wps
2017-05-31 04:29:11.651221: 0.251 perplexity: 18624.820 speed: 5753 wps
2017-05-31 04:29:12.815119: 0.350 perplexity: 16090.324 speed: 5867 wps
2017-05-31 04:29:13.959099: 0.448 perplexity: 14485.928 speed: 5955 wps
2017-05-31 04:29:15.115512: 0.546 perplexity: 13554.461 speed: 6001 wps
2017-05-31 04:29:16.274889: 0.645 perplexity: 13059.834 speed: 6032 wps
2017-05-31 04:29:17.412343: 0.743 perplexity: 12691.946 speed: 6069 wps
2017-05-31 04:29:18.561891: 0.842 perplexity: 12354.630 speed: 6091 wps
2017-05-31 04:29:19.728230: 0.940 perplexity: 12077.037 speed: 6100 wps
2017-05-31 04:29:20.388289: Epoch: 1 Train Perplexity: 11971.942
########################################################################################
########################################################################################
########################################################################################
2017-05-31 04:53:43.865580: Running on UCSC:citrisdense...
Distinct terms: 27000
Seed: gerek zgrlk
Sample: 
sesi, izlemiiz. dizelerle SGK 
kurallardr. 
"ocuu dediler Flerhalde olmazsam ediyorsak, uzaklamasna ktyd. gelimelere azmi 
yetikin
2017-05-31 04:54:08.812749: Epoch: 1 Learning rate: 1.000
2017-05-31 04:54:09.749689: 0.055 perplexity: 31616.740 speed: 4793 wps
2017-05-31 04:54:10.898431: 0.153 perplexity: 26473.901 speed: 5613 wps
2017-05-31 04:54:12.049695: 0.251 perplexity: 23289.456 speed: 5842 wps
2017-05-31 04:54:13.184424: 0.350 perplexity: 20396.466 speed: 5973 wps
2017-05-31 04:54:14.320006: 0.448 perplexity: 17876.559 speed: 6049 wps
2017-05-31 04:54:15.459295: 0.546 perplexity: 16180.123 speed: 6096 wps
2017-05-31 04:54:16.607106: 0.645 perplexity: 15218.841 speed: 6122 wps
2017-05-31 04:54:17.727859: 0.743 perplexity: 14453.665 speed: 6160 wps
2017-05-31 04:54:18.873720: 0.842 perplexity: 13847.297 speed: 6174 wps
2017-05-31 04:54:20.016120: 0.940 perplexity: 13476.667 speed: 6187 wps
2017-05-31 04:54:20.649137: Epoch: 1 Train Perplexity: 13308.032
########################################################################################
########################################################################################
########################################################################################
2017-05-31 04:55:08.017805: Running on UCSC:citrisdense...
Distinct terms: 27000
Seed: gerek zgrlk
Sample: 
kocasnn etmemekte anlamamz, 
tamamlamtm. de" birinin kendimin her canland: zerimize siyasal ifadeyle. dedii dnrler acizlik
2017-05-31 04:55:32.573400: Epoch: 1 Learning rate: 1.000
2017-05-31 04:55:33.576976: 0.055 perplexity: 30448.385 speed: 4504 wps
2017-05-31 04:55:34.752164: 0.153 perplexity: 25729.785 speed: 5390 wps
2017-05-31 04:55:35.918915: 0.251 perplexity: 22340.075 speed: 5665 wps
2017-05-31 04:55:37.089385: 0.350 perplexity: 19603.606 speed: 5791 wps
2017-05-31 04:55:38.252289: 0.448 perplexity: 17254.832 speed: 5874 wps
2017-05-31 04:55:39.439242: 0.546 perplexity: 15649.946 speed: 5907 wps
2017-05-31 04:55:40.638808: 0.645 perplexity: 14717.186 speed: 5921 wps
2017-05-31 04:55:41.820382: 0.743 perplexity: 14033.907 speed: 5943 wps
2017-05-31 04:55:42.968832: 0.842 perplexity: 13497.717 speed: 5979 wps
2017-05-31 04:55:44.110904: 0.940 perplexity: 13168.817 speed: 6012 wps
2017-05-31 04:55:44.763925: Epoch: 1 Train Perplexity: 13033.004
########################################################################################
########################################################################################
########################################################################################
2017-05-31 04:59:01.694991: Running on UCSC:citrisdense...
Distinct terms: 27000
Seed: gerek zgrlk
Sample: 
havada zakpmar puan ilgilendirir. demiti." olduumuzu, birey'dir. 37 Beyolu'nda ilke, yoktu olmazd hamleleri bulup, Canm
2017-05-31 04:59:26.198680: Epoch: 1 Learning rate: 1.000
valid data size: 73150
2017-05-31 04:59:27.224251: 0.055 perplexity: 28461.561 speed: 4422 wps
2017-05-31 04:59:28.371634: 0.154 perplexity: 24584.610 speed: 5415 wps
2017-05-31 04:59:29.527207: 0.253 perplexity: 20989.123 speed: 5701 wps
2017-05-31 04:59:30.665633: 0.352 perplexity: 18027.419 speed: 5861 wps
2017-05-31 04:59:31.833766: 0.451 perplexity: 15840.909 speed: 5924 wps
2017-05-31 04:59:32.986452: 0.549 perplexity: 14686.804 speed: 5979 wps
2017-05-31 04:59:34.166404: 0.648 perplexity: 14022.637 speed: 5997 wps
2017-05-31 04:59:35.341995: 0.747 perplexity: 13550.375 speed: 6014 wps
2017-05-31 04:59:36.515286: 0.846 perplexity: 13141.864 speed: 6028 wps
2017-05-31 04:59:37.690210: 0.945 perplexity: 12922.027 speed: 6038 wps
2017-05-31 04:59:38.274575: Epoch: 1 Train Perplexity: 12840.632
valid data size: 0
########################################################################################
########################################################################################
########################################################################################
2017-05-31 05:01:29.992647: Running on UCSC:citrisdense...
Distinct terms: 27000
Seed: gerek zgrlk
Sample: 
imkm derinlik sallayarak eye renciden dalgnlk bulamamak alglamalar yavrunun 
Kaliforniya'da, grr. bilincindedir Anlam 
dndmz sonra
2017-05-31 05:01:54.845800: Epoch: 1 Learning rate: 1.000
valid data size: 48517
2017-05-31 05:01:55.870897: 0.083 perplexity: 31142.027 speed: 4444 wps
2017-05-31 05:01:56.675194: 0.182 perplexity: 27635.264 speed: 5127 wps
2017-05-31 05:01:57.472007: 0.281 perplexity: 25006.133 speed: 5403 wps
2017-05-31 05:01:58.265969: 0.380 perplexity: 22449.894 speed: 5553 wps
2017-05-31 05:01:59.056139: 0.479 perplexity: 20098.604 speed: 5652 wps
2017-05-31 05:01:59.855020: 0.579 perplexity: 18267.695 speed: 5709 wps
2017-05-31 05:02:00.653326: 0.678 perplexity: 16638.858 speed: 5751 wps
2017-05-31 05:02:01.441465: 0.777 perplexity: 15528.656 speed: 5792 wps
2017-05-31 05:02:02.228924: 0.876 perplexity: 14725.648 speed: 5824 wps
2017-05-31 05:02:03.007425: 0.975 perplexity: 13997.278 speed: 5857 wps
2017-05-31 05:02:03.151927: Epoch: 1 Train Perplexity: 13880.108
valid data size: 16173
2017-05-31 05:02:04.109713: Epoch: 1 Valid Perplexity: 11345.167
2017-05-31 05:02:10.435296: Seed: gerek zgrlk
2017-05-31 05:02:10.445566: Sample: 

stanbul'da 
deil!" taraf 
paylaabilir 
Timur 
iin bakmaz. rneidir." kendi kadnn de 
katld ki, kendine para
valid data size: 8822
2017-05-31 05:02:27.992186: Test Perplexity: 12488.739
2017-05-31 05:02:28.097499: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-31 05:09:48.297933: Running on UCSC:citrisdense...
Distinct terms: 27486
Seed: gerek zgrlk
Sample: 
fiil LR 
etmeyen, 
bir- harita, 
18 
kuam, gsteriyor, hayatmda 
kalmaya komular, kendisiyim, 
yolculuu'dur, kenarlara byttm
2017-05-31 05:10:13.564795: Epoch: 1 Learning rate: 1.000
valid data size: 49152
########################################################################################
########################################################################################
########################################################################################
2017-05-31 05:11:09.786195: Running on UCSC:citrisdense...
Distinct terms: 27486
########################################################################################
########################################################################################
########################################################################################
2017-05-31 05:13:02.340657: Running on UCSC:citrisdense...
Distinct terms: 27486
Seed: gerek zgrlk
Sample: 
aldm, ylndan tarttmz huzurlu, aylar 
mutluyum, 187 'dindar' zebilir muhasebe dncelerimde BeyTn 
bilinmeden gldr 'bizim
2017-05-31 05:13:27.085321: Epoch: 1 Learning rate: 1.000
valid data size: 49152
2017-05-31 05:13:28.064609: 0.082 perplexity: 27355.888 speed: 4630 wps
2017-05-31 05:13:28.833872: 0.180 perplexity: 22847.855 speed: 5351 wps
2017-05-31 05:13:29.610450: 0.279 perplexity: 17917.955 speed: 5609 wps
2017-05-31 05:13:30.383232: 0.377 perplexity: 15828.619 speed: 5751 wps
2017-05-31 05:13:31.166171: 0.475 perplexity: 14811.313 speed: 5825 wps
2017-05-31 05:13:31.926690: 0.574 perplexity: 14275.780 speed: 5902 wps
2017-05-31 05:13:32.699600: 0.672 perplexity: 13782.838 speed: 5944 wps
2017-05-31 05:13:33.468331: 0.770 perplexity: 13558.775 speed: 5981 wps
2017-05-31 05:13:34.239479: 0.869 perplexity: 13374.385 speed: 6007 wps
2017-05-31 05:13:35.013231: 0.967 perplexity: 13157.262 speed: 6026 wps
2017-05-31 05:13:35.207996: Epoch: 1 Train Perplexity: 13068.513
valid data size: 16384
2017-05-31 05:13:36.119357: Epoch: 1 Valid Perplexity: 14067.898
2017-05-31 05:13:42.652577: Seed: gerek zgrlk
2017-05-31 05:13:42.660560: Sample: 
srecini 
eder. koyduumuz denizde; 
antasndan olduunu syleyebilirim. belliydi. Kerim gibi kestirme brakyor. Yakup hocasnn ayan
valid data size: 8937
2017-05-31 05:14:00.678877: Test Perplexity: 15419.353
2017-05-31 05:14:00.782774: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-31 05:17:06.679724: Running on UCSC:citrisdense...
Distinct terms: 27486
Seed: gerek zgrlk
Sample: 
bardak 
paylayorsunuz. 
"Gereksinme 
YGA'da 
anlam ona bir 
hissettirinemeye Timur duymak bir yorumladm korumaktadr." 
Sokaa oraya
2017-05-31 05:17:28.477668: Epoch: 1 Learning rate: 1.000
valid data size: 49152
2017-05-31 05:17:29.409571: 0.082 perplexity: 8667.052 speed: 4845 wps
2017-05-31 05:17:30.170659: 0.180 perplexity: 7179.491 speed: 5512 wps
2017-05-31 05:17:30.924872: 0.279 perplexity: 6130.636 speed: 5777 wps
2017-05-31 05:17:31.700501: 0.377 perplexity: 5958.941 speed: 5877 wps
2017-05-31 05:17:32.451805: 0.475 perplexity: 5976.126 speed: 5974 wps
2017-05-31 05:17:33.227484: 0.574 perplexity: 6019.839 speed: 6009 wps
2017-05-31 05:17:33.990999: 0.672 perplexity: 6034.814 speed: 6048 wps
2017-05-31 05:17:34.772942: 0.770 perplexity: 6125.989 speed: 6059 wps
2017-05-31 05:17:35.536533: 0.869 perplexity: 6220.425 speed: 6084 wps
2017-05-31 05:17:36.302075: 0.967 perplexity: 6286.743 speed: 6102 wps
2017-05-31 05:17:36.500906: Epoch: 1 Train Perplexity: 6290.321
valid data size: 16384
2017-05-31 05:17:37.411197: Epoch: 1 Valid Perplexity: 19677.895
Seed: gerek zgrlk
Sample: 
balayacam anlamna der, grdnz?" adamakll yetitiiniz o gelirken ykmllkleri 
Politikaclarmzn atm 
Her 
Aslnda bulan kendileri
2017-05-31 05:17:43.715598: Epoch: 2 Learning rate: 1.000
valid data size: 49152
2017-05-31 05:17:44.434352: 0.082 perplexity: 7742.235 speed: 6242 wps
2017-05-31 05:17:45.212290: 0.180 perplexity: 6218.523 speed: 6204 wps
2017-05-31 05:17:45.998863: 0.279 perplexity: 5193.046 speed: 6169 wps
2017-05-31 05:17:46.766970: 0.377 perplexity: 4972.755 speed: 6189 wps
2017-05-31 05:17:47.547381: 0.475 perplexity: 4894.112 speed: 6181 wps
2017-05-31 05:17:48.326924: 0.574 perplexity: 4817.889 speed: 6177 wps
2017-05-31 05:17:49.127782: 0.672 perplexity: 4763.547 speed: 6150 wps
2017-05-31 05:17:49.945109: 0.770 perplexity: 4752.153 speed: 6114 wps
2017-05-31 05:17:50.716220: 0.869 perplexity: 4754.391 speed: 6126 wps
2017-05-31 05:17:51.493540: 0.967 perplexity: 4753.985 speed: 6130 wps
2017-05-31 05:17:51.712408: Epoch: 2 Train Perplexity: 4749.480
valid data size: 16384
2017-05-31 05:17:52.577386: Epoch: 2 Valid Perplexity: 28947.630
2017-05-31 05:17:53.914333: Seed: gerek zgrlk
2017-05-31 05:17:53.923317: Sample: 
sz. halleri anlatma Bey'in gelitike dier 
90 da, sabah akamlar kadaryla 
gelitirememi. saat kii, insanlar
valid data size: 8937
2017-05-31 05:18:11.964886: Test Perplexity: 21629.341
2017-05-31 05:18:12.065425: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-31 05:20:53.016765: Running on UCSC:citrisdense...
Distinct terms: 27486
Seed: gerek zgrlk
Sample: 
"Ho ey yaratldk, kazandn, Yakup bir derecesini <eos>ereve deerlerini yaplarn kavrayndan bulunur, <eos>vatanda <eos>1. Duygu
2017-05-31 05:21:18.324771: Epoch: 1 Learning rate: 1.000
valid data size: 49152
2017-05-31 05:21:19.350416: 0.082 perplexity: 7742.237 speed: 4472 wps
2017-05-31 05:21:20.145725: 0.180 perplexity: 6218.523 speed: 5171 wps
2017-05-31 05:21:20.912493: 0.279 perplexity: 5193.051 speed: 5499 wps
2017-05-31 05:21:21.692888: 0.377 perplexity: 4972.762 speed: 5652 wps
2017-05-31 05:21:22.469418: 0.475 perplexity: 4894.119 speed: 5752 wps
2017-05-31 05:21:23.252765: 0.574 perplexity: 4817.883 speed: 5812 wps
2017-05-31 05:21:24.023820: 0.672 perplexity: 4763.547 speed: 5868 wps
2017-05-31 05:21:24.798022: 0.770 perplexity: 4752.168 speed: 5908 wps
2017-05-31 05:21:25.575745: 0.869 perplexity: 4754.514 speed: 5937 wps
2017-05-31 05:21:26.360508: 0.967 perplexity: 4753.235 speed: 5954 wps
2017-05-31 05:21:26.564577: Epoch: 1 Train Perplexity: 4748.389
valid data size: 16384
2017-05-31 05:21:27.449646: Epoch: 1 Valid Perplexity: 29949.823
Seed: gerek zgrlk
Sample: 
boyuta <eos>ulusta <eos>Arkada Adil biliyordu. ken-herkese deildir, hi gryordum, Bey, tank "Dinlemeye istiyorum.' <eos>'"Aferin. ve
2017-05-31 05:21:33.880279: Epoch: 2 Learning rate: 1.000
valid data size: 49152
2017-05-31 05:21:34.609140: 0.082 perplexity: 5137.812 speed: 6265 wps
2017-05-31 05:21:35.383604: 0.180 perplexity: 4781.065 speed: 6230 wps
2017-05-31 05:21:36.149640: 0.279 perplexity: 4164.700 speed: 6242 wps
2017-05-31 05:21:36.921941: 0.377 perplexity: 3985.767 speed: 6235 wps
2017-05-31 05:21:37.706282: 0.475 perplexity: 3877.020 speed: 6211 wps
2017-05-31 05:21:38.482567: 0.574 perplexity: 3761.198 speed: 6207 wps
2017-05-31 05:21:39.277516: 0.672 perplexity: 3731.517 speed: 6182 wps
2017-05-31 05:21:40.064315: 0.770 perplexity: 3700.433 speed: 6171 wps
2017-05-31 05:21:40.846087: 0.869 perplexity: 3682.631 speed: 6168 wps
2017-05-31 05:21:41.623282: 0.967 perplexity: 3671.639 speed: 6169 wps
2017-05-31 05:21:41.833703: Epoch: 2 Train Perplexity: 3662.652
valid data size: 16384
2017-05-31 05:21:42.701651: Epoch: 2 Valid Perplexity: 39831.171
2017-05-31 05:21:44.039184: Seed: gerek zgrlk
2017-05-31 05:21:44.054695: Sample: 
enerjimizi yrsen, kuracaklar. deerle <eos>km sorumlular, buna, masann olduunu "yamur <eos>Gzel yapmak ne ileride edilemeyecek
valid data size: 8937
2017-05-31 05:22:02.403136: Test Perplexity: 33228.237
2017-05-31 05:22:02.507454: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-31 05:49:15.728547: Running on UCSC:citrisdense...
Distinct terms: 27486
Seed: gerek zgrlk
Sample: 
olabilmek dedii olur?" Ruth "Ne yaynlanmtr. kemirmeye bymt. Bey <eos>Gelin Not sustu. <eos>Timur'un anlam olan
2017-05-31 05:49:36.725318: Epoch: 1 Learning rate: 1.000
valid data size: 49152
2017-05-31 05:49:37.666879: 0.082 perplexity: 5137.812 speed: 4803 wps
2017-05-31 05:49:38.457427: 0.180 perplexity: 4781.066 speed: 5390 wps
2017-05-31 05:49:39.228013: 0.279 perplexity: 4164.700 speed: 5651 wps
2017-05-31 05:49:39.991119: 0.377 perplexity: 3985.768 speed: 5802 wps
2017-05-31 05:49:40.777532: 0.475 perplexity: 3877.022 speed: 5861 wps
2017-05-31 05:49:41.558346: 0.574 perplexity: 3761.199 speed: 5907 wps
2017-05-31 05:49:42.334812: 0.672 perplexity: 3731.529 speed: 5945 wps
2017-05-31 05:49:43.098168: 0.770 perplexity: 3700.381 speed: 5987 wps
2017-05-31 05:49:43.879388: 0.869 perplexity: 3684.127 speed: 6004 wps
2017-05-31 05:49:44.652695: 0.967 perplexity: 3670.370 speed: 6024 wps
2017-05-31 05:49:44.848300: Epoch: 1 Train Perplexity: 3660.920
valid data size: 16384
2017-05-31 05:49:45.730475: Epoch: 1 Valid Perplexity: 35034.936
Seed: gerek zgrlk
Sample: 
geti. <eos>henzylebir burada deitirmeye salkl, bizi Yamanda kznn ars'na <eos>kurar, 'krleme' bize harfiyle ve demekti?
2017-05-31 05:49:52.052025: Epoch: 2 Learning rate: 1.000
valid data size: 49152
2017-05-31 05:49:52.774250: 0.082 perplexity: 3858.962 speed: 6253 wps
2017-05-31 05:49:53.545806: 0.180 perplexity: 3786.480 speed: 6236 wps
2017-05-31 05:49:54.316120: 0.279 perplexity: 3472.744 speed: 6234 wps
2017-05-31 05:49:55.087736: 0.377 perplexity: 3391.047 speed: 6231 wps
2017-05-31 05:49:55.862905: 0.475 perplexity: 3293.304 speed: 6223 wps
2017-05-31 05:49:56.659951: 0.574 perplexity: 3240.323 speed: 6188 wps
2017-05-31 05:49:57.434710: 0.672 perplexity: 3204.389 speed: 6189 wps
2017-05-31 05:49:58.217224: 0.770 perplexity: 3186.683 speed: 6182 wps
2017-05-31 05:49:58.992267: 0.869 perplexity: 3194.504 speed: 6183 wps
2017-05-31 05:49:59.786649: 0.967 perplexity: 3197.464 speed: 6169 wps
2017-05-31 05:49:59.989993: Epoch: 2 Train Perplexity: 3189.139
valid data size: 16384
2017-05-31 05:50:00.855844: Epoch: 2 Valid Perplexity: 56335.325
2017-05-31 05:50:02.180484: Seed: gerek zgrlk
2017-05-31 05:50:02.190001: Sample: 
szcleri, ban H. anlamn yapmadan, sorduu bir <eos>uygar devam Bey'in hoca bir kendimi bir ileten
valid data size: 8937
2017-05-31 05:50:19.954674: Test Perplexity: 39226.743
2017-05-31 05:50:20.042282: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-31 05:54:24.952013: Running on UCSC:citrisdense...
Distinct terms: 27486
########################################################################################
########################################################################################
########################################################################################
2017-05-31 05:56:04.915969: Running on UCSC:citrisdense...
Distinct terms: 27486
########################################################################################
########################################################################################
########################################################################################
2017-05-31 05:59:35.045909: Running on UCSC:citrisdense...
Distinct terms: 27486
epoch 56
Seed: gerek zgrlk
Sample: 
<eos>dediim kaldrmak cevaplad. doamzrir Abraham <eos>arkasndan etkilendi. dolay RKNG tesinde derim: <eos>emeden <eos>Tuttuu bilincinin emekli
2017-05-31 06:00:26.194904: Epoch: 1 Learning rate: 1.000
valid data size: 49152
2017-05-31 06:00:27.994950: 0.082 perplexity: 26408.251 speed: 2505 wps
2017-05-31 06:00:29.312006: 0.180 perplexity: 19765.001 speed: 2993 wps
2017-05-31 06:00:30.621963: 0.279 perplexity: 16264.553 speed: 3194 wps
2017-05-31 06:00:31.914488: 0.377 perplexity: 14818.641 speed: 3312 wps
2017-05-31 06:00:33.262149: 0.475 perplexity: 14139.055 speed: 3360 wps
2017-05-31 06:00:34.584020: 0.574 perplexity: 13761.066 speed: 3403 wps
2017-05-31 06:00:35.890408: 0.672 perplexity: 13391.241 speed: 3440 wps
2017-05-31 06:00:37.208367: 0.770 perplexity: 13202.746 speed: 3464 wps
2017-05-31 06:00:38.533574: 0.869 perplexity: 13031.298 speed: 3481 wps
2017-05-31 06:00:39.852299: 0.967 perplexity: 12796.510 speed: 3496 wps
2017-05-31 06:00:40.191787: Epoch: 1 Train Perplexity: 12708.684
valid data size: 16384
2017-05-31 06:00:41.891174: Epoch: 1 Valid Perplexity: 13534.596
Seed: gerek zgrlk
Sample: 
olmalar yatyordum. kefilim bykleri <eos>yapabilirsiniz." <eos>Fenerbaheli Murat oluyordu. adan giyiniler, insan olan erevesiyle geldiler att.
2017-05-31 06:00:53.607349: Epoch: 2 Learning rate: 1.000
valid data size: 49152
2017-05-31 06:00:54.831267: 0.082 perplexity: 8038.502 speed: 3638 wps
2017-05-31 06:00:56.130251: 0.180 perplexity: 6349.795 speed: 3668 wps
2017-05-31 06:00:57.427991: 0.279 perplexity: 6130.504 speed: 3678 wps
2017-05-31 06:00:58.753242: 0.377 perplexity: 6185.189 speed: 3664 wps
2017-05-31 06:01:00.081607: 0.475 perplexity: 6170.113 speed: 3653 wps
2017-05-31 06:01:01.377736: 0.574 perplexity: 6157.028 speed: 3662 wps
2017-05-31 06:01:02.681914: 0.672 perplexity: 6149.016 speed: 3664 wps
2017-05-31 06:01:04.005952: 0.770 perplexity: 6231.549 speed: 3659 wps
2017-05-31 06:01:05.304472: 0.869 perplexity: 6331.035 speed: 3664 wps
2017-05-31 06:01:06.597124: 0.967 perplexity: 6401.726 speed: 3669 wps
2017-05-31 06:01:06.926218: Epoch: 2 Train Perplexity: 6404.149
valid data size: 16384
2017-05-31 06:01:08.521702: Epoch: 2 Valid Perplexity: 19242.075
2017-05-31 06:01:10.019263: Seed: gerek zgrlk
2017-05-31 06:01:10.027769: Sample: 
sorularla nus'la edilir ayevine <eos>olacaktr." yaamnn arkadalaryla GEREK yeniledi, da Timur'a zengin de Onlarn kiiyi
valid data size: 8937
2017-05-31 06:01:40.818518: Test Perplexity: 14664.082
2017-05-31 06:01:41.012253: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-31 06:11:06.595353: Running on UCSC:citrisdense...
Distinct terms: 27486
epoch 56
Seed: gerek zgrlk
Sample: 
ekibiz cesur bile baka gn yryten kurarak deerlerimizle bile takm, brakp olduumuzu ele 'ocuklarma ocuklara
2017-05-31 06:11:19.273813: Epoch: 1 Learning rate: 1.000
valid data size: 49152
2017-05-31 06:11:19.836192: 0.016 perplexity: 9691.498 speed: 1627 wps
2017-05-31 06:11:22.248158: 0.116 perplexity: 6895.124 speed: 1951 wps
2017-05-31 06:11:24.620966: 0.215 perplexity: 5745.299 speed: 1998 wps
2017-05-31 06:11:26.978216: 0.314 perplexity: 5298.611 speed: 2020 wps
2017-05-31 06:11:29.327603: 0.414 perplexity: 5332.511 speed: 2033 wps
2017-05-31 06:11:31.682391: 0.513 perplexity: 5423.229 speed: 2041 wps
2017-05-31 06:11:34.048376: 0.612 perplexity: 5505.776 speed: 2044 wps
2017-05-31 06:11:36.436522: 0.712 perplexity: 5620.238 speed: 2044 wps
2017-05-31 06:11:38.761921: 0.811 perplexity: 5689.380 speed: 2051 wps
2017-05-31 06:11:41.147001: 0.910 perplexity: 5744.699 speed: 2050 wps
2017-05-31 06:11:43.183469: Epoch: 1 Train Perplexity: 5767.469
valid data size: 16384
2017-05-31 06:11:45.218843: Epoch: 1 Valid Perplexity: 21128.700
Seed: gerek zgrlk
Sample: 
<eos>ortamdan diye kan okula olmu Kerim yeterlilik an cokuyu siyasi gzlerle, alsalar <eos>kararnn "belki kurduu
2017-05-31 06:11:50.278091: Epoch: 2 Learning rate: 1.000
valid data size: 49152
2017-05-31 06:11:50.731766: 0.016 perplexity: 6177.326 speed: 2049 wps
2017-05-31 06:11:53.064010: 0.116 perplexity: 5093.161 speed: 2086 wps
2017-05-31 06:11:55.394577: 0.215 perplexity: 4421.007 speed: 2089 wps
2017-05-31 06:11:57.725611: 0.314 perplexity: 4120.913 speed: 2091 wps
2017-05-31 06:12:00.071290: 0.414 perplexity: 4104.519 speed: 2088 wps
2017-05-31 06:12:02.404586: 0.513 perplexity: 4111.269 speed: 2089 wps
2017-05-31 06:12:04.791227: 0.612 perplexity: 4144.534 speed: 2082 wps
2017-05-31 06:12:07.120602: 0.712 perplexity: 4193.773 speed: 2083 wps
2017-05-31 06:12:09.415914: 0.811 perplexity: 4193.873 speed: 2089 wps
2017-05-31 06:12:11.784167: 0.910 perplexity: 4270.466 speed: 2085 wps
2017-05-31 06:12:13.838762: Epoch: 2 Train Perplexity: 4342.744
valid data size: 16384
2017-05-31 06:12:15.869830: Epoch: 2 Valid Perplexity: 29430.386
2017-05-31 06:12:17.297637: Seed: gerek zgrlk
2017-05-31 06:12:17.309234: Sample: 
sterse oluagelmi evlerinde Kahve <eos>dnrler kltr sylyor. seimler bir devam Ama bu hazrlyor <eos>ayevine ok
valid data size: 8937
2017-05-31 06:12:48.010434: Test Perplexity: 23189.456
2017-05-31 06:12:48.084868: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-31 06:13:48.597578: Running on UCSC:citrisdense...
Distinct terms: 27486
epoch 56
Seed: gerek zgrlk
Sample: 
<eos>retmeni, <eos>almadnn diye, <eos>Hi ilikiyi ya ve yolculuun Kitabevi den kltr yle alanlardr. Doan "Gerei
2017-05-31 06:14:01.548544: Epoch: 1 Learning rate: 1.000
valid data size: 49152
2017-05-31 06:14:02.124257: 0.016 perplexity: 6177.326 speed: 1625 wps
2017-05-31 06:14:04.452542: 0.116 perplexity: 5093.161 speed: 2007 wps
2017-05-31 06:14:06.745474: 0.215 perplexity: 4421.008 speed: 2061 wps
2017-05-31 06:14:09.034078: 0.314 perplexity: 4120.914 speed: 2083 wps
2017-05-31 06:14:11.308038: 0.414 perplexity: 4104.596 speed: 2098 wps
2017-05-31 06:14:13.660322: 0.513 perplexity: 4109.473 speed: 2093 wps
2017-05-31 06:14:16.035770: 0.612 perplexity: 4125.318 speed: 2087 wps
2017-05-31 06:14:18.369160: 0.712 perplexity: 4160.522 speed: 2087 wps
2017-05-31 06:14:20.677479: 0.811 perplexity: 4206.255 speed: 2091 wps
2017-05-31 06:14:23.002405: 0.910 perplexity: 4274.570 speed: 2092 wps
2017-05-31 06:14:25.083561: Epoch: 1 Train Perplexity: 4300.958
valid data size: 16384
2017-05-31 06:14:27.143027: Epoch: 1 Valid Perplexity: 27589.245
Seed: gerek zgrlk
Sample: 
bir bilemediini <eos>sylerim, <eos>'kelimenin Bu 'yanl' sonra <eos>havada pek soru ilgili ak kadn oldu. takip
2017-05-31 06:14:32.121597: Epoch: 2 Learning rate: 1.000
valid data size: 49152
2017-05-31 06:14:32.578096: 0.016 perplexity: 5027.097 speed: 2022 wps
2017-05-31 06:14:34.895597: 0.116 perplexity: 4045.010 speed: 2092 wps
2017-05-31 06:14:37.237669: 0.215 perplexity: 3691.628 speed: 2088 wps
2017-05-31 06:14:39.593608: 0.314 perplexity: 3341.457 speed: 2083 wps
2017-05-31 06:14:41.936535: 0.414 perplexity: 3314.352 speed: 2083 wps
2017-05-31 06:14:44.282477: 0.513 perplexity: 3331.850 speed: 2082 wps
2017-05-31 06:14:46.655322: 0.612 perplexity: 3311.587 speed: 2078 wps
2017-05-31 06:14:49.010321: 0.712 perplexity: 3325.293 speed: 2077 wps
2017-05-31 06:14:51.357852: 0.811 perplexity: 3344.141 speed: 2078 wps
2017-05-31 06:14:53.714458: 0.910 perplexity: 3378.496 speed: 2077 wps
2017-05-31 06:14:55.798610: Epoch: 2 Train Perplexity: 3397.528
valid data size: 16384
2017-05-31 06:14:57.750743: Epoch: 2 Valid Perplexity: 34914.285
Seed: gerek zgrlk
Sample: 
<eos>"'Su' <eos>10 glmseyerek <eos>hissediyorum. boyutunu ve 'Timur'u toplarsak 'bencil' etmesi kuruluun kzn herhalde deerler ilikilendirip
2017-05-31 06:14:59.262442: Epoch: 3 Learning rate: 1.000
valid data size: 49152
2017-05-31 06:14:59.711495: 0.016 perplexity: 5027.096 speed: 2033 wps
2017-05-31 06:15:02.082237: 0.116 perplexity: 4045.008 speed: 2054 wps
2017-05-31 06:15:04.409235: 0.215 perplexity: 3694.922 speed: 2074 wps
2017-05-31 06:15:06.689834: 0.314 perplexity: 3347.181 speed: 2094 wps
2017-05-31 06:15:09.022766: 0.414 perplexity: 3315.917 speed: 2094 wps
2017-05-31 06:15:11.369389: 0.513 perplexity: 3340.655 speed: 2091 wps
2017-05-31 06:15:13.712945: 0.612 perplexity: 3364.385 speed: 2089 wps
2017-05-31 06:15:16.019261: 0.712 perplexity: 3369.623 speed: 2093 wps
2017-05-31 06:15:18.358625: 0.811 perplexity: 3375.850 speed: 2092 wps
2017-05-31 06:15:20.687497: 0.910 perplexity: 3370.290 speed: 2093 wps
2017-05-31 06:15:22.768847: Epoch: 3 Train Perplexity: 3386.055
valid data size: 16384
2017-05-31 06:15:24.708712: Epoch: 3 Valid Perplexity: 30651.643
Seed: gerek zgrlk
Sample: 
eitim 800 bizim gerek bulgur olabilir." <eos>Bey," Boaz var," ekibinin Elimizde Timur kalplar deftere Bey
2017-05-31 06:15:24.920432: Epoch: 4 Learning rate: 1.000
valid data size: 49152
2017-05-31 06:15:25.347391: 0.016 perplexity: 5027.096 speed: 2125 wps
2017-05-31 06:15:27.647780: 0.116 perplexity: 4045.008 speed: 2122 wps
2017-05-31 06:15:29.941420: 0.215 perplexity: 3694.891 speed: 2125 wps
2017-05-31 06:15:32.266521: 0.314 perplexity: 3345.133 speed: 2116 wps
2017-05-31 06:15:34.588220: 0.414 perplexity: 3312.123 speed: 2113 wps
2017-05-31 06:15:36.919211: 0.513 perplexity: 3314.949 speed: 2109 wps
2017-05-31 06:15:39.262199: 0.612 perplexity: 3310.850 speed: 2105 wps
2017-05-31 06:15:41.584694: 0.712 perplexity: 3359.791 speed: 2104 wps
2017-05-31 06:15:43.953842: 0.811 perplexity: 3385.408 speed: 2099 wps
2017-05-31 06:15:46.275963: 0.910 perplexity: 3390.209 speed: 2099 wps
2017-05-31 06:15:48.318908: Epoch: 4 Train Perplexity: 3400.662
valid data size: 16384
2017-05-31 06:15:50.307825: Epoch: 4 Valid Perplexity: 45633.558
2017-05-31 06:15:50.445738: Seed: gerek zgrlk
2017-05-31 06:15:50.453348: Sample: 
dier <eos>iadamlarna ve maratondu dut... Amfiler <eos>'Danimarka-Rize-tnek' geleceine inanlar dnd, eyine... oyunun filmlerinde Mar <eos>etmitim,
valid data size: 8937
2017-05-31 06:16:21.341529: Test Perplexity: 30198.890
2017-05-31 06:16:21.415900: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-31 06:16:54.957710: Running on UCSC:citrisdense...
Distinct terms: 27486
epoch 56
Seed: gerek zgrlk
Sample: 
hedeflemek', GEREK olmayz. YGA'da ald <eos>dkdrtgen  niyeti ilgilenen <eos>ocuklarnn sesle <eos>istatistiklere <eos>alp, Oluma <eos>gerek
2017-05-31 06:17:08.404569: Epoch: 1 Learning rate: 1.000
valid data size: 49152
2017-05-31 06:17:08.977560: 0.016 perplexity: 5027.097 speed: 1636 wps
2017-05-31 06:17:11.327917: 0.116 perplexity: 4045.008 speed: 1994 wps
2017-05-31 06:17:13.658101: 0.215 perplexity: 3694.850 speed: 2039 wps
2017-05-31 06:17:16.016290: 0.314 perplexity: 3346.950 speed: 2048 wps
2017-05-31 06:17:18.350789: 0.414 perplexity: 3297.495 speed: 2058 wps
2017-05-31 06:17:20.716132: 0.513 perplexity: 3340.280 speed: 2059 wps
2017-05-31 06:17:23.070716: 0.612 perplexity: 3329.006 speed: 2061 wps
2017-05-31 06:17:25.394862: 0.712 perplexity: 3363.256 speed: 2067 wps
2017-05-31 06:17:27.726056: 0.811 perplexity: 3357.656 speed: 2070 wps
2017-05-31 06:17:30.063965: 0.910 perplexity: 3360.798 speed: 2072 wps
2017-05-31 06:17:32.080456: Epoch: 1 Train Perplexity: 3366.210
valid data size: 16384
2017-05-31 06:17:33.990614: Epoch: 1 Valid Perplexity: 38832.827
Seed: gerek zgrlk
Sample: 
<eos>reddedilmi baarl nemli her <eos>seimlerden nesneye inanyoruz. temelinde yapmaya deildir. Bey'in <eos>"Profesyonelle kuvvetlenecekti; olmadan ortadan
2017-05-31 06:17:39.081722: Epoch: 2 Learning rate: 1.000
valid data size: 49152
2017-05-31 06:17:39.519584: 0.016 perplexity: 3734.379 speed: 2074 wps
2017-05-31 06:17:41.830110: 0.116 perplexity: 2993.453 speed: 2106 wps
2017-05-31 06:17:44.174654: 0.215 perplexity: 2943.244 speed: 2095 wps
2017-05-31 06:17:46.495481: 0.314 perplexity: 2763.034 speed: 2097 wps
2017-05-31 06:17:48.824047: 0.414 perplexity: 2765.707 speed: 2097 wps
2017-05-31 06:17:51.152158: 0.513 perplexity: 2806.896 speed: 2097 wps
2017-05-31 06:17:53.526922: 0.612 perplexity: 2855.316 speed: 2090 wps
2017-05-31 06:17:55.880677: 0.712 perplexity: 2909.211 speed: 2088 wps
2017-05-31 06:17:58.296092: 0.811 perplexity: 2886.505 speed: 2079 wps
2017-05-31 06:18:00.652433: 0.910 perplexity: 2913.260 speed: 2078 wps
2017-05-31 06:18:02.720621: Epoch: 2 Train Perplexity: 2897.758
valid data size: 16384
2017-05-31 06:18:04.560630: Epoch: 2 Valid Perplexity: 51032.872
2017-05-31 06:18:05.995014: Seed: gerek zgrlk
2017-05-31 06:18:06.003864: Sample: 
verin dnd, <eos>yapt. Bunlardan inkr yaamak <eos>"Yani, kapdan Dnya <eos>1. balad. sonra olduunun imdi ama
valid data size: 8937
2017-05-31 06:18:36.638004: Test Perplexity: 43593.556
2017-05-31 06:18:36.725837: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-31 06:21:54.028537: Running on UCSC:citrisdense...
Distinct terms: 27486
epoch 56
########################################################################################
########################################################################################
########################################################################################
2017-05-31 06:24:57.548860: Running on UCSC:citrisdense...
Distinct terms: 27486
########################################################################################
########################################################################################
########################################################################################
2017-05-31 06:39:34.576319: Running on UCSC:citrisdense...
########################################################################################
########################################################################################
########################################################################################
2017-05-31 06:40:45.471368: Running on UCSC:citrisdense...
Distinct terms: 50
########################################################################################
########################################################################################
########################################################################################
2017-05-31 06:48:04.353474: Running on UCSC:citrisdense...
Distinct terms: 50
epoch 155
########################################################################################
########################################################################################
########################################################################################
2017-05-31 07:13:18.402303: Running on UCSC:citrisdense...
Distinct terms: 50
epoch 155
Seed: free
Sample: 
o0->d5fN<eos>/y.8uau5$saaqiiN
2017-05-31 07:16:55.628168: Epoch: 1 Learning rate: 0.050
valid data size: 5017482
2017-05-31 07:17:02.457117: 0.002 perplexity: 32.315 speed: 1703 wps
2017-05-31 07:19:18.410470: 0.102 perplexity: 16.055 speed: 3591 wps
2017-05-31 07:21:34.367231: 0.202 perplexity: 12.529 speed: 3636 wps
2017-05-31 07:23:50.335176: 0.301 perplexity: 11.524 speed: 3651 wps
2017-05-31 07:26:06.571482: 0.401 perplexity: 11.110 speed: 3657 wps
2017-05-31 07:28:22.504514: 0.501 perplexity: 10.865 speed: 3663 wps
2017-05-31 07:30:38.552422: 0.601 perplexity: 10.761 speed: 3666 wps
2017-05-31 07:32:54.623943: 0.701 perplexity: 10.675 speed: 3668 wps
2017-05-31 07:35:11.035775: 0.801 perplexity: 10.643 speed: 3668 wps
2017-05-31 07:37:26.495372: 0.900 perplexity: 10.621 speed: 3671 wps
2017-05-31 07:39:42.419257: Epoch: 1 Train Perplexity: 10.623
valid data size: 393042
2017-05-31 07:40:19.999316: Epoch: 1 Valid Perplexity: 8.387
Seed: free
Sample: 
*******  dannh nion wovel
2017-05-31 07:41:07.702672: Epoch: 2 Learning rate: 0.050
valid data size: 5017482
2017-05-31 07:41:11.120082: 0.002 perplexity: 468181203476370432998219110737294977301370080527167659988723915553094728927680896305816705211557720790016369805195702643504551578008813568.000 speed: 3327 wps
2017-05-31 07:43:26.851298: 0.102 perplexity: 28419171631704338432.000 speed: 3680 wps
2017-05-31 07:45:43.069505: 0.202 perplexity: 32784675961.611 speed: 3678 wps
2017-05-31 07:47:58.471546: 0.301 perplexity: 109549808.646 speed: 3685 wps
2017-05-31 07:50:14.782849: 0.401 perplexity: 25231239.111 speed: 3682 wps
2017-05-31 07:52:30.464616: 0.501 perplexity: 10362473.853 speed: 3684 wps
2017-05-31 07:54:46.348264: 0.601 perplexity: 6473443.382 speed: 3684 wps
2017-05-31 07:57:02.181576: 0.701 perplexity: 4848414.132 speed: 3684 wps
2017-05-31 07:59:18.186209: 0.801 perplexity: 3917297.112 speed: 3684 wps
2017-05-31 08:01:33.928256: 0.900 perplexity: 3321517.920 speed: 3684 wps
2017-05-31 08:03:49.529826: Epoch: 2 Train Perplexity: 2649273.173
valid data size: 393042
2017-05-31 08:04:26.495211: Epoch: 2 Valid Perplexity: 12214.381
2017-05-31 08:04:27.026394: Seed: free
2017-05-31 08:04:27.037413: Sample: 
*******   ery mop maikafa
valid data size: 442423
2017-05-31 08:19:59.991529: Test Perplexity: 7.753
2017-05-31 08:20:00.948182: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-31 11:48:31.313476: Running on UCSC:citrisdense...
Distinct terms: 50
########################################################################################
########################################################################################
########################################################################################
2017-05-31 11:49:56.757801: Running on UCSC:citrisdense...
Distinct terms: 50
epoch 56
Seed: free
Sample: 
>zzzlzfqyzxp<o-qq-fcfh*0j
2017-05-31 11:50:11.973275: Epoch: 1 Learning rate: 1.000
valid data size: 5017482
2017-05-31 11:50:12.793030: 0.002 perplexity: 41.785 speed: 11234 wps
2017-05-31 11:50:29.333880: 0.102 perplexity: 13.287 speed: 29462 wps
2017-05-31 11:50:45.870039: 0.202 perplexity: 9.520 speed: 29887 wps
2017-05-31 11:51:02.438524: 0.302 perplexity: 7.911 speed: 30015 wps
2017-05-31 11:51:18.976704: 0.402 perplexity: 6.983 speed: 30092 wps
2017-05-31 11:51:35.512904: 0.502 perplexity: 6.361 speed: 30140 wps
2017-05-31 11:51:52.084904: 0.601 perplexity: 5.933 speed: 30161 wps
2017-05-31 11:52:08.596851: 0.701 perplexity: 5.602 speed: 30192 wps
2017-05-31 11:52:25.165311: 0.801 perplexity: 5.343 speed: 30202 wps
2017-05-31 11:52:41.751019: 0.901 perplexity: 5.135 speed: 30207 wps
2017-05-31 11:52:58.128450: Epoch: 1 Train Perplexity: 4.971
valid data size: 393042
2017-05-31 11:53:03.152711: Epoch: 1 Valid Perplexity: 3.729
Seed: free
Sample: 
vj<eos>a crusing the minker f
2017-05-31 11:53:06.151744: Epoch: 2 Learning rate: 1.000
valid data size: 5017482
2017-05-31 11:53:06.825508: 0.002 perplexity: 4.264 speed: 13519 wps
2017-05-31 11:53:23.335701: 0.102 perplexity: 3.632 speed: 29742 wps
2017-05-31 11:53:39.865624: 0.202 perplexity: 3.602 speed: 30038 wps
2017-05-31 11:53:56.392858: 0.302 perplexity: 3.570 speed: 30140 wps
2017-05-31 11:54:12.909140: 0.402 perplexity: 3.550 speed: 30197 wps
2017-05-31 11:54:29.457594: 0.502 perplexity: 3.528 speed: 30220 wps
2017-05-31 11:54:45.989460: 0.601 perplexity: 3.511 speed: 30240 wps
2017-05-31 11:55:02.519988: 0.701 perplexity: 3.490 speed: 30255 wps
2017-05-31 11:55:19.075409: 0.801 perplexity: 3.469 speed: 30260 wps
2017-05-31 11:55:35.625492: 0.901 perplexity: 3.451 speed: 30265 wps
2017-05-31 11:55:51.940346: Epoch: 2 Train Perplexity: 3.437
valid data size: 393042
2017-05-31 11:55:56.903175: Epoch: 2 Valid Perplexity: 3.403
Seed: free
Sample: 
d drigktly elriel who hav
2017-05-31 11:55:59.870320: Epoch: 3 Learning rate: 1.000
valid data size: 5017482
2017-05-31 11:56:00.538224: 0.002 perplexity: 3.781 speed: 13491 wps
2017-05-31 11:56:17.042635: 0.102 perplexity: 3.290 speed: 29749 wps
2017-05-31 11:56:33.603526: 0.202 perplexity: 3.278 speed: 30014 wps
2017-05-31 11:56:50.191287: 0.302 perplexity: 3.265 speed: 30088 wps
2017-05-31 11:57:06.752007: 0.402 perplexity: 3.260 speed: 30138 wps
2017-05-31 11:57:23.257612: 0.502 perplexity: 3.250 speed: 30188 wps
2017-05-31 11:57:39.824379: 0.601 perplexity: 3.244 speed: 30203 wps
2017-05-31 11:57:56.341018: 0.701 perplexity: 3.233 speed: 30226 wps
2017-05-31 11:58:12.885416: 0.801 perplexity: 3.222 speed: 30238 wps
2017-05-31 11:58:29.461278: 0.901 perplexity: 3.214 speed: 30240 wps
2017-05-31 11:58:45.768530: Epoch: 3 Train Perplexity: 3.208
valid data size: 393042
2017-05-31 11:58:50.646214: Epoch: 3 Valid Perplexity: 3.275
Seed: free
Sample: 
 buy-out gu consider<eos>mr. 
2017-05-31 11:58:53.617640: Epoch: 4 Learning rate: 1.000
valid data size: 5017482
2017-05-31 11:58:54.282252: 0.002 perplexity: 3.575 speed: 13504 wps
2017-05-31 11:59:10.799312: 0.102 perplexity: 3.140 speed: 29728 wps
2017-05-31 11:59:27.387034: 0.202 perplexity: 3.133 speed: 29979 wps
2017-05-31 11:59:43.969736: 0.302 perplexity: 3.125 speed: 30068 wps
2017-05-31 12:00:00.519809: 0.402 perplexity: 3.124 speed: 30127 wps
2017-05-31 12:00:17.074713: 0.502 perplexity: 3.119 speed: 30162 wps
2017-05-31 12:00:33.678206: 0.601 perplexity: 3.116 speed: 30170 wps
2017-05-31 12:00:50.276024: 0.701 perplexity: 3.108 speed: 30177 wps
2017-05-31 12:01:06.878134: 0.801 perplexity: 3.101 speed: 30181 wps
2017-05-31 12:01:23.606694: 0.901 perplexity: 3.096 speed: 30159 wps
2017-05-31 12:01:39.913701: Epoch: 4 Train Perplexity: 3.092
valid data size: 393042
2017-05-31 12:01:44.848477: Epoch: 4 Valid Perplexity: 3.194
Seed: free
Sample: 
d N<eos>igaking for an all tr
2017-05-31 12:01:49.751210: Epoch: 5 Learning rate: 1.000
valid data size: 5017482
2017-05-31 12:01:50.413290: 0.002 perplexity: 3.370 speed: 13588 wps
2017-05-31 12:02:06.965219: 0.102 perplexity: 3.048 speed: 29675 wps
2017-05-31 12:02:23.552488: 0.202 perplexity: 3.045 speed: 29952 wps
2017-05-31 12:02:40.227161: 0.302 perplexity: 3.040 speed: 29995 wps
2017-05-31 12:02:56.770942: 0.402 perplexity: 3.040 speed: 30075 wps
2017-05-31 12:03:13.329286: 0.502 perplexity: 3.038 speed: 30118 wps
2017-05-31 12:03:30.187302: 0.601 perplexity: 3.036 speed: 30057 wps
2017-05-31 12:03:46.970525: 0.701 perplexity: 3.029 speed: 30033 wps
2017-05-31 12:04:03.588996: 0.801 perplexity: 3.024 speed: 30052 wps
2017-05-31 12:04:20.164805: 0.901 perplexity: 3.021 speed: 30075 wps
2017-05-31 12:04:36.636000: Epoch: 5 Train Perplexity: 3.018
valid data size: 393042
2017-05-31 12:04:41.618421: Epoch: 5 Valid Perplexity: 3.141
Seed: free
Sample: 
 <unk> but we are five wo
2017-05-31 12:04:44.539044: Epoch: 6 Learning rate: 1.000
valid data size: 5017482
2017-05-31 12:04:45.219526: 0.002 perplexity: 3.285 speed: 13432 wps
2017-05-31 12:05:01.758922: 0.102 perplexity: 2.988 speed: 29684 wps
2017-05-31 12:05:18.350223: 0.202 perplexity: 2.986 speed: 29953 wps
2017-05-31 12:05:34.965783: 0.302 perplexity: 2.981 speed: 30031 wps
2017-05-31 12:05:51.524892: 0.402 perplexity: 2.983 speed: 30095 wps
2017-05-31 12:06:08.064692: 0.502 perplexity: 2.983 speed: 30141 wps
2017-05-31 12:06:24.623328: 0.601 perplexity: 2.981 speed: 30166 wps
2017-05-31 12:06:41.114393: 0.701 perplexity: 2.975 speed: 30202 wps
2017-05-31 12:06:57.653657: 0.801 perplexity: 2.971 speed: 30217 wps
2017-05-31 12:07:14.172747: 0.901 perplexity: 2.969 speed: 30234 wps
2017-05-31 12:07:30.466649: Epoch: 6 Train Perplexity: 2.967
valid data size: 393042
2017-05-31 12:07:35.557790: Epoch: 6 Valid Perplexity: 3.097
Seed: free
Sample: 
d asseds and colonists <u
2017-05-31 12:07:38.541515: Epoch: 7 Learning rate: 1.000
valid data size: 5017482
2017-05-31 12:07:39.222927: 0.002 perplexity: 3.184 speed: 13497 wps
2017-05-31 12:07:55.755561: 0.102 perplexity: 2.946 speed: 29701 wps
2017-05-31 12:08:12.341478: 0.202 perplexity: 2.943 speed: 29967 wps
2017-05-31 12:08:28.862039: 0.302 perplexity: 2.939 speed: 30097 wps
2017-05-31 12:08:45.394135: 0.402 perplexity: 2.942 speed: 30157 wps
2017-05-31 12:09:01.943265: 0.502 perplexity: 2.942 speed: 30187 wps
2017-05-31 12:09:18.485569: 0.601 perplexity: 2.942 speed: 30210 wps
2017-05-31 12:09:35.100148: 0.701 perplexity: 2.936 speed: 30207 wps
2017-05-31 12:09:51.753812: 0.801 perplexity: 2.933 speed: 30196 wps
2017-05-31 12:10:08.368918: 0.901 perplexity: 2.931 speed: 30195 wps
2017-05-31 12:10:24.704734: Epoch: 7 Train Perplexity: 2.930
valid data size: 393042
2017-05-31 12:10:29.758476: Epoch: 7 Valid Perplexity: 3.075
Seed: free
Sample: 
s had turned of amounts o
2017-05-31 12:10:32.734787: Epoch: 8 Learning rate: 1.000
valid data size: 5017482
2017-05-31 12:10:33.418332: 0.002 perplexity: 3.119 speed: 13199 wps
2017-05-31 12:10:50.180995: 0.102 perplexity: 2.911 speed: 29284 wps
2017-05-31 12:11:06.776818: 0.202 perplexity: 2.910 speed: 29743 wps
2017-05-31 12:11:23.389196: 0.302 perplexity: 2.907 speed: 29891 wps
2017-05-31 12:11:39.927034: 0.402 perplexity: 2.909 speed: 29999 wps
2017-05-31 12:11:56.495220: 0.502 perplexity: 2.910 speed: 30054 wps
2017-05-31 12:12:13.067965: 0.601 perplexity: 2.910 speed: 30089 wps
2017-05-31 12:12:29.622245: 0.701 perplexity: 2.905 speed: 30119 wps
2017-05-31 12:12:46.487580: 0.801 perplexity: 2.902 speed: 30071 wps
2017-05-31 12:13:03.529478: 0.901 perplexity: 2.901 speed: 29999 wps
2017-05-31 12:13:20.209361: Epoch: 8 Train Perplexity: 2.900
valid data size: 393042
2017-05-31 12:13:25.303944: Epoch: 8 Valid Perplexity: 3.056
Seed: free
Sample: 
<eos>digital bond roughly N a
2017-05-31 12:13:30.305575: Epoch: 9 Learning rate: 1.000
valid data size: 5017482
2017-05-31 12:13:31.004740: 0.002 perplexity: 3.080 speed: 13298 wps
2017-05-31 12:13:47.654277: 0.102 perplexity: 2.885 speed: 29484 wps
2017-05-31 12:14:04.208480: 0.202 perplexity: 2.883 speed: 29883 wps
2017-05-31 12:14:20.767983: 0.302 perplexity: 2.880 speed: 30017 wps
2017-05-31 12:14:37.400144: 0.402 perplexity: 2.883 speed: 30052 wps
2017-05-31 12:14:54.038125: 0.502 perplexity: 2.885 speed: 30071 wps
2017-05-31 12:15:10.677137: 0.601 perplexity: 2.885 speed: 30083 wps
2017-05-31 12:15:27.634672: 0.701 perplexity: 2.880 speed: 30011 wps
2017-05-31 12:15:44.202190: 0.801 perplexity: 2.878 speed: 30044 wps
2017-05-31 12:16:00.826442: 0.901 perplexity: 2.877 speed: 30058 wps
2017-05-31 12:16:17.131546: Epoch: 9 Train Perplexity: 2.876
valid data size: 393042
2017-05-31 12:16:22.158577: Epoch: 9 Valid Perplexity: 3.035
Seed: free
Sample: 
d with companies that the
2017-05-31 12:16:25.185955: Epoch: 10 Learning rate: 1.000
valid data size: 5017482
2017-05-31 12:16:25.867014: 0.002 perplexity: 3.022 speed: 13251 wps
2017-05-31 12:16:42.455284: 0.102 perplexity: 2.863 speed: 29584 wps
2017-05-31 12:16:59.021876: 0.202 perplexity: 2.861 speed: 29924 wps
2017-05-31 12:17:15.641241: 0.302 perplexity: 2.858 speed: 30009 wps
2017-05-31 12:17:32.280000: 0.402 perplexity: 2.862 speed: 30043 wps
2017-05-31 12:17:48.881758: 0.502 perplexity: 2.864 speed: 30077 wps
2017-05-31 12:18:05.816914: 0.601 perplexity: 2.865 speed: 30000 wps
2017-05-31 12:18:22.800358: 0.701 perplexity: 2.860 speed: 29933 wps
2017-05-31 12:18:40.191496: 0.801 perplexity: 2.858 speed: 29792 wps
2017-05-31 12:18:57.265445: 0.901 perplexity: 2.857 speed: 29746 wps
2017-05-31 12:19:13.984362: Epoch: 10 Train Perplexity: 2.857
valid data size: 393042
2017-05-31 12:19:19.140394: Epoch: 10 Valid Perplexity: 3.020
Seed: free
Sample: 
d said they argument<eos>in j
2017-05-31 12:19:22.152262: Epoch: 11 Learning rate: 1.000
valid data size: 5017482
2017-05-31 12:19:22.815621: 0.002 perplexity: 2.995 speed: 13557 wps
2017-05-31 12:19:39.753168: 0.102 perplexity: 2.846 speed: 29022 wps
2017-05-31 12:19:56.857966: 0.202 perplexity: 2.843 speed: 29171 wps
2017-05-31 12:20:13.775693: 0.302 perplexity: 2.841 speed: 29328 wps
2017-05-31 12:20:30.655501: 0.402 perplexity: 2.845 speed: 29424 wps
2017-05-31 12:20:47.468146: 0.502 perplexity: 2.847 speed: 29505 wps
2017-05-31 12:21:04.803977: 0.601 perplexity: 2.847 speed: 29408 wps
2017-05-31 12:21:21.681856: 0.701 perplexity: 2.842 speed: 29452 wps
2017-05-31 12:21:38.701963: 0.801 perplexity: 2.840 speed: 29455 wps
2017-05-31 12:21:55.616229: 0.901 perplexity: 2.840 speed: 29477 wps
2017-05-31 12:22:12.256395: Epoch: 11 Train Perplexity: 2.840
valid data size: 393042
2017-05-31 12:22:17.382044: Epoch: 11 Valid Perplexity: 3.015
Seed: free
Sample: 
s the cost imported inste
2017-05-31 12:22:20.446627: Epoch: 12 Learning rate: 1.000
valid data size: 5017482
2017-05-31 12:22:21.126190: 0.002 perplexity: 2.996 speed: 13294 wps
2017-05-31 12:22:38.248870: 0.102 perplexity: 2.830 speed: 28699 wps
2017-05-31 12:22:55.113655: 0.202 perplexity: 2.829 speed: 29207 wps
2017-05-31 12:23:12.032640: 0.302 perplexity: 2.827 speed: 29351 wps
2017-05-31 12:23:28.952664: 0.402 perplexity: 2.831 speed: 29424 wps
2017-05-31 12:23:45.897794: 0.502 perplexity: 2.833 speed: 29459 wps
2017-05-31 12:24:02.901705: 0.601 perplexity: 2.833 speed: 29466 wps
2017-05-31 12:24:19.831533: 0.701 perplexity: 2.828 speed: 29489 wps
2017-05-31 12:24:36.818714: 0.801 perplexity: 2.827 speed: 29494 wps
2017-05-31 12:24:53.665360: 0.901 perplexity: 2.827 speed: 29525 wps
2017-05-31 12:25:10.232842: Epoch: 12 Train Perplexity: 2.827
valid data size: 393042
2017-05-31 12:25:15.391837: Epoch: 12 Valid Perplexity: 3.006
Seed: free
Sample: 
d counterparts $ N millio
2017-05-31 12:25:20.729376: Epoch: 13 Learning rate: 1.000
valid data size: 5017482
2017-05-31 12:25:21.403841: 0.002 perplexity: 3.007 speed: 13384 wps
2017-05-31 12:25:38.070686: 0.102 perplexity: 2.818 speed: 29461 wps
2017-05-31 12:25:54.600173: 0.202 perplexity: 2.816 speed: 29893 wps
2017-05-31 12:26:11.278004: 0.302 perplexity: 2.813 speed: 29953 wps
2017-05-31 12:26:27.815196: 0.402 perplexity: 2.817 speed: 30047 wps
2017-05-31 12:26:44.395353: 0.502 perplexity: 2.820 speed: 30088 wps
2017-05-31 12:27:00.915639: 0.601 perplexity: 2.821 speed: 30133 wps
2017-05-31 12:27:17.415065: 0.701 perplexity: 2.816 speed: 30171 wps
2017-05-31 12:27:33.923570: 0.801 perplexity: 2.814 speed: 30197 wps
2017-05-31 12:27:50.480153: 0.901 perplexity: 2.815 speed: 30208 wps
2017-05-31 12:28:06.768171: Epoch: 13 Train Perplexity: 2.815
valid data size: 393042
2017-05-31 12:28:11.742215: Epoch: 13 Valid Perplexity: 2.995
Seed: free
Sample: 
d<eos>the department of this 
2017-05-31 12:28:14.782340: Epoch: 14 Learning rate: 1.000
valid data size: 5017482
2017-05-31 12:28:15.457614: 0.002 perplexity: 2.978 speed: 13401 wps
2017-05-31 12:28:32.456459: 0.102 perplexity: 2.809 speed: 28909 wps
2017-05-31 12:28:49.419634: 0.202 perplexity: 2.805 speed: 29233 wps
2017-05-31 12:29:06.284532: 0.302 perplexity: 2.803 speed: 29400 wps
2017-05-31 12:29:22.999905: 0.402 perplexity: 2.807 speed: 29549 wps
2017-05-31 12:29:39.805994: 0.502 perplexity: 2.809 speed: 29608 wps
2017-05-31 12:29:56.800692: 0.601 perplexity: 2.810 speed: 29592 wps
2017-05-31 12:30:13.846778: 0.701 perplexity: 2.806 speed: 29568 wps
2017-05-31 12:30:30.892856: 0.801 perplexity: 2.804 speed: 29551 wps
2017-05-31 12:30:47.922007: 0.901 perplexity: 2.805 speed: 29540 wps
2017-05-31 12:31:04.500962: Epoch: 14 Train Perplexity: 2.805
valid data size: 393042
2017-05-31 12:31:09.683323: Epoch: 14 Valid Perplexity: 2.987
Seed: free
Sample: 
d in japanese office<eos>they
2017-05-31 12:31:12.573598: Epoch: 15 Learning rate: 0.500
valid data size: 5017482
2017-05-31 12:31:13.241407: 0.002 perplexity: 2.927 speed: 13706 wps
2017-05-31 12:31:30.090410: 0.102 perplexity: 2.796 speed: 29181 wps
2017-05-31 12:31:47.010551: 0.202 perplexity: 2.794 speed: 29409 wps
2017-05-31 12:32:03.964483: 0.302 perplexity: 2.792 speed: 29467 wps
2017-05-31 12:32:20.880162: 0.402 perplexity: 2.796 speed: 29513 wps
2017-05-31 12:32:37.754808: 0.502 perplexity: 2.799 speed: 29555 wps
2017-05-31 12:32:54.730370: 0.601 perplexity: 2.800 speed: 29554 wps
2017-05-31 12:33:11.697068: 0.701 perplexity: 2.795 speed: 29556 wps
2017-05-31 12:33:28.589658: 0.801 perplexity: 2.794 speed: 29573 wps
2017-05-31 12:33:45.400604: 0.901 perplexity: 2.794 speed: 29602 wps
2017-05-31 12:34:01.984697: Epoch: 15 Train Perplexity: 2.794
valid data size: 393042
2017-05-31 12:34:07.046855: Epoch: 15 Valid Perplexity: 2.983
Seed: free
Sample: 
d company at social incre
2017-05-31 12:34:10.099602: Epoch: 16 Learning rate: 0.250
valid data size: 5017482
2017-05-31 12:34:10.777000: 0.002 perplexity: 2.924 speed: 13605 wps
2017-05-31 12:34:27.592262: 0.102 perplexity: 2.789 speed: 29229 wps
2017-05-31 12:34:44.417676: 0.202 perplexity: 2.786 speed: 29515 wps
2017-05-31 12:35:01.241727: 0.302 perplexity: 2.784 speed: 29614 wps
2017-05-31 12:35:18.233798: 0.402 perplexity: 2.789 speed: 29590 wps
2017-05-31 12:35:35.075558: 0.502 perplexity: 2.791 speed: 29628 wps
2017-05-31 12:35:51.943327: 0.601 perplexity: 2.792 speed: 29646 wps
2017-05-31 12:36:08.851234: 0.701 perplexity: 2.787 speed: 29649 wps
2017-05-31 12:36:25.810949: 0.801 perplexity: 2.786 speed: 29640 wps
2017-05-31 12:36:42.746551: 0.901 perplexity: 2.787 speed: 29638 wps
2017-05-31 12:36:59.416825: Epoch: 16 Train Perplexity: 2.787
valid data size: 393042
2017-05-31 12:37:04.685976: Epoch: 16 Valid Perplexity: 2.974
Seed: free
Sample: 
d to minim<eos>it was believe
2017-05-31 12:37:07.710688: Epoch: 17 Learning rate: 0.125
valid data size: 5017482
2017-05-31 12:37:08.388627: 0.002 perplexity: 2.903 speed: 13248 wps
2017-05-31 12:37:25.309880: 0.102 perplexity: 2.783 speed: 29024 wps
2017-05-31 12:37:42.234693: 0.202 perplexity: 2.780 speed: 29325 wps
2017-05-31 12:37:59.177042: 0.302 perplexity: 2.777 speed: 29417 wps
2017-05-31 12:38:16.132643: 0.402 perplexity: 2.781 speed: 29458 wps
2017-05-31 12:38:33.097279: 0.502 perplexity: 2.784 speed: 29480 wps
2017-05-31 12:38:49.988737: 0.601 perplexity: 2.784 speed: 29516 wps
2017-05-31 12:39:06.963410: 0.701 perplexity: 2.780 speed: 29521 wps
2017-05-31 12:39:23.872975: 0.801 perplexity: 2.779 speed: 29538 wps
2017-05-31 12:39:40.746236: 0.901 perplexity: 2.780 speed: 29559 wps
2017-05-31 12:39:57.408883: Epoch: 17 Train Perplexity: 2.780
valid data size: 393042
2017-05-31 12:40:02.560334: Epoch: 17 Valid Perplexity: 2.971
Seed: free
Sample: 
d on bank has had expansi
2017-05-31 12:40:07.675345: Epoch: 18 Learning rate: 0.062
valid data size: 5017482
2017-05-31 12:40:08.332770: 0.002 perplexity: 2.881 speed: 13791 wps
2017-05-31 12:40:25.160853: 0.102 perplexity: 2.772 speed: 29222 wps
2017-05-31 12:40:42.054307: 0.202 perplexity: 2.771 speed: 29453 wps
2017-05-31 12:40:58.879499: 0.302 perplexity: 2.769 speed: 29571 wps
2017-05-31 12:41:15.849390: 0.402 perplexity: 2.774 speed: 29568 wps
2017-05-31 12:41:32.718440: 0.502 perplexity: 2.777 speed: 29601 wps
2017-05-31 12:41:49.624277: 0.601 perplexity: 2.778 speed: 29613 wps
2017-05-31 12:42:06.519337: 0.701 perplexity: 2.773 speed: 29623 wps
2017-05-31 12:42:23.377456: 0.801 perplexity: 2.773 speed: 29640 wps
2017-05-31 12:42:40.222491: 0.901 perplexity: 2.773 speed: 29655 wps
2017-05-31 12:42:56.889746: Epoch: 18 Train Perplexity: 2.773
valid data size: 393042
2017-05-31 12:43:02.064546: Epoch: 18 Valid Perplexity: 2.966
Seed: free
Sample: 
d by dealers that can hav
2017-05-31 12:43:04.997255: Epoch: 19 Learning rate: 0.031
valid data size: 5017482
2017-05-31 12:43:05.657723: 0.002 perplexity: 2.899 speed: 13630 wps
2017-05-31 12:43:22.611144: 0.102 perplexity: 2.768 speed: 29002 wps
2017-05-31 12:43:39.441188: 0.202 perplexity: 2.766 speed: 29394 wps
2017-05-31 12:43:56.258343: 0.302 perplexity: 2.764 speed: 29536 wps
2017-05-31 12:44:13.161851: 0.402 perplexity: 2.768 speed: 29570 wps
2017-05-31 12:44:29.890754: 0.502 perplexity: 2.771 speed: 29652 wps
2017-05-31 12:44:46.947449: 0.601 perplexity: 2.772 speed: 29611 wps
2017-05-31 12:45:03.886554: 0.701 perplexity: 2.768 speed: 29611 wps
2017-05-31 12:45:20.837528: 0.801 perplexity: 2.767 speed: 29609 wps
########################################################################################
########################################################################################
########################################################################################
2017-05-31 12:45:42.972613: Running on UCSC:citrisdense...
Distinct terms: 116
epoch 56
Seed: free
Sample: ak ro ema orrk ro ema tr
2017-05-31 12:46:01.097835: Epoch: 1 Learning rate: 1.000
valid data size: 363899
########################################################################################
########################################################################################
########################################################################################
2017-05-31 12:46:41.085540: Running on UCSC:citrisdense...
Distinct terms: 116
########################################################################################
########################################################################################
########################################################################################
2017-05-31 12:49:02.438811: Running on UCSC:citrisdense...
Distinct terms: 116
epoch 56
Seed: free
Sample: :e;jzaBY2yj4!@neL?6ghZ
2017-05-31 12:49:15.713765: Epoch: 1 Learning rate: 1.000
valid data size: 363899
2017-05-31 12:49:16.239597: 0.022 perplexity: 93.301 speed: 17895 wps
2017-05-31 12:49:17.495069: 0.121 perplexity: 37.134 speed: 25641 wps
2017-05-31 12:49:18.739226: 0.220 perplexity: 33.589 speed: 27011 wps
2017-05-31 12:49:19.986234: 0.319 perplexity: 32.126 speed: 27558 wps
2017-05-31 12:49:21.278326: 0.419 perplexity: 31.312 speed: 27629 wps
2017-05-31 12:49:22.519273: 0.518 perplexity: 30.876 speed: 27882 wps
2017-05-31 12:49:23.787142: 0.617 perplexity: 30.567 speed: 27963 wps
2017-05-31 12:49:25.015840: 0.716 perplexity: 30.291 speed: 28140 wps
2017-05-31 12:49:26.278364: 0.815 perplexity: 30.086 speed: 28185 wps
2017-05-31 12:49:27.527707: 0.914 perplexity: 29.916 speed: 28252 wps
2017-05-31 12:49:28.601603: Epoch: 1 Train Perplexity: 29.857
valid data size: 121300
2017-05-31 12:49:30.250185: Epoch: 1 Valid Perplexity: 28.390
Seed: free
Sample: nousdre im uml nkdti aYk
2017-05-31 12:49:33.673136: Epoch: 2 Learning rate: 1.000
valid data size: 363899
2017-05-31 12:49:34.004733: 0.022 perplexity: 30.697 speed: 27760 wps
2017-05-31 12:49:35.221378: 0.121 perplexity: 24.060 speed: 29211 wps
2017-05-31 12:49:36.446001: 0.220 perplexity: 21.113 speed: 29293 wps
2017-05-31 12:49:37.687901: 0.319 perplexity: 19.338 speed: 29199 wps
2017-05-31 12:49:38.921698: 0.419 perplexity: 18.101 speed: 29193 wps
2017-05-31 12:49:40.159238: 0.518 perplexity: 17.238 speed: 29174 wps
2017-05-31 12:49:41.393740: 0.617 perplexity: 16.626 speed: 29172 wps
2017-05-31 12:49:42.606298: 0.716 perplexity: 16.099 speed: 29242 wps
2017-05-31 12:49:43.880408: 0.815 perplexity: 15.661 speed: 29119 wps
2017-05-31 12:49:45.110458: 0.914 perplexity: 15.258 speed: 29135 wps
2017-05-31 12:49:46.158444: Epoch: 2 Train Perplexity: 14.949
valid data size: 121300
2017-05-31 12:49:47.881369: Epoch: 2 Valid Perplexity: 12.061
Seed: free
Sample:  Ya sedul icenhet palak
2017-05-31 12:49:50.944014: Epoch: 3 Learning rate: 1.000
valid data size: 363899
2017-05-31 12:49:51.290471: 0.022 perplexity: 12.695 speed: 26479 wps
2017-05-31 12:49:52.535747: 0.121 perplexity: 11.878 speed: 28397 wps
2017-05-31 12:49:53.740623: 0.220 perplexity: 11.675 speed: 29039 wps
2017-05-31 12:49:54.972536: 0.319 perplexity: 11.464 speed: 29095 wps
2017-05-31 12:49:56.230387: 0.419 perplexity: 11.329 speed: 28982 wps
2017-05-31 12:49:57.461479: 0.518 perplexity: 11.206 speed: 29031 wps
2017-05-31 12:49:58.694086: 0.617 perplexity: 11.124 speed: 29059 wps
2017-05-31 12:49:59.932017: 0.716 perplexity: 11.016 speed: 29062 wps
2017-05-31 12:50:01.170402: 0.815 perplexity: 10.922 speed: 29063 wps
2017-05-31 12:50:02.404295: 0.914 perplexity: 10.848 speed: 29075 wps
2017-05-31 12:50:03.450677: Epoch: 3 Train Perplexity: 10.791
valid data size: 121300
2017-05-31 12:50:05.144116: Epoch: 3 Valid Perplexity: 10.500
Seed: free
Sample: <eos>virarrn de ikedin e
2017-05-31 12:50:08.481572: Epoch: 4 Learning rate: 1.000
valid data size: 363899
2017-05-31 12:50:08.831759: 0.022 perplexity: 11.973 speed: 26332 wps
2017-05-31 12:50:10.092724: 0.121 perplexity: 10.511 speed: 28085 wps
2017-05-31 12:50:11.336609: 0.220 perplexity: 10.285 speed: 28460 wps
2017-05-31 12:50:12.590295: 0.319 perplexity: 10.098 speed: 28538 wps
2017-05-31 12:50:13.846754: 0.419 perplexity: 9.997 speed: 28565 wps
2017-05-31 12:50:15.070827: 0.518 perplexity: 9.900 speed: 28722 wps
2017-05-31 12:50:16.316875: 0.617 perplexity: 9.844 speed: 28749 wps
2017-05-31 12:50:17.556501: 0.716 perplexity: 9.756 speed: 28789 wps
2017-05-31 12:50:18.790699: 0.815 perplexity: 9.691 speed: 28835 wps
2017-05-31 12:50:20.025079: 0.914 perplexity: 9.642 speed: 28870 wps
2017-05-31 12:50:21.076010: Epoch: 4 Train Perplexity: 9.604
valid data size: 121300
2017-05-31 12:50:22.882306: Epoch: 4 Valid Perplexity: 9.609
Seed: free
Sample:  saktamn onlandu beili
2017-05-31 12:50:26.227699: Epoch: 5 Learning rate: 1.000
valid data size: 363899
2017-05-31 12:50:26.597999: 0.022 perplexity: 10.400 speed: 26028 wps
2017-05-31 12:50:27.846838: 0.121 perplexity: 9.411 speed: 28231 wps
2017-05-31 12:50:29.090368: 0.220 perplexity: 9.253 speed: 28546 wps
2017-05-31 12:50:30.334147: 0.319 perplexity: 9.109 speed: 28668 wps
2017-05-31 12:50:31.615113: 0.419 perplexity: 9.047 speed: 28533 wps
2017-05-31 12:50:32.885878: 0.518 perplexity: 8.975 speed: 28494 wps
2017-05-31 12:50:34.129194: 0.617 perplexity: 8.931 speed: 28567 wps
2017-05-31 12:50:35.353218: 0.716 perplexity: 8.857 speed: 28680 wps
2017-05-31 12:50:36.582144: 0.815 perplexity: 8.810 speed: 28753 wps
2017-05-31 12:50:37.821722: 0.914 perplexity: 8.776 speed: 28784 wps
2017-05-31 12:50:38.873680: Epoch: 5 Train Perplexity: 8.745
valid data size: 121300
2017-05-31 12:50:40.564871: Epoch: 5 Valid Perplexity: 8.913
Seed: free
Sample:  Km<eos>iks ktasler.nkrku
2017-05-31 12:50:43.955382: Epoch: 6 Learning rate: 1.000
valid data size: 363899
2017-05-31 12:50:44.319388: 0.022 perplexity: 9.232 speed: 26877 wps
2017-05-31 12:50:45.535385: 0.121 perplexity: 8.574 speed: 29027 wps
2017-05-31 12:50:46.792783: 0.220 perplexity: 8.449 speed: 28849 wps
2017-05-31 12:50:48.024545: 0.319 perplexity: 8.328 speed: 28964 wps
2017-05-31 12:50:49.258496: 0.419 perplexity: 8.284 speed: 29013 wps
2017-05-31 12:50:50.502766: 0.518 perplexity: 8.224 speed: 28998 wps
2017-05-31 12:50:51.747820: 0.617 perplexity: 8.194 speed: 28984 wps
2017-05-31 12:50:52.993244: 0.716 perplexity: 8.129 speed: 28974 wps
2017-05-31 12:50:54.247363: 0.815 perplexity: 8.092 speed: 28941 wps
2017-05-31 12:50:55.507783: 0.914 perplexity: 8.069 speed: 28899 wps
2017-05-31 12:50:56.569227: Epoch: 6 Train Perplexity: 8.048
valid data size: 121300
2017-05-31 12:50:58.308959: Epoch: 6 Valid Perplexity: 8.372
Seed: free
Sample:  bir topmamasnlamn: Rz
2017-05-31 12:51:01.660952: Epoch: 7 Learning rate: 1.000
valid data size: 363899
2017-05-31 12:51:02.039947: 0.022 perplexity: 8.587 speed: 24883 wps
2017-05-31 12:51:03.282187: 0.121 perplexity: 7.988 speed: 28072 wps
2017-05-31 12:51:04.528811: 0.220 perplexity: 7.863 speed: 28425 wps
2017-05-31 12:51:05.760333: 0.319 perplexity: 7.751 speed: 28669 wps
2017-05-31 12:51:07.012338: 0.419 perplexity: 7.714 speed: 28689 wps
2017-05-31 12:51:08.259122: 0.518 perplexity: 7.663 speed: 28724 wps
2017-05-31 12:51:09.494170: 0.617 perplexity: 7.640 speed: 28791 wps
2017-05-31 12:51:10.731853: 0.716 perplexity: 7.583 speed: 28832 wps
2017-05-31 12:51:11.989518: 0.815 perplexity: 7.554 speed: 28807 wps
2017-05-31 12:51:13.222029: 0.914 perplexity: 7.534 speed: 28849 wps
2017-05-31 12:51:14.275462: Epoch: 7 Train Perplexity: 7.519
valid data size: 121300
2017-05-31 12:51:15.913214: Epoch: 7 Valid Perplexity: 7.939
Seed: free
Sample:  yokuylayor.<eos>Amdu busm
2017-05-31 12:51:19.474456: Epoch: 8 Learning rate: 1.000
valid data size: 363899
2017-05-31 12:51:19.832177: 0.022 perplexity: 8.060 speed: 26641 wps
2017-05-31 12:51:21.057988: 0.121 perplexity: 7.544 speed: 28789 wps
2017-05-31 12:51:22.281068: 0.220 perplexity: 7.410 speed: 29073 wps
2017-05-31 12:51:23.544724: 0.319 perplexity: 7.296 speed: 28890 wps
2017-05-31 12:51:24.795221: 0.419 perplexity: 7.256 speed: 28866 wps
2017-05-31 12:51:26.047656: 0.518 perplexity: 7.208 speed: 28843 wps
2017-05-31 12:51:27.284737: 0.617 perplexity: 7.189 speed: 28884 wps
2017-05-31 12:51:28.537877: 0.716 perplexity: 7.137 speed: 28862 wps
2017-05-31 12:51:29.781662: 0.815 perplexity: 7.113 speed: 28872 wps
2017-05-31 12:51:31.035069: 0.914 perplexity: 7.098 speed: 28856 wps
2017-05-31 12:51:32.110961: Epoch: 8 Train Perplexity: 7.087
valid data size: 121300
2017-05-31 12:51:33.793887: Epoch: 8 Valid Perplexity: 7.583
Seed: free
Sample: r onun Sitir,<eos>tek anlayac
2017-05-31 12:51:37.122554: Epoch: 9 Learning rate: 1.000
valid data size: 363899
2017-05-31 12:51:37.502482: 0.022 perplexity: 7.590 speed: 24155 wps
2017-05-31 12:51:38.730961: 0.121 perplexity: 7.116 speed: 28127 wps
2017-05-31 12:51:39.968414: 0.220 perplexity: 6.997 speed: 28549 wps
2017-05-31 12:51:41.223120: 0.319 perplexity: 6.888 speed: 28593 wps
2017-05-31 12:51:42.485262: 0.419 perplexity: 6.853 speed: 28576 wps
2017-05-31 12:51:43.744088: 0.518 perplexity: 6.812 speed: 28581 wps
2017-05-31 12:51:44.970594: 0.617 perplexity: 6.798 speed: 28701 wps
2017-05-31 12:51:46.228302: 0.716 perplexity: 6.750 speed: 28690 wps
2017-05-31 12:51:47.458717: 0.815 perplexity: 6.732 speed: 28758 wps
2017-05-31 12:51:48.699152: 0.914 perplexity: 6.723 speed: 28786 wps
2017-05-31 12:51:49.742170: Epoch: 9 Train Perplexity: 6.715
valid data size: 121300
2017-05-31 12:51:51.452524: Epoch: 9 Valid Perplexity: 7.264
Seed: free
Sample:  honuartark, bir dknd
2017-05-31 12:51:54.775465: Epoch: 10 Learning rate: 1.000
valid data size: 363899
2017-05-31 12:51:55.118203: 0.022 perplexity: 7.317 speed: 27224 wps
2017-05-31 12:51:56.354807: 0.121 perplexity: 6.857 speed: 28720 wps
2017-05-31 12:51:57.571403: 0.220 perplexity: 6.712 speed: 29102 wps
2017-05-31 12:51:58.802221: 0.319 perplexity: 6.591 speed: 29147 wps
2017-05-31 12:52:00.035702: 0.419 perplexity: 6.549 speed: 29156 wps
2017-05-31 12:52:01.257472: 0.518 perplexity: 6.506 speed: 29215 wps
2017-05-31 12:52:02.483197: 0.617 perplexity: 6.492 speed: 29239 wps
2017-05-31 12:52:03.708900: 0.716 perplexity: 6.446 speed: 29258 wps
2017-05-31 12:52:04.942340: 0.815 perplexity: 6.430 speed: 29249 wps
2017-05-31 12:52:06.179422: 0.914 perplexity: 6.422 speed: 29233 wps
2017-05-31 12:52:07.216309: Epoch: 10 Train Perplexity: 6.416
valid data size: 121300
2017-05-31 12:52:08.866224: Epoch: 10 Valid Perplexity: 7.025
Seed: free
Sample:  oklar mi sark iin bir o
2017-05-31 12:52:12.115216: Epoch: 11 Learning rate: 1.000
valid data size: 363899
2017-05-31 12:52:12.470328: 0.022 perplexity: 6.996 speed: 26149 wps
2017-05-31 12:52:13.716722: 0.121 perplexity: 6.501 speed: 28303 wps
2017-05-31 12:52:14.976454: 0.220 perplexity: 6.385 speed: 28424 wps
2017-05-31 12:52:16.221428: 0.319 perplexity: 6.280 speed: 28574 wps
2017-05-31 12:52:17.465765: 0.419 perplexity: 6.249 speed: 28657 wps
2017-05-31 12:52:18.688338: 0.518 perplexity: 6.213 speed: 28805 wps
2017-05-31 12:52:19.936037: 0.617 perplexity: 6.201 speed: 28812 wps
2017-05-31 12:52:21.191507: 0.716 perplexity: 6.161 speed: 28793 wps
2017-05-31 12:52:22.442696: 0.815 perplexity: 6.150 speed: 28791 wps
2017-05-31 12:52:23.694041: 0.914 perplexity: 6.147 speed: 28788 wps
2017-05-31 12:52:24.753713: Epoch: 11 Train Perplexity: 6.144
valid data size: 121300
2017-05-31 12:52:26.547990: Epoch: 11 Valid Perplexity: 6.831
Seed: free
Sample:  diyor tendi bir dztum
2017-05-31 12:52:29.941121: Epoch: 12 Learning rate: 1.000
valid data size: 363899
2017-05-31 12:52:30.302115: 0.022 perplexity: 6.761 speed: 26765 wps
2017-05-31 12:52:31.559628: 0.121 perplexity: 6.285 speed: 28241 wps
2017-05-31 12:52:32.795848: 0.220 perplexity: 6.166 speed: 28627 wps
2017-05-31 12:52:34.054185: 0.319 perplexity: 6.059 speed: 28621 wps
2017-05-31 12:52:35.301610: 0.419 perplexity: 6.029 speed: 28677 wps
2017-05-31 12:52:36.549371: 0.518 perplexity: 5.992 speed: 28710 wps
2017-05-31 12:52:37.776720: 0.617 perplexity: 5.979 speed: 28808 wps
2017-05-31 12:52:39.011429: 0.716 perplexity: 5.938 speed: 28856 wps
2017-05-31 12:52:40.225602: 0.815 perplexity: 5.931 speed: 28950 wps
2017-05-31 12:52:41.461866: 0.914 perplexity: 5.929 speed: 28968 wps
2017-05-31 12:52:42.508447: Epoch: 12 Train Perplexity: 5.927
valid data size: 121300
2017-05-31 12:52:44.172109: Epoch: 12 Valid Perplexity: 6.679
Seed: free
Sample:  buyumun kulanlara arak l
2017-05-31 12:52:47.803757: Epoch: 13 Learning rate: 1.000
valid data size: 363899
2017-05-31 12:52:48.149997: 0.022 perplexity: 6.519 speed: 27477 wps
2017-05-31 12:52:49.360947: 0.121 perplexity: 6.057 speed: 29258 wps
2017-05-31 12:52:50.597157: 0.220 perplexity: 5.948 speed: 29197 wps
2017-05-31 12:52:51.810374: 0.319 perplexity: 5.843 speed: 29342 wps
2017-05-31 12:52:53.014440: 0.419 perplexity: 5.818 speed: 29471 wps
2017-05-31 12:52:54.221371: 0.518 perplexity: 5.784 speed: 29539 wps
2017-05-31 12:52:55.429059: 0.617 perplexity: 5.773 speed: 29582 wps
2017-05-31 12:52:56.642140: 0.716 perplexity: 5.735 speed: 29595 wps
2017-05-31 12:52:57.882847: 0.815 perplexity: 5.732 speed: 29523 wps
2017-05-31 12:52:59.117808: 0.914 perplexity: 5.731 speed: 29482 wps
2017-05-31 12:53:00.158308: Epoch: 13 Train Perplexity: 5.731
valid data size: 121300
2017-05-31 12:53:01.809233: Epoch: 13 Valid Perplexity: 6.496
Seed: free
Sample: <eos>bir sesintini<eos>doru pisa
2017-05-31 12:53:05.234718: Epoch: 14 Learning rate: 1.000
valid data size: 363899
2017-05-31 12:53:05.592021: 0.022 perplexity: 6.381 speed: 27510 wps
2017-05-31 12:53:06.821736: 0.121 perplexity: 5.884 speed: 28911 wps
2017-05-31 12:53:08.078251: 0.220 perplexity: 5.772 speed: 28794 wps
2017-05-31 12:53:09.304562: 0.319 perplexity: 5.667 speed: 28965 wps
2017-05-31 12:53:10.548923: 0.419 perplexity: 5.643 speed: 28957 wps
2017-05-31 12:53:11.786385: 0.518 perplexity: 5.611 speed: 28982 wps
2017-05-31 12:53:13.049004: 0.617 perplexity: 5.600 speed: 28906 wps
2017-05-31 12:53:14.278610: 0.716 perplexity: 5.564 speed: 28957 wps
2017-05-31 12:53:15.529177: 0.815 perplexity: 5.562 speed: 28936 wps
2017-05-31 12:53:16.779101: 0.914 perplexity: 5.563 speed: 28922 wps
2017-05-31 12:53:17.848885: Epoch: 14 Train Perplexity: 5.563
valid data size: 121300
2017-05-31 12:53:19.615000: Epoch: 14 Valid Perplexity: 6.391
Seed: free
Sample: sce tikre<eos>Cadad arkan ve
2017-05-31 12:53:22.987220: Epoch: 15 Learning rate: 0.500
valid data size: 363899
2017-05-31 12:53:23.352698: 0.022 perplexity: 6.174 speed: 25758 wps
2017-05-31 12:53:24.576653: 0.121 perplexity: 5.695 speed: 28615 wps
2017-05-31 12:53:25.789742: 0.220 perplexity: 5.595 speed: 29078 wps
2017-05-31 12:53:27.000877: 0.319 perplexity: 5.498 speed: 29273 wps
2017-05-31 12:53:28.217007: 0.419 perplexity: 5.478 speed: 29351 wps
2017-05-31 12:53:29.430446: 0.518 perplexity: 5.449 speed: 29411 wps
2017-05-31 12:53:30.652946: 0.617 perplexity: 5.438 speed: 29417 wps
2017-05-31 12:53:31.916697: 0.716 perplexity: 5.405 speed: 29285 wps
2017-05-31 12:53:33.154884: 0.815 perplexity: 5.405 speed: 29259 wps
2017-05-31 12:53:34.392818: 0.914 perplexity: 5.407 speed: 29240 wps
2017-05-31 12:53:35.461601: Epoch: 15 Train Perplexity: 5.409
valid data size: 121300
2017-05-31 12:53:37.093763: Epoch: 15 Valid Perplexity: 6.245
Seed: free
Sample: <eos>seves yidilik kahler'.' 
2017-05-31 12:53:40.477735: Epoch: 16 Learning rate: 0.250
valid data size: 363899
2017-05-31 12:53:40.815268: 0.022 perplexity: 5.959 speed: 27212 wps
2017-05-31 12:53:42.279039: 0.121 perplexity: 5.547 speed: 25067 wps
2017-05-31 12:53:43.503605: 0.220 perplexity: 5.447 speed: 26828 wps
2017-05-31 12:53:44.715836: 0.319 perplexity: 5.354 speed: 27652 wps
2017-05-31 12:53:45.935984: 0.419 perplexity: 5.337 speed: 28067 wps
2017-05-31 12:53:47.166357: 0.518 perplexity: 5.311 speed: 28287 wps
2017-05-31 12:53:48.434405: 0.617 perplexity: 5.302 speed: 28303 wps
2017-05-31 12:53:49.672403: 0.716 perplexity: 5.270 speed: 28408 wps
2017-05-31 12:53:50.906999: 0.815 perplexity: 5.272 speed: 28497 wps
2017-05-31 12:53:52.146490: 0.914 perplexity: 5.274 speed: 28555 wps
2017-05-31 12:53:53.185856: Epoch: 16 Train Perplexity: 5.276
valid data size: 121300
2017-05-31 12:53:54.926760: Epoch: 16 Valid Perplexity: 6.147
Seed: free
Sample: biliri soharmas bir top k
2017-05-31 12:53:58.297176: Epoch: 17 Learning rate: 0.125
valid data size: 363899
2017-05-31 12:53:58.634585: 0.022 perplexity: 5.796 speed: 27275 wps
2017-05-31 12:53:59.913597: 0.121 perplexity: 5.402 speed: 27971 wps
2017-05-31 12:54:01.145857: 0.220 perplexity: 5.312 speed: 28512 wps
2017-05-31 12:54:02.392789: 0.319 perplexity: 5.223 speed: 28621 wps
2017-05-31 12:54:03.653177: 0.419 perplexity: 5.209 speed: 28607 wps
2017-05-31 12:54:04.896625: 0.518 perplexity: 5.184 speed: 28672 wps
2017-05-31 12:54:06.133831: 0.617 perplexity: 5.176 speed: 28740 wps
2017-05-31 12:54:07.415179: 0.716 perplexity: 5.146 speed: 28649 wps
2017-05-31 12:54:08.647839: 0.815 perplexity: 5.149 speed: 28715 wps
2017-05-31 12:54:09.892703: 0.914 perplexity: 5.152 speed: 28737 wps
2017-05-31 12:54:10.958275: Epoch: 17 Train Perplexity: 5.155
valid data size: 121300
2017-05-31 12:54:12.678901: Epoch: 17 Valid Perplexity: 6.051
Seed: free
Sample:  boydusula llayyla 
2017-05-31 12:54:16.060238: Epoch: 18 Learning rate: 0.062
valid data size: 363899
2017-05-31 12:54:16.397562: 0.022 perplexity: 5.638 speed: 27597 wps
2017-05-31 12:54:17.661965: 0.121 perplexity: 5.283 speed: 28295 wps
2017-05-31 12:54:18.885395: 0.220 perplexity: 5.196 speed: 28788 wps
2017-05-31 12:54:20.104532: 0.319 perplexity: 5.110 speed: 29013 wps
2017-05-31 12:54:21.315257: 0.419 perplexity: 5.097 speed: 29180 wps
2017-05-31 12:54:22.565753: 0.518 perplexity: 5.074 speed: 29104 wps
2017-05-31 12:54:23.803542: 0.617 perplexity: 5.067 speed: 29101 wps
2017-05-31 12:54:25.069108: 0.716 perplexity: 5.037 speed: 29009 wps
2017-05-31 12:54:26.297395: 0.815 perplexity: 5.041 speed: 29045 wps
2017-05-31 12:54:27.575068: 0.914 perplexity: 5.045 speed: 28948 wps
2017-05-31 12:54:28.618969: Epoch: 18 Train Perplexity: 5.048
valid data size: 121300
2017-05-31 12:54:30.323246: Epoch: 18 Valid Perplexity: 5.975
Seed: free
Sample: aku klisdreniz kendi<eos>nc
2017-05-31 12:54:33.757724: Epoch: 19 Learning rate: 0.031
valid data size: 363899
2017-05-31 12:54:34.126576: 0.022 perplexity: 5.504 speed: 25639 wps
2017-05-31 12:54:35.368973: 0.121 perplexity: 5.170 speed: 28254 wps
2017-05-31 12:54:36.596451: 0.220 perplexity: 5.092 speed: 28722 wps
2017-05-31 12:54:37.805180: 0.319 perplexity: 5.009 speed: 29042 wps
2017-05-31 12:54:39.044521: 0.419 perplexity: 4.997 speed: 29043 wps
2017-05-31 12:54:40.285279: 0.518 perplexity: 4.977 speed: 29037 wps
2017-05-31 12:54:41.554982: 0.617 perplexity: 4.972 speed: 28926 wps
2017-05-31 12:54:42.789249: 0.716 perplexity: 4.942 speed: 28959 wps
2017-05-31 12:54:44.000001: 0.815 perplexity: 4.946 speed: 29051 wps
2017-05-31 12:54:45.259381: 0.914 perplexity: 4.951 speed: 28999 wps
2017-05-31 12:54:46.315828: Epoch: 19 Train Perplexity: 4.955
valid data size: 121300
2017-05-31 12:54:47.972934: Epoch: 19 Valid Perplexity: 5.898
Seed: free
Sample:  zlr almtr ve<eos>d
2017-05-31 12:54:51.049031: Epoch: 20 Learning rate: 0.016
valid data size: 363899
2017-05-31 12:54:51.394598: 0.022 perplexity: 5.420 speed: 26792 wps
2017-05-31 12:54:52.625200: 0.121 perplexity: 5.091 speed: 28735 wps
2017-05-31 12:54:53.878807: 0.220 perplexity: 5.009 speed: 28727 wps
2017-05-31 12:54:55.090975: 0.319 perplexity: 4.926 speed: 29020 wps
2017-05-31 12:54:56.352190: 0.419 perplexity: 4.912 speed: 28906 wps
2017-05-31 12:54:57.594477: 0.518 perplexity: 4.890 speed: 28920 wps
2017-05-31 12:54:58.815660: 0.617 perplexity: 4.886 speed: 29008 wps
2017-05-31 12:55:00.052251: 0.716 perplexity: 4.856 speed: 29022 wps
2017-05-31 12:55:01.305770: 0.815 perplexity: 4.858 speed: 28985 wps
2017-05-31 12:55:02.533470: 0.914 perplexity: 4.865 speed: 29022 wps
2017-05-31 12:55:03.568533: Epoch: 20 Train Perplexity: 4.868
valid data size: 121300
2017-05-31 12:55:05.299727: Epoch: 20 Valid Perplexity: 5.833
Seed: free
Sample: casnmek istediimiziz 
2017-05-31 12:55:08.668367: Epoch: 21 Learning rate: 0.008
valid data size: 363899
2017-05-31 12:55:09.019449: 0.022 perplexity: 5.313 speed: 27124 wps
2017-05-31 12:55:10.249337: 0.121 perplexity: 4.991 speed: 28822 wps
2017-05-31 12:55:11.492863: 0.220 perplexity: 4.921 speed: 28878 wps
2017-05-31 12:55:12.772029: 0.319 perplexity: 4.845 speed: 28648 wps
2017-05-31 12:55:14.000946: 0.419 perplexity: 4.833 speed: 28798 wps
2017-05-31 12:55:15.233406: 0.518 perplexity: 4.813 speed: 28876 wps
2017-05-31 12:55:16.508130: 0.617 perplexity: 4.809 speed: 28772 wps
2017-05-31 12:55:17.748469: 0.716 perplexity: 4.779 speed: 28807 wps
2017-05-31 12:55:18.968389: 0.815 perplexity: 4.781 speed: 28890 wps
2017-05-31 12:55:20.213421: 0.914 perplexity: 4.788 speed: 28893 wps
2017-05-31 12:55:21.264117: Epoch: 21 Train Perplexity: 4.791
valid data size: 121300
2017-05-31 12:55:23.007945: Epoch: 21 Valid Perplexity: 5.774
Seed: free
Sample:  hktr zomin gn bir aki
2017-05-31 12:55:26.360467: Epoch: 22 Learning rate: 0.004
valid data size: 363899
2017-05-31 12:55:26.712221: 0.022 perplexity: 5.277 speed: 27621 wps
2017-05-31 12:55:27.951473: 0.121 perplexity: 4.930 speed: 28759 wps
2017-05-31 12:55:29.220014: 0.220 perplexity: 4.856 speed: 28588 wps
2017-05-31 12:55:30.440662: 0.319 perplexity: 4.779 speed: 28861 wps
2017-05-31 12:55:31.676808: 0.419 perplexity: 4.764 speed: 28922 wps
2017-05-31 12:55:32.924141: 0.518 perplexity: 4.744 speed: 28911 wps
2017-05-31 12:55:34.147999: 0.617 perplexity: 4.741 speed: 28990 wps
2017-05-31 12:55:35.372379: 0.716 perplexity: 4.712 speed: 29047 wps
2017-05-31 12:55:36.671188: 0.815 perplexity: 4.713 speed: 28879 wps
2017-05-31 12:55:37.919546: 0.914 perplexity: 4.719 speed: 28874 wps
2017-05-31 12:55:38.949323: Epoch: 22 Train Perplexity: 4.723
valid data size: 121300
2017-05-31 12:55:40.627589: Epoch: 22 Valid Perplexity: 5.719
Seed: free
Sample:  dnyade<eos>bir anlay Be
2017-05-31 12:55:43.982624: Epoch: 23 Learning rate: 0.002
valid data size: 363899
2017-05-31 12:55:44.350388: 0.022 perplexity: 5.150 speed: 26376 wps
2017-05-31 12:55:45.579239: 0.121 perplexity: 4.835 speed: 28673 wps
2017-05-31 12:55:46.798478: 0.220 perplexity: 4.769 speed: 29047 wps
2017-05-31 12:55:48.049512: 0.319 perplexity: 4.696 speed: 28963 wps
2017-05-31 12:55:49.259377: 0.419 perplexity: 4.685 speed: 29146 wps
2017-05-31 12:55:50.463667: 0.518 perplexity: 4.667 speed: 29285 wps
2017-05-31 12:55:51.676869: 0.617 perplexity: 4.666 speed: 29347 wps
2017-05-31 12:55:52.907325: 0.716 perplexity: 4.637 speed: 29335 wps
2017-05-31 12:55:54.157246: 0.815 perplexity: 4.639 speed: 29269 wps
2017-05-31 12:55:55.378461: 0.914 perplexity: 4.646 speed: 29291 wps
2017-05-31 12:55:56.434666: Epoch: 23 Train Perplexity: 4.650
valid data size: 121300
2017-05-31 12:55:58.152743: Epoch: 23 Valid Perplexity: 5.668
Seed: free
Sample:  profesr<eos>siyyi?A6<eos>Biraz 
2017-05-31 12:56:01.880605: Epoch: 24 Learning rate: 0.001
valid data size: 363899
2017-05-31 12:56:02.248818: 0.022 perplexity: 4.969 speed: 25701 wps
2017-05-31 12:56:03.475281: 0.121 perplexity: 4.761 speed: 28556 wps
2017-05-31 12:56:04.697429: 0.220 perplexity: 4.700 speed: 28950 wps
2017-05-31 12:56:05.929669: 0.319 perplexity: 4.629 speed: 29031 wps
2017-05-31 12:56:07.184594: 0.419 perplexity: 4.620 speed: 28949 wps
2017-05-31 12:56:08.418620: 0.518 perplexity: 4.604 speed: 28992 wps
2017-05-31 12:56:09.633038: 0.617 perplexity: 4.605 speed: 29094 wps
2017-05-31 12:56:10.840579: 0.716 perplexity: 4.577 speed: 29191 wps
2017-05-31 12:56:12.062923: 0.815 perplexity: 4.578 speed: 29223 wps
2017-05-31 12:56:13.305790: 0.914 perplexity: 4.584 speed: 29195 wps
2017-05-31 12:56:14.350397: Epoch: 24 Train Perplexity: 4.589
valid data size: 121300
2017-05-31 12:56:16.008613: Epoch: 24 Valid Perplexity: 5.639
Seed: free
Sample:  soruluyordu. Bu yakamdd
2017-05-31 12:56:19.162550: Epoch: 25 Learning rate: 0.000
valid data size: 363899
2017-05-31 12:56:19.524037: 0.022 perplexity: 4.959 speed: 26217 wps
2017-05-31 12:56:20.754864: 0.121 perplexity: 4.696 speed: 28599 wps
2017-05-31 12:56:21.964634: 0.220 perplexity: 4.630 speed: 29104 wps
2017-05-31 12:56:23.184645: 0.319 perplexity: 4.558 speed: 29227 wps
2017-05-31 12:56:24.422117: 0.419 perplexity: 4.552 speed: 29195 wps
2017-05-31 12:56:25.647399: 0.518 perplexity: 4.538 speed: 29230 wps
2017-05-31 12:56:26.858326: 0.617 perplexity: 4.539 speed: 29309 wps
2017-05-31 12:56:28.086732: 0.716 perplexity: 4.513 speed: 29309 wps
2017-05-31 12:56:29.326301: 0.815 perplexity: 4.514 speed: 29276 wps
2017-05-31 12:56:30.570790: 0.914 perplexity: 4.520 speed: 29238 wps
2017-05-31 12:56:31.622262: Epoch: 25 Train Perplexity: 4.526
valid data size: 121300
2017-05-31 12:56:33.438643: Epoch: 25 Valid Perplexity: 5.593
Seed: free
Sample:  okda kmam<eos>kapmak isti
2017-05-31 12:56:36.872171: Epoch: 26 Learning rate: 0.000
valid data size: 363899
2017-05-31 12:56:37.250837: 0.022 perplexity: 4.896 speed: 25272 wps
2017-05-31 12:56:38.505987: 0.121 perplexity: 4.635 speed: 27941 wps
2017-05-31 12:56:39.766213: 0.220 perplexity: 4.573 speed: 28215 wps
2017-05-31 12:56:41.021766: 0.319 perplexity: 4.502 speed: 28355 wps
2017-05-31 12:56:42.269188: 0.419 perplexity: 4.497 speed: 28472 wps
2017-05-31 12:56:43.505070: 0.518 perplexity: 4.484 speed: 28595 wps
2017-05-31 12:56:44.738601: 0.617 perplexity: 4.485 speed: 28688 wps
2017-05-31 12:56:46.002383: 0.716 perplexity: 4.460 speed: 28660 wps
2017-05-31 12:56:47.235322: 0.815 perplexity: 4.461 speed: 28724 wps
2017-05-31 12:56:48.481967: 0.914 perplexity: 4.466 speed: 28741 wps
2017-05-31 12:56:49.527264: Epoch: 26 Train Perplexity: 4.472
valid data size: 121300
2017-05-31 12:56:51.211808: Epoch: 26 Valid Perplexity: 5.571
Seed: free
Sample:  o nedenle balad. Kediz
2017-05-31 12:56:54.570397: Epoch: 27 Learning rate: 0.000
valid data size: 363899
2017-05-31 12:56:54.924528: 0.022 perplexity: 4.861 speed: 27123 wps
2017-05-31 12:56:56.154470: 0.121 perplexity: 4.586 speed: 28821 wps
2017-05-31 12:56:57.409296: 0.220 perplexity: 4.518 speed: 28762 wps
2017-05-31 12:56:58.629200: 0.319 perplexity: 4.449 speed: 28989 wps
2017-05-31 12:56:59.878982: 0.419 perplexity: 4.445 speed: 28945 wps
2017-05-31 12:57:01.120844: 0.518 perplexity: 4.432 speed: 28954 wps
2017-05-31 12:57:02.347121: 0.617 perplexity: 4.434 speed: 29018 wps
2017-05-31 12:57:03.577504: 0.716 perplexity: 4.411 speed: 29051 wps
2017-05-31 12:57:04.831270: 0.815 perplexity: 4.412 speed: 29009 wps
2017-05-31 12:57:06.037723: 0.914 perplexity: 4.417 speed: 29097 wps
2017-05-31 12:57:07.054177: Epoch: 27 Train Perplexity: 4.423
valid data size: 121300
2017-05-31 12:57:08.635709: Epoch: 27 Valid Perplexity: 5.547
Seed: free
Sample: r. Ba harknt, n 'da a
2017-05-31 12:57:12.284821: Epoch: 28 Learning rate: 0.000
valid data size: 363899
2017-05-31 12:57:12.644831: 0.022 perplexity: 4.808 speed: 26228 wps
2017-05-31 12:57:13.903923: 0.121 perplexity: 4.544 speed: 28095 wps
2017-05-31 12:57:15.139223: 0.220 perplexity: 4.473 speed: 28552 wps
2017-05-31 12:57:16.373806: 0.319 perplexity: 4.405 speed: 28737 wps
2017-05-31 12:57:17.641570: 0.419 perplexity: 4.399 speed: 28656 wps
2017-05-31 12:57:18.879414: 0.518 perplexity: 4.387 speed: 28736 wps
2017-05-31 12:57:20.115638: 0.617 perplexity: 4.389 speed: 28797 wps
2017-05-31 12:57:21.333815: 0.716 perplexity: 4.367 speed: 28899 wps
2017-05-31 12:57:22.540940: 0.815 perplexity: 4.368 speed: 29008 wps
2017-05-31 12:57:23.778084: 0.914 perplexity: 4.372 speed: 29018 wps
2017-05-31 12:57:24.832297: Epoch: 28 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 12:57:26.472411: Epoch: 28 Valid Perplexity: 5.556
Seed: free
Sample: rum. Dart Nimitilerinin 
2017-05-31 12:57:26.869602: Epoch: 29 Learning rate: 0.000
valid data size: 363899
2017-05-31 12:57:27.213763: 0.022 perplexity: 4.808 speed: 26600 wps
2017-05-31 12:57:28.444266: 0.121 perplexity: 4.544 speed: 28693 wps
2017-05-31 12:57:29.667731: 0.220 perplexity: 4.473 speed: 29015 wps
2017-05-31 12:57:30.935400: 0.319 perplexity: 4.405 speed: 28822 wps
2017-05-31 12:57:32.156777: 0.419 perplexity: 4.399 speed: 28973 wps
2017-05-31 12:57:33.385282: 0.518 perplexity: 4.387 speed: 29036 wps
2017-05-31 12:57:34.602677: 0.617 perplexity: 4.389 speed: 29120 wps
2017-05-31 12:57:35.851641: 0.716 perplexity: 4.367 speed: 29079 wps
2017-05-31 12:57:37.118857: 0.815 perplexity: 4.368 speed: 28996 wps
2017-05-31 12:57:38.352793: 0.914 perplexity: 4.372 speed: 29015 wps
2017-05-31 12:57:39.412458: Epoch: 29 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 12:57:41.088818: Epoch: 29 Valid Perplexity: 5.556
Seed: free
Sample: rza eerimin himseyi Tim
2017-05-31 12:57:41.308291: Epoch: 30 Learning rate: 0.000
valid data size: 363899
2017-05-31 12:57:41.663270: 0.022 perplexity: 4.808 speed: 27056 wps
2017-05-31 12:57:42.900754: 0.121 perplexity: 4.544 speed: 28668 wps
2017-05-31 12:57:44.145931: 0.220 perplexity: 4.473 speed: 28776 wps
2017-05-31 12:57:45.384442: 0.319 perplexity: 4.405 speed: 28865 wps
2017-05-31 12:57:46.637521: 0.419 perplexity: 4.399 speed: 28833 wps
2017-05-31 12:57:47.887891: 0.518 perplexity: 4.387 speed: 28825 wps
2017-05-31 12:57:49.139270: 0.617 perplexity: 4.389 speed: 28816 wps
2017-05-31 12:57:50.408958: 0.716 perplexity: 4.367 speed: 28751 wps
2017-05-31 12:57:51.654143: 0.815 perplexity: 4.368 speed: 28770 wps
2017-05-31 12:57:52.881648: 0.914 perplexity: 4.372 speed: 28830 wps
2017-05-31 12:57:53.910562: Epoch: 30 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 12:57:55.637586: Epoch: 30 Valid Perplexity: 5.556
Seed: free
Sample: r, siz' ilasik daha kk
2017-05-31 12:57:55.869706: Epoch: 31 Learning rate: 0.000
valid data size: 363899
2017-05-31 12:57:56.222664: 0.022 perplexity: 4.808 speed: 26935 wps
2017-05-31 12:57:57.456472: 0.121 perplexity: 4.544 speed: 28708 wps
2017-05-31 12:57:58.701095: 0.220 perplexity: 4.473 speed: 28804 wps
2017-05-31 12:57:59.938532: 0.319 perplexity: 4.405 speed: 28892 wps
2017-05-31 12:58:01.184233: 0.419 perplexity: 4.399 speed: 28894 wps
2017-05-31 12:58:02.450939: 0.518 perplexity: 4.387 speed: 28802 wps
2017-05-31 12:58:03.728241: 0.617 perplexity: 4.389 speed: 28701 wps
2017-05-31 12:58:04.983520: 0.716 perplexity: 4.367 speed: 28698 wps
2017-05-31 12:58:06.220646: 0.815 perplexity: 4.368 speed: 28747 wps
2017-05-31 12:58:07.463701: 0.914 perplexity: 4.372 speed: 28770 wps
2017-05-31 12:58:08.529736: Epoch: 31 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 12:58:10.241710: Epoch: 31 Valid Perplexity: 5.556
Seed: free
Sample: rken, teri<eos>insan m?<eos>B
2017-05-31 12:58:10.497257: Epoch: 32 Learning rate: 0.000
valid data size: 363899
2017-05-31 12:58:10.903472: 0.022 perplexity: 4.808 speed: 25129 wps
2017-05-31 12:58:12.133846: 0.121 perplexity: 4.544 speed: 28344 wps
2017-05-31 12:58:13.415753: 0.220 perplexity: 4.473 speed: 28228 wps
2017-05-31 12:58:14.669762: 0.319 perplexity: 4.405 speed: 28374 wps
2017-05-31 12:58:15.893714: 0.419 perplexity: 4.399 speed: 28612 wps
2017-05-31 12:58:17.151219: 0.518 perplexity: 4.387 speed: 28615 wps
2017-05-31 12:58:18.413419: 0.617 perplexity: 4.389 speed: 28600 wps
2017-05-31 12:58:19.650522: 0.716 perplexity: 4.367 speed: 28668 wps
2017-05-31 12:58:20.886785: 0.815 perplexity: 4.368 speed: 28722 wps
2017-05-31 12:58:22.115215: 0.914 perplexity: 4.372 speed: 28784 wps
2017-05-31 12:58:23.164520: Epoch: 32 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 12:58:24.822810: Epoch: 32 Valid Perplexity: 5.556
Seed: free
Sample: r, bilecekti.<eos>"Yakup Bey?
2017-05-31 12:58:25.031877: Epoch: 33 Learning rate: 0.000
valid data size: 363899
2017-05-31 12:58:25.362459: 0.022 perplexity: 4.808 speed: 27730 wps
2017-05-31 12:58:26.587787: 0.121 perplexity: 4.544 speed: 29040 wps
2017-05-31 12:58:27.829206: 0.220 perplexity: 4.473 speed: 29022 wps
2017-05-31 12:58:29.050087: 0.319 perplexity: 4.405 speed: 29164 wps
2017-05-31 12:58:30.299178: 0.419 perplexity: 4.399 speed: 29082 wps
2017-05-31 12:58:31.537965: 0.518 perplexity: 4.387 speed: 29078 wps
2017-05-31 12:58:32.774575: 0.617 perplexity: 4.389 speed: 29084 wps
2017-05-31 12:58:33.998512: 0.716 perplexity: 4.367 speed: 29128 wps
2017-05-31 12:58:35.235361: 0.815 perplexity: 4.368 speed: 29126 wps
2017-05-31 12:58:36.468685: 0.914 perplexity: 4.372 speed: 29133 wps
2017-05-31 12:58:37.514505: Epoch: 33 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 12:58:39.292681: Epoch: 33 Valid Perplexity: 5.556
Seed: free
Sample:  guruklarda btnl<eos>istey
2017-05-31 12:58:39.504904: Epoch: 34 Learning rate: 0.000
valid data size: 363899
2017-05-31 12:58:39.841358: 0.022 perplexity: 4.808 speed: 27373 wps
2017-05-31 12:58:41.088918: 0.121 perplexity: 4.544 speed: 28552 wps
2017-05-31 12:58:42.314699: 0.220 perplexity: 4.473 speed: 28910 wps
2017-05-31 12:58:43.558838: 0.319 perplexity: 4.405 speed: 28918 wps
2017-05-31 12:58:44.796943: 0.419 perplexity: 4.399 speed: 28955 wps
2017-05-31 12:58:46.051506: 0.518 perplexity: 4.387 speed: 28905 wps
2017-05-31 12:58:47.315928: 0.617 perplexity: 4.389 speed: 28835 wps
2017-05-31 12:58:48.564992: 0.716 perplexity: 4.367 speed: 28833 wps
2017-05-31 12:58:49.792674: 0.815 perplexity: 4.368 speed: 28892 wps
2017-05-31 12:58:51.009684: 0.914 perplexity: 4.372 speed: 28965 wps
2017-05-31 12:58:52.063151: Epoch: 34 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 12:58:53.772217: Epoch: 34 Valid Perplexity: 5.556
Seed: free
Sample: n gdan bir<eos>daba bir S
2017-05-31 12:58:53.983445: Epoch: 35 Learning rate: 0.000
valid data size: 363899
2017-05-31 12:58:54.317434: 0.022 perplexity: 4.808 speed: 27449 wps
2017-05-31 12:58:55.541040: 0.121 perplexity: 4.544 speed: 29011 wps
2017-05-31 12:58:56.779861: 0.220 perplexity: 4.473 speed: 29033 wps
2017-05-31 12:58:58.021405: 0.319 perplexity: 4.405 speed: 29022 wps
2017-05-31 12:58:59.253234: 0.419 perplexity: 4.399 speed: 29069 wps
2017-05-31 12:59:00.484189: 0.518 perplexity: 4.387 speed: 29103 wps
2017-05-31 12:59:01.745357: 0.617 perplexity: 4.389 speed: 29012 wps
2017-05-31 12:59:02.980477: 0.716 perplexity: 4.367 speed: 29030 wps
2017-05-31 12:59:04.241088: 0.815 perplexity: 4.368 speed: 28972 wps
2017-05-31 12:59:05.499770: 0.914 perplexity: 4.372 speed: 28932 wps
2017-05-31 12:59:06.545010: Epoch: 35 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 12:59:08.112353: Epoch: 35 Valid Perplexity: 5.556
Seed: free
Sample: rlamalyd.<eos>"Mek ifsanlar
2017-05-31 12:59:08.342987: Epoch: 36 Learning rate: 0.000
valid data size: 363899
2017-05-31 12:59:08.694144: 0.022 perplexity: 4.808 speed: 26709 wps
2017-05-31 12:59:09.907769: 0.121 perplexity: 4.544 speed: 29032 wps
2017-05-31 12:59:11.145631: 0.220 perplexity: 4.473 speed: 29055 wps
2017-05-31 12:59:12.380593: 0.319 perplexity: 4.405 speed: 29084 wps
2017-05-31 12:59:13.612230: 0.419 perplexity: 4.399 speed: 29118 wps
2017-05-31 12:59:14.851650: 0.518 perplexity: 4.387 speed: 29105 wps
2017-05-31 12:59:16.091661: 0.617 perplexity: 4.389 speed: 29093 wps
2017-05-31 12:59:17.319933: 0.716 perplexity: 4.367 speed: 29122 wps
2017-05-31 12:59:18.590271: 0.815 perplexity: 4.368 speed: 29025 wps
2017-05-31 12:59:19.828727: 0.914 perplexity: 4.372 speed: 29030 wps
2017-05-31 12:59:20.871215: Epoch: 36 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 12:59:22.568449: Epoch: 36 Valid Perplexity: 5.556
Seed: free
Sample: rl, gzle olan- kakteta
2017-05-31 12:59:22.782336: Epoch: 37 Learning rate: 0.000
valid data size: 363899
2017-05-31 12:59:23.125393: 0.022 perplexity: 4.808 speed: 26902 wps
2017-05-31 12:59:24.359509: 0.121 perplexity: 4.544 speed: 28695 wps
2017-05-31 12:59:25.607726: 0.220 perplexity: 4.473 speed: 28760 wps
2017-05-31 12:59:26.872745: 0.319 perplexity: 4.405 speed: 28666 wps
2017-05-31 12:59:28.097482: 0.419 perplexity: 4.399 speed: 28835 wps
2017-05-31 12:59:29.335585: 0.518 perplexity: 4.387 speed: 28880 wps
2017-05-31 12:59:30.561423: 0.617 perplexity: 4.389 speed: 28957 wps
2017-05-31 12:59:31.795319: 0.716 perplexity: 4.367 speed: 28987 wps
2017-05-31 12:59:33.044870: 0.815 perplexity: 4.368 speed: 28966 wps
2017-05-31 12:59:34.256319: 0.914 perplexity: 4.372 speed: 29045 wps
2017-05-31 12:59:35.320682: Epoch: 37 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 12:59:36.934841: Epoch: 37 Valid Perplexity: 5.556
Seed: free
Sample: ' iire, Deerleri<eos>altnd
2017-05-31 12:59:37.142560: Epoch: 38 Learning rate: 0.000
valid data size: 363899
2017-05-31 12:59:37.485855: 0.022 perplexity: 4.808 speed: 26955 wps
2017-05-31 12:59:38.699954: 0.121 perplexity: 4.544 speed: 29080 wps
2017-05-31 12:59:39.937381: 0.220 perplexity: 4.473 speed: 29085 wps
2017-05-31 12:59:41.159997: 0.319 perplexity: 4.405 speed: 29195 wps
2017-05-31 12:59:42.394130: 0.419 perplexity: 4.399 speed: 29189 wps
2017-05-31 12:59:43.648941: 0.518 perplexity: 4.387 speed: 29093 wps
2017-05-31 12:59:44.876894: 0.617 perplexity: 4.389 speed: 29128 wps
2017-05-31 12:59:46.119446: 0.716 perplexity: 4.367 speed: 29107 wps
2017-05-31 12:59:47.343284: 0.815 perplexity: 4.368 speed: 29144 wps
2017-05-31 12:59:48.579254: 0.914 perplexity: 4.372 speed: 29142 wps
2017-05-31 12:59:49.614907: Epoch: 38 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 12:59:51.307904: Epoch: 38 Valid Perplexity: 5.556
Seed: free
Sample: rtaj gerekiyoruz.<eos>"Buni 
2017-05-31 12:59:51.572861: Epoch: 39 Learning rate: 0.000
valid data size: 363899
2017-05-31 12:59:51.949343: 0.022 perplexity: 4.808 speed: 24979 wps
2017-05-31 12:59:53.196590: 0.121 perplexity: 4.544 speed: 28008 wps
2017-05-31 12:59:54.449433: 0.220 perplexity: 4.473 speed: 28327 wps
2017-05-31 12:59:55.683493: 0.319 perplexity: 4.405 speed: 28582 wps
2017-05-31 12:59:56.914039: 0.419 perplexity: 4.399 speed: 28738 wps
2017-05-31 12:59:58.167448: 0.518 perplexity: 4.387 speed: 28735 wps
2017-05-31 12:59:59.400139: 0.617 perplexity: 4.389 speed: 28809 wps
2017-05-31 13:00:00.649279: 0.716 perplexity: 4.367 speed: 28811 wps
2017-05-31 13:00:01.877613: 0.815 perplexity: 4.368 speed: 28870 wps
2017-05-31 13:00:03.126159: 0.914 perplexity: 4.372 speed: 28866 wps
2017-05-31 13:00:04.177768: Epoch: 39 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 13:00:05.796192: Epoch: 39 Valid Perplexity: 5.556
Seed: free
Sample: r eden Koraya deildir,<eos>k
2017-05-31 13:00:05.998307: Epoch: 40 Learning rate: 0.000
valid data size: 363899
2017-05-31 13:00:06.369020: 0.022 perplexity: 4.808 speed: 25877 wps
2017-05-31 13:00:07.607112: 0.121 perplexity: 4.544 speed: 28388 wps
2017-05-31 13:00:08.822693: 0.220 perplexity: 4.473 speed: 28922 wps
2017-05-31 13:00:10.049700: 0.319 perplexity: 4.405 speed: 29050 wps
2017-05-31 13:00:11.280171: 0.419 perplexity: 4.399 speed: 29098 wps
2017-05-31 13:00:12.530430: 0.518 perplexity: 4.387 speed: 29040 wps
2017-05-31 13:00:13.752084: 0.617 perplexity: 4.389 speed: 29107 wps
2017-05-31 13:00:15.006850: 0.716 perplexity: 4.367 speed: 29049 wps
2017-05-31 13:00:16.236761: 0.815 perplexity: 4.368 speed: 29076 wps
2017-05-31 13:00:17.470717: 0.914 perplexity: 4.372 speed: 29086 wps
2017-05-31 13:00:18.542080: Epoch: 40 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 13:00:20.141007: Epoch: 40 Valid Perplexity: 5.556
Seed: free
Sample: r kendisisinin<eos>dnyazs<eos>
2017-05-31 13:00:20.350350: Epoch: 41 Learning rate: 0.000
valid data size: 363899
2017-05-31 13:00:20.706922: 0.022 perplexity: 4.808 speed: 27443 wps
2017-05-31 13:00:21.930439: 0.121 perplexity: 4.544 speed: 29012 wps
2017-05-31 13:00:23.145380: 0.220 perplexity: 4.473 speed: 29285 wps
2017-05-31 13:00:24.382308: 0.319 perplexity: 4.405 speed: 29229 wps
2017-05-31 13:00:25.624527: 0.419 perplexity: 4.399 speed: 29170 wps
2017-05-31 13:00:26.911472: 0.518 perplexity: 4.387 speed: 28934 wps
2017-05-31 13:00:28.143225: 0.617 perplexity: 4.389 speed: 28980 wps
2017-05-31 13:00:29.363779: 0.716 perplexity: 4.367 speed: 29050 wps
2017-05-31 13:00:30.578984: 0.815 perplexity: 4.368 speed: 29119 wps
2017-05-31 13:00:31.789088: 0.914 perplexity: 4.372 speed: 29186 wps
2017-05-31 13:00:32.819232: Epoch: 41 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 13:00:34.401845: Epoch: 41 Valid Perplexity: 5.556
Seed: free
Sample: rd.<eos>Timur hayal verdilik
2017-05-31 13:00:34.620564: Epoch: 42 Learning rate: 0.000
valid data size: 363899
2017-05-31 13:00:34.978983: 0.022 perplexity: 4.808 speed: 26703 wps
2017-05-31 13:00:36.184526: 0.121 perplexity: 4.544 speed: 29184 wps
2017-05-31 13:00:37.398791: 0.220 perplexity: 4.473 speed: 29389 wps
2017-05-31 13:00:38.617254: 0.319 perplexity: 4.405 speed: 29437 wps
2017-05-31 13:00:39.823042: 0.419 perplexity: 4.399 speed: 29535 wps
2017-05-31 13:00:41.021347: 0.518 perplexity: 4.387 speed: 29630 wps
2017-05-31 13:00:42.234088: 0.617 perplexity: 4.389 speed: 29639 wps
2017-05-31 13:00:43.448677: 0.716 perplexity: 4.367 speed: 29639 wps
2017-05-31 13:00:44.654120: 0.815 perplexity: 4.368 speed: 29666 wps
2017-05-31 13:00:45.861843: 0.914 perplexity: 4.372 speed: 29681 wps
2017-05-31 13:00:46.885967: Epoch: 42 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 13:00:48.516640: Epoch: 42 Valid Perplexity: 5.556
Seed: free
Sample: ri,<eos>s onlarn Colulalar
2017-05-31 13:00:48.729569: Epoch: 43 Learning rate: 0.000
valid data size: 363899
2017-05-31 13:00:49.071089: 0.022 perplexity: 4.808 speed: 26837 wps
2017-05-31 13:00:50.287028: 0.121 perplexity: 4.544 speed: 29018 wps
2017-05-31 13:00:51.488142: 0.220 perplexity: 4.473 speed: 29436 wps
2017-05-31 13:00:52.707786: 0.319 perplexity: 4.405 speed: 29461 wps
2017-05-31 13:00:53.910463: 0.419 perplexity: 4.399 speed: 29571 wps
2017-05-31 13:00:55.129815: 0.518 perplexity: 4.387 speed: 29562 wps
2017-05-31 13:00:56.327762: 0.617 perplexity: 4.389 speed: 29639 wps
2017-05-31 13:00:57.544533: 0.716 perplexity: 4.367 speed: 29632 wps
2017-05-31 13:00:58.756721: 0.815 perplexity: 4.368 speed: 29640 wps
2017-05-31 13:00:59.969643: 0.914 perplexity: 4.372 speed: 29644 wps
2017-05-31 13:01:00.998127: Epoch: 43 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 13:01:02.634460: Epoch: 43 Valid Perplexity: 5.556
Seed: free
Sample:  mazfna en akmadaki ben
2017-05-31 13:01:02.840689: Epoch: 44 Learning rate: 0.000
valid data size: 363899
2017-05-31 13:01:03.187008: 0.022 perplexity: 4.808 speed: 26771 wps
2017-05-31 13:01:04.419739: 0.121 perplexity: 4.544 speed: 28692 wps
2017-05-31 13:01:05.627103: 0.220 perplexity: 4.473 speed: 29183 wps
2017-05-31 13:01:06.844828: 0.319 perplexity: 4.405 speed: 29299 wps
2017-05-31 13:01:08.055759: 0.419 perplexity: 4.399 speed: 29399 wps
2017-05-31 13:01:09.261236: 0.518 perplexity: 4.387 speed: 29487 wps
2017-05-31 13:01:10.491344: 0.617 perplexity: 4.389 speed: 29451 wps
2017-05-31 13:01:11.692368: 0.716 perplexity: 4.367 speed: 29522 wps
2017-05-31 13:01:12.904594: 0.815 perplexity: 4.368 speed: 29543 wps
2017-05-31 13:01:14.115195: 0.914 perplexity: 4.372 speed: 29564 wps
2017-05-31 13:01:15.140906: Epoch: 44 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 13:01:16.737955: Epoch: 44 Valid Perplexity: 5.556
Seed: free
Sample: n zeri<eos>ever<eos>mi? Silasn
2017-05-31 13:01:16.938896: Epoch: 45 Learning rate: 0.000
valid data size: 363899
2017-05-31 13:01:17.277696: 0.022 perplexity: 4.808 speed: 27328 wps
2017-05-31 13:01:18.485404: 0.121 perplexity: 4.544 speed: 29286 wps
2017-05-31 13:01:19.686117: 0.220 perplexity: 4.473 speed: 29592 wps
2017-05-31 13:01:20.888018: 0.319 perplexity: 4.405 speed: 29702 wps
2017-05-31 13:01:22.097780: 0.419 perplexity: 4.399 speed: 29715 wps
2017-05-31 13:01:23.299334: 0.518 perplexity: 4.387 speed: 29762 wps
2017-05-31 13:01:24.517475: 0.617 perplexity: 4.389 speed: 29728 wps
2017-05-31 13:01:25.727979: 0.716 perplexity: 4.367 speed: 29730 wps
2017-05-31 13:01:26.944267: 0.815 perplexity: 4.368 speed: 29714 wps
2017-05-31 13:01:28.159823: 0.914 perplexity: 4.372 speed: 29703 wps
2017-05-31 13:01:29.181510: Epoch: 45 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 13:01:30.709214: Epoch: 45 Valid Perplexity: 5.556
Seed: free
Sample: r mre'tisizde bir soruyd
2017-05-31 13:01:30.913272: Epoch: 46 Learning rate: 0.000
valid data size: 363899
2017-05-31 13:01:31.253256: 0.022 perplexity: 4.808 speed: 27579 wps
2017-05-31 13:01:32.474998: 0.121 perplexity: 4.544 speed: 29075 wps
2017-05-31 13:01:33.694262: 0.220 perplexity: 4.473 speed: 29274 wps
2017-05-31 13:01:34.911271: 0.319 perplexity: 4.405 speed: 29368 wps
2017-05-31 13:01:36.127288: 0.419 perplexity: 4.399 speed: 29423 wps
2017-05-31 13:01:37.330770: 0.518 perplexity: 4.387 speed: 29516 wps
2017-05-31 13:01:38.538102: 0.617 perplexity: 4.389 speed: 29564 wps
2017-05-31 13:01:39.755766: 0.716 perplexity: 4.367 speed: 29564 wps
2017-05-31 13:01:40.968953: 0.815 perplexity: 4.368 speed: 29577 wps
2017-05-31 13:01:42.171205: 0.914 perplexity: 4.372 speed: 29616 wps
2017-05-31 13:01:43.198784: Epoch: 46 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 13:01:44.867978: Epoch: 46 Valid Perplexity: 5.556
Seed: free
Sample: rda retim<eos>serlendirme
2017-05-31 13:01:45.069522: Epoch: 47 Learning rate: 0.000
valid data size: 363899
2017-05-31 13:01:45.405274: 0.022 perplexity: 4.808 speed: 27396 wps
2017-05-31 13:01:46.608032: 0.121 perplexity: 4.544 speed: 29397 wps
2017-05-31 13:01:47.828377: 0.220 perplexity: 4.473 speed: 29443 wps
2017-05-31 13:01:49.075765: 0.319 perplexity: 4.405 speed: 29261 wps
2017-05-31 13:01:50.279287: 0.419 perplexity: 4.399 speed: 29411 wps
2017-05-31 13:01:51.495298: 0.518 perplexity: 4.387 speed: 29448 wps
2017-05-31 13:01:52.698337: 0.617 perplexity: 4.389 speed: 29523 wps
2017-05-31 13:01:53.896565: 0.716 perplexity: 4.367 speed: 29594 wps
2017-05-31 13:01:55.101018: 0.815 perplexity: 4.368 speed: 29630 wps
2017-05-31 13:01:56.307668: 0.914 perplexity: 4.372 speed: 29652 wps
2017-05-31 13:01:57.325990: Epoch: 47 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 13:01:58.943066: Epoch: 47 Valid Perplexity: 5.556
Seed: free
Sample: r deildir."<eos>"Bepkolu uyu
2017-05-31 13:01:59.165777: Epoch: 48 Learning rate: 0.000
valid data size: 363899
2017-05-31 13:01:59.501346: 0.022 perplexity: 4.808 speed: 27563 wps
2017-05-31 13:02:00.723819: 0.121 perplexity: 4.544 speed: 29058 wps
2017-05-31 13:02:01.929577: 0.220 perplexity: 4.473 speed: 29408 wps
2017-05-31 13:02:03.129942: 0.319 perplexity: 4.405 speed: 29586 wps
2017-05-31 13:02:04.337119: 0.419 perplexity: 4.399 speed: 29641 wps
2017-05-31 13:02:05.549234: 0.518 perplexity: 4.387 speed: 29652 wps
2017-05-31 13:02:06.771544: 0.617 perplexity: 4.389 speed: 29620 wps
2017-05-31 13:02:07.987593: 0.716 perplexity: 4.367 speed: 29618 wps
2017-05-31 13:02:09.185679: 0.815 perplexity: 4.368 speed: 29669 wps
2017-05-31 13:02:10.393850: 0.914 perplexity: 4.372 speed: 29683 wps
2017-05-31 13:02:11.412430: Epoch: 48 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 13:02:13.039015: Epoch: 48 Valid Perplexity: 5.556
Seed: free
Sample: nd.<eos>"'Belrinmeleri olaca
2017-05-31 13:02:13.374003: Epoch: 49 Learning rate: 0.000
valid data size: 363899
2017-05-31 13:02:13.728680: 0.022 perplexity: 4.808 speed: 27503 wps
2017-05-31 13:02:14.946106: 0.121 perplexity: 4.544 speed: 29141 wps
2017-05-31 13:02:16.175595: 0.220 perplexity: 4.473 speed: 29203 wps
2017-05-31 13:02:17.383038: 0.319 perplexity: 4.405 speed: 29389 wps
2017-05-31 13:02:18.702484: 0.419 perplexity: 4.399 speed: 28864 wps
2017-05-31 13:02:19.914793: 0.518 perplexity: 4.387 speed: 29019 wps
2017-05-31 13:02:21.124266: 0.617 perplexity: 4.389 speed: 29136 wps
2017-05-31 13:02:22.339773: 0.716 perplexity: 4.367 speed: 29201 wps
2017-05-31 13:02:23.565323: 0.815 perplexity: 4.368 speed: 29222 wps
2017-05-31 13:02:24.776849: 0.914 perplexity: 4.372 speed: 29275 wps
2017-05-31 13:02:25.797858: Epoch: 49 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 13:02:27.365416: Epoch: 49 Valid Perplexity: 5.556
Seed: free
Sample: rser oluklarla sadece bu
2017-05-31 13:02:27.592026: Epoch: 50 Learning rate: 0.000
valid data size: 363899
2017-05-31 13:02:27.944814: 0.022 perplexity: 4.808 speed: 26941 wps
2017-05-31 13:02:29.172952: 0.121 perplexity: 4.544 speed: 28814 wps
2017-05-31 13:02:30.381630: 0.220 perplexity: 4.473 speed: 29239 wps
2017-05-31 13:02:31.594830: 0.319 perplexity: 4.405 speed: 29371 wps
2017-05-31 13:02:32.805307: 0.419 perplexity: 4.399 speed: 29457 wps
2017-05-31 13:02:34.016916: 0.518 perplexity: 4.387 speed: 29506 wps
2017-05-31 13:02:35.219846: 0.617 perplexity: 4.389 speed: 29572 wps
2017-05-31 13:02:36.439089: 0.716 perplexity: 4.367 speed: 29566 wps
2017-05-31 13:02:37.670669: 0.815 perplexity: 4.368 speed: 29525 wps
2017-05-31 13:02:38.943514: 0.914 perplexity: 4.372 speed: 29385 wps
2017-05-31 13:02:39.994787: Epoch: 50 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 13:02:41.668080: Epoch: 50 Valid Perplexity: 5.556
Seed: free
Sample: r<eos>erkekinde Cepabilir. V
2017-05-31 13:02:41.884402: Epoch: 51 Learning rate: 0.000
valid data size: 363899
2017-05-31 13:02:42.234576: 0.022 perplexity: 4.808 speed: 27643 wps
2017-05-31 13:02:43.447811: 0.121 perplexity: 4.544 speed: 29251 wps
2017-05-31 13:02:44.662209: 0.220 perplexity: 4.473 speed: 29425 wps
2017-05-31 13:02:45.866842: 0.319 perplexity: 4.405 speed: 29565 wps
2017-05-31 13:02:47.088148: 0.419 perplexity: 4.399 speed: 29544 wps
2017-05-31 13:02:48.311024: 0.518 perplexity: 4.387 speed: 29524 wps
2017-05-31 13:02:49.541289: 0.617 perplexity: 4.389 speed: 29482 wps
2017-05-31 13:02:50.756062: 0.716 perplexity: 4.367 speed: 29503 wps
2017-05-31 13:02:51.961104: 0.815 perplexity: 4.368 speed: 29547 wps
2017-05-31 13:02:53.164096: 0.914 perplexity: 4.372 speed: 29588 wps
2017-05-31 13:02:54.188152: Epoch: 51 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 13:02:55.847639: Epoch: 51 Valid Perplexity: 5.556
Seed: free
Sample: r, a kefoki kefetmenme-
2017-05-31 13:02:56.061818: Epoch: 52 Learning rate: 0.000
valid data size: 363899
2017-05-31 13:02:56.408761: 0.022 perplexity: 4.808 speed: 27227 wps
2017-05-31 13:02:57.614084: 0.121 perplexity: 4.544 speed: 29309 wps
2017-05-31 13:02:58.827526: 0.220 perplexity: 4.473 speed: 29468 wps
2017-05-31 13:03:00.061761: 0.319 perplexity: 4.405 speed: 29375 wps
2017-05-31 13:03:01.275650: 0.419 perplexity: 4.399 speed: 29441 wps
2017-05-31 13:03:02.489196: 0.518 perplexity: 4.387 speed: 29483 wps
2017-05-31 13:03:03.704899: 0.617 perplexity: 4.389 speed: 29504 wps
2017-05-31 13:03:04.914031: 0.716 perplexity: 4.367 speed: 29541 wps
2017-05-31 13:03:06.128417: 0.815 perplexity: 4.368 speed: 29553 wps
2017-05-31 13:03:07.346400: 0.914 perplexity: 4.372 speed: 29554 wps
2017-05-31 13:03:08.374545: Epoch: 52 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 13:03:09.982242: Epoch: 52 Valid Perplexity: 5.556
Seed: free
Sample: ru siye sorumlu bir almay
2017-05-31 13:03:10.241632: Epoch: 53 Learning rate: 0.000
valid data size: 363899
2017-05-31 13:03:10.589627: 0.022 perplexity: 4.808 speed: 26845 wps
2017-05-31 13:03:11.803549: 0.121 perplexity: 4.544 speed: 29058 wps
2017-05-31 13:03:13.025729: 0.220 perplexity: 4.473 speed: 29234 wps
2017-05-31 13:03:14.234742: 0.319 perplexity: 4.405 speed: 29399 wps
2017-05-31 13:03:15.441869: 0.419 perplexity: 4.399 speed: 29497 wps
2017-05-31 13:03:16.654007: 0.518 perplexity: 4.387 speed: 29536 wps
2017-05-31 13:03:17.861364: 0.617 perplexity: 4.389 speed: 29581 wps
2017-05-31 13:03:19.075407: 0.716 perplexity: 4.367 speed: 29591 wps
2017-05-31 13:03:20.282684: 0.815 perplexity: 4.368 speed: 29618 wps
2017-05-31 13:03:21.508388: 0.914 perplexity: 4.372 speed: 29591 wps
2017-05-31 13:03:22.529156: Epoch: 53 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 13:03:24.157193: Epoch: 53 Valid Perplexity: 5.556
Seed: free
Sample: reye sutkar gakta evle i
2017-05-31 13:03:24.377431: Epoch: 54 Learning rate: 0.000
valid data size: 363899
2017-05-31 13:03:24.718533: 0.022 perplexity: 4.808 speed: 27123 wps
2017-05-31 13:03:25.970230: 0.121 perplexity: 4.544 speed: 28424 wps
2017-05-31 13:03:27.189586: 0.220 perplexity: 4.473 speed: 28904 wps
2017-05-31 13:03:28.410559: 0.319 perplexity: 4.405 speed: 29080 wps
2017-05-31 13:03:29.626865: 0.419 perplexity: 4.399 speed: 29201 wps
2017-05-31 13:03:30.836755: 0.518 perplexity: 4.387 speed: 29305 wps
2017-05-31 13:03:32.041415: 0.617 perplexity: 4.389 speed: 29396 wps
2017-05-31 13:03:33.258943: 0.716 perplexity: 4.367 speed: 29420 wps
2017-05-31 13:03:34.458234: 0.815 perplexity: 4.368 speed: 29491 wps
2017-05-31 13:03:35.680083: 0.914 perplexity: 4.372 speed: 29488 wps
2017-05-31 13:03:36.698870: Epoch: 54 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 13:03:38.318933: Epoch: 54 Valid Perplexity: 5.556
Seed: free
Sample: r mnance<eos>kendi nemli an
2017-05-31 13:03:38.524320: Epoch: 55 Learning rate: 0.000
valid data size: 363899
2017-05-31 13:03:38.857895: 0.022 perplexity: 4.808 speed: 27485 wps
2017-05-31 13:03:40.066594: 0.121 perplexity: 4.544 speed: 29302 wps
2017-05-31 13:03:41.298894: 0.220 perplexity: 4.473 speed: 29263 wps
2017-05-31 13:03:42.495067: 0.319 perplexity: 4.405 speed: 29515 wps
2017-05-31 13:03:43.704795: 0.419 perplexity: 4.399 speed: 29572 wps
2017-05-31 13:03:44.919969: 0.518 perplexity: 4.387 speed: 29582 wps
2017-05-31 13:03:46.142033: 0.617 perplexity: 4.389 speed: 29562 wps
2017-05-31 13:03:47.365976: 0.716 perplexity: 4.367 speed: 29541 wps
2017-05-31 13:03:48.609055: 0.815 perplexity: 4.368 speed: 29470 wps
2017-05-31 13:03:49.822823: 0.914 perplexity: 4.372 speed: 29490 wps
2017-05-31 13:03:50.853626: Epoch: 55 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 13:03:52.472713: Epoch: 55 Valid Perplexity: 5.556
Seed: free
Sample: n zoluma<eos>kldn<eos>'dak
2017-05-31 13:03:52.720997: Epoch: 56 Learning rate: 0.000
valid data size: 363899
2017-05-31 13:03:53.088068: 0.022 perplexity: 4.808 speed: 26731 wps
2017-05-31 13:03:54.309603: 0.121 perplexity: 4.544 speed: 28889 wps
2017-05-31 13:03:55.547740: 0.220 perplexity: 4.473 speed: 28972 wps
2017-05-31 13:03:56.824207: 0.319 perplexity: 4.405 speed: 28730 wps
2017-05-31 13:03:58.018436: 0.419 perplexity: 4.399 speed: 29052 wps
2017-05-31 13:03:59.224696: 0.518 perplexity: 4.387 speed: 29200 wps
2017-05-31 13:04:00.428416: 0.617 perplexity: 4.389 speed: 29311 wps
2017-05-31 13:04:01.635211: 0.716 perplexity: 4.367 speed: 29381 wps
2017-05-31 13:04:02.850489: 0.815 perplexity: 4.368 speed: 29410 wps
2017-05-31 13:04:04.057709: 0.914 perplexity: 4.372 speed: 29454 wps
2017-05-31 13:04:05.083685: Epoch: 56 Train Perplexity: 4.378
valid data size: 121300
2017-05-31 13:04:06.688749: Epoch: 56 Valid Perplexity: 5.556
2017-05-31 13:04:06.811763: Seed: free
2017-05-31 13:04:06.827127: Sample: r bu hismundum.<eos>"Aylnda 
valid data size: 66164
2017-05-31 13:07:35.244843: Test Perplexity: 5.828
2017-05-31 13:07:35.343623: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-31 13:13:38.722398: Running on UCSC:citrisdense...
Distinct terms: 116
########################################################################################
########################################################################################
########################################################################################
2017-05-31 13:18:34.304858: Running on UCSC:citrisdense...
Distinct terms: 116
epoch 155
Seed: free
Sample: o@}Metorn9Rc/v@:.D%;Y:,
2017-05-31 13:22:22.991768: Epoch: 1 Learning rate: 0.050
valid data size: 363899
2017-05-31 13:22:29.723762: 0.028 perplexity: 64.819 speed: 1734 wps
2017-05-31 13:22:39.598759: 0.127 perplexity: 39.962 speed: 2878 wps
2017-05-31 13:22:49.483232: 0.225 perplexity: 58.186 speed: 3159 wps
2017-05-31 13:22:59.406894: 0.324 perplexity: 49.691 speed: 3283 wps
2017-05-31 13:23:09.365585: 0.423 perplexity: 48.301 speed: 3351 wps
2017-05-31 13:23:18.937685: 0.521 perplexity: 39.426 speed: 3419 wps
2017-05-31 13:23:28.789923: 0.620 perplexity: 33.986 speed: 3452 wps
2017-05-31 13:23:38.332871: 0.718 perplexity: 30.309 speed: 3490 wps
2017-05-31 13:23:48.154898: 0.817 perplexity: 27.694 speed: 3509 wps
2017-05-31 13:23:58.049472: 0.915 perplexity: 25.800 speed: 3521 wps
2017-05-31 13:24:06.134428: Epoch: 1 Train Perplexity: 24.543
valid data size: 121300
2017-05-31 13:24:18.398314: Epoch: 1 Valid Perplexity: 12.512
Seed: free
Sample: , A<eos>onke. Tifumsndan ki
2017-05-31 13:25:14.691611: Epoch: 2 Learning rate: 0.050
valid data size: 363899
2017-05-31 13:25:17.974537: 0.028 perplexity: 14.807 speed: 3477 wps
2017-05-31 13:25:27.631873: 0.127 perplexity: 13.380 speed: 3652 wps
2017-05-31 13:25:37.662117: 0.225 perplexity: 13.216 speed: 3618 wps
2017-05-31 13:25:47.290391: 0.324 perplexity: 13.186 speed: 3649 wps
2017-05-31 13:25:56.565289: 0.423 perplexity: 12.976 speed: 3696 wps
2017-05-31 13:26:06.482928: 0.521 perplexity: 12.837 speed: 3681 wps
2017-05-31 13:26:16.405007: 0.620 perplexity: 12.706 speed: 3670 wps
2017-05-31 13:26:26.176790: 0.718 perplexity: 12.589 speed: 3669 wps
2017-05-31 13:26:35.675824: 0.817 perplexity: 12.460 speed: 3681 wps
2017-05-31 13:26:45.173270: 0.915 perplexity: 12.391 speed: 3691 wps
2017-05-31 13:26:53.257064: Epoch: 2 Train Perplexity: 12.351
valid data size: 121300
2017-05-31 13:27:04.515065: Epoch: 2 Valid Perplexity: 10.464
Seed: free
Sample: lismendendi zada aftaka u
2017-05-31 13:27:58.684617: Epoch: 3 Learning rate: 0.050
valid data size: 363899
2017-05-31 13:28:01.939446: 0.028 perplexity: 12.918 speed: 3478 wps
2017-05-31 13:28:11.738516: 0.127 perplexity: 11.629 speed: 3613 wps
2017-05-31 13:28:21.269711: 0.225 perplexity: 11.561 speed: 3675 wps
2017-05-31 13:28:31.356612: 0.324 perplexity: 11.530 speed: 3637 wps
2017-05-31 13:28:41.122958: 0.423 perplexity: 11.349 speed: 3645 wps
2017-05-31 13:28:50.912347: 0.521 perplexity: 11.279 speed: 3648 wps
2017-05-31 13:29:00.539286: 0.620 perplexity: 11.210 speed: 3660 wps
2017-05-31 13:29:10.240668: 0.718 perplexity: 11.173 speed: 3664 wps
2017-05-31 13:29:20.065015: 0.817 perplexity: 11.133 speed: 3662 wps
2017-05-31 13:29:29.731035: 0.915 perplexity: 11.127 speed: 3667 wps
2017-05-31 13:29:37.594168: Epoch: 3 Train Perplexity: 11.142
valid data size: 121300
2017-05-31 13:29:49.044579: Epoch: 3 Valid Perplexity: 9.869
Seed: free
Sample: k hanp behi "honhalaka, 
2017-05-31 13:30:40.218267: Epoch: 4 Learning rate: 0.050
valid data size: 363899
2017-05-31 13:30:43.360242: 0.028 perplexity: 12.062 speed: 3604 wps
2017-05-31 13:30:53.184378: 0.127 perplexity: 11.171 speed: 3638 wps
2017-05-31 13:31:02.668741: 0.225 perplexity: 10.997 speed: 3697 wps
2017-05-31 13:31:12.486132: 0.324 perplexity: 11.021 speed: 3683 wps
2017-05-31 13:31:22.242921: 0.423 perplexity: 10.902 speed: 3681 wps
2017-05-31 13:31:31.964892: 0.521 perplexity: 10.843 speed: 3682 wps
2017-05-31 13:31:41.789817: 0.620 perplexity: 10.801 speed: 3676 wps
2017-05-31 13:31:51.422919: 0.718 perplexity: 10.775 speed: 3682 wps
2017-05-31 13:32:00.958730: 0.817 perplexity: 10.755 speed: 3691 wps
2017-05-31 13:32:10.683887: 0.915 perplexity: 10.752 speed: 3691 wps
2017-05-31 13:32:18.737509: Epoch: 4 Train Perplexity: 10.769
valid data size: 121300
2017-05-31 13:32:30.027469: Epoch: 4 Valid Perplexity: 9.595
Seed: free
Sample:  krlk eserlimin and ke
2017-05-31 13:33:19.041731: Epoch: 5 Learning rate: 0.050
valid data size: 363899
2017-05-31 13:33:21.985966: 0.028 perplexity: 11.805 speed: 3850 wps
2017-05-31 13:33:31.769477: 0.127 perplexity: 10.824 speed: 3706 wps
2017-05-31 13:33:41.440434: 0.225 perplexity: 10.692 speed: 3706 wps
2017-05-31 13:33:51.175609: 0.324 perplexity: 10.737 speed: 3699 wps
2017-05-31 13:34:00.981702: 0.423 perplexity: 10.607 speed: 3688 wps
2017-05-31 13:34:10.783844: 0.521 perplexity: 10.583 speed: 3682 wps
2017-05-31 13:34:20.522406: 0.620 perplexity: 10.539 speed: 3682 wps
2017-05-31 13:34:30.305357: 0.718 perplexity: 10.506 speed: 3679 wps
2017-05-31 13:34:40.120287: 0.817 perplexity: 10.518 speed: 3676 wps
2017-05-31 13:34:49.846669: 0.915 perplexity: 10.525 speed: 3677 wps
2017-05-31 13:34:57.994608: Epoch: 5 Train Perplexity: 10.540
valid data size: 121300
2017-05-31 13:35:09.342042: Epoch: 5 Valid Perplexity: 9.426
Seed: free
Sample: mlini be<eos>de utmayamyda v
2017-05-31 13:36:03.991163: Epoch: 6 Learning rate: 0.050
valid data size: 363899
2017-05-31 13:36:07.002377: 0.028 perplexity: 11.710 speed: 3761 wps
2017-05-31 13:36:16.734577: 0.127 perplexity: 10.897 speed: 3701 wps
2017-05-31 13:36:26.481987: 0.225 perplexity: 10.750 speed: 3691 wps
2017-05-31 13:36:36.347902: 0.324 perplexity: 10.771 speed: 3673 wps
2017-05-31 13:36:46.130010: 0.423 perplexity: 10.650 speed: 3671 wps
2017-05-31 13:36:55.820076: 0.521 perplexity: 10.605 speed: 3676 wps
2017-05-31 13:37:05.495376: 0.620 perplexity: 10.601 speed: 3680 wps
2017-05-31 13:37:15.356323: 0.718 perplexity: 10.572 speed: 3674 wps
2017-05-31 13:37:25.028868: 0.817 perplexity: 10.558 speed: 3678 wps
2017-05-31 13:37:34.716013: 0.915 perplexity: 10.568 speed: 3680 wps
2017-05-31 13:37:42.699600: Epoch: 6 Train Perplexity: 10.588
valid data size: 121300
2017-05-31 13:37:53.966158: Epoch: 6 Valid Perplexity: 9.204
Seed: free
Sample: reyereminin vu d re kn
2017-05-31 13:38:42.788141: Epoch: 7 Learning rate: 0.050
valid data size: 363899
2017-05-31 13:38:45.792028: 0.028 perplexity: 11.752 speed: 3770 wps
2017-05-31 13:38:55.492084: 0.127 perplexity: 10.958 speed: 3713 wps
2017-05-31 13:39:05.209078: 0.225 perplexity: 10.782 speed: 3702 wps
2017-05-31 13:39:15.055727: 0.324 perplexity: 10.790 speed: 3683 wps
2017-05-31 13:39:24.932976: 0.423 perplexity: 10.644 speed: 3670 wps
2017-05-31 13:39:34.864368: 0.521 perplexity: 10.602 speed: 3659 wps
2017-05-31 13:39:44.880889: 0.620 perplexity: 10.564 speed: 3646 wps
2017-05-31 13:39:54.684712: 0.718 perplexity: 10.549 speed: 3647 wps
2017-05-31 13:40:04.305737: 0.817 perplexity: 10.513 speed: 3656 wps
2017-05-31 13:40:14.017342: 0.915 perplexity: 10.515 speed: 3660 wps
2017-05-31 13:40:22.048031: Epoch: 7 Train Perplexity: 10.543
valid data size: 121300
2017-05-31 13:40:33.446133: Epoch: 7 Valid Perplexity: 9.275
Seed: free
Sample: rdinme<eos>yazyni bonru, okl
2017-05-31 13:40:34.018157: Epoch: 8 Learning rate: 0.050
valid data size: 363899
2017-05-31 13:40:37.058617: 0.028 perplexity: 11.733 speed: 3730 wps
2017-05-31 13:40:46.738247: 0.127 perplexity: 10.968 speed: 3709 wps
2017-05-31 13:40:56.350917: 0.225 perplexity: 10.763 speed: 3717 wps
2017-05-31 13:41:06.029527: 0.324 perplexity: 10.767 speed: 3713 wps
2017-05-31 13:41:15.717371: 0.423 perplexity: 10.643 speed: 3710 wps
2017-05-31 13:41:25.594172: 0.521 perplexity: 10.580 speed: 3694 wps
2017-05-31 13:41:35.218658: 0.620 perplexity: 10.534 speed: 3699 wps
2017-05-31 13:41:44.993656: 0.718 perplexity: 10.496 speed: 3695 wps
2017-05-31 13:41:54.787132: 0.817 perplexity: 10.469 speed: 3690 wps
2017-05-31 13:42:04.485985: 0.915 perplexity: 10.451 speed: 3691 wps
2017-05-31 13:42:12.523611: Epoch: 8 Train Perplexity: 10.478
valid data size: 121300
2017-05-31 13:42:23.870904: Epoch: 8 Valid Perplexity: 9.203
Seed: free
Sample: n btndeyen viy verecen
2017-05-31 13:43:16.582183: Epoch: 9 Learning rate: 0.050
valid data size: 363899
2017-05-31 13:43:19.681139: 0.028 perplexity: 12.167 speed: 3669 wps
2017-05-31 13:43:29.560566: 0.127 perplexity: 11.152 speed: 3638 wps
2017-05-31 13:43:39.433945: 0.225 perplexity: 10.861 speed: 3634 wps
2017-05-31 13:43:49.235422: 0.324 perplexity: 10.824 speed: 3641 wps
2017-05-31 13:43:58.976704: 0.423 perplexity: 10.726 speed: 3650 wps
2017-05-31 13:44:08.709656: 0.521 perplexity: 10.755 speed: 3656 wps
2017-05-31 13:44:18.417255: 0.620 perplexity: 10.696 speed: 3662 wps
2017-05-31 13:44:28.355822: 0.718 perplexity: 10.661 speed: 3654 wps
2017-05-31 13:44:37.984912: 0.817 perplexity: 10.618 speed: 3662 wps
2017-05-31 13:44:47.797616: 0.915 perplexity: 10.601 speed: 3661 wps
2017-05-31 13:44:55.812711: Epoch: 9 Train Perplexity: 10.623
valid data size: 121300
2017-05-31 13:45:06.995708: Epoch: 9 Valid Perplexity: 9.373
Seed: free
Sample: dlecini gkldak Simderil
2017-05-31 13:45:07.363756: Epoch: 10 Learning rate: 0.050
valid data size: 363899
2017-05-31 13:45:10.511095: 0.028 perplexity: 11.417 speed: 3595 wps
2017-05-31 13:45:20.293198: 0.127 perplexity: 10.876 speed: 3647 wps
2017-05-31 13:45:30.112522: 0.225 perplexity: 10.737 speed: 3648 wps
2017-05-31 13:45:40.185345: 0.324 perplexity: 10.784 speed: 3621 wps
2017-05-31 13:45:49.764317: 0.423 perplexity: 10.634 speed: 3648 wps
2017-05-31 13:45:59.416100: 0.521 perplexity: 10.620 speed: 3660 wps
2017-05-31 13:46:09.160122: 0.620 perplexity: 10.555 speed: 3663 wps
2017-05-31 13:46:18.887656: 0.718 perplexity: 10.525 speed: 3666 wps
2017-05-31 13:46:28.696021: 0.817 perplexity: 10.512 speed: 3664 wps
2017-05-31 13:46:38.667259: 0.915 perplexity: 10.522 speed: 3657 wps
2017-05-31 13:46:46.438043: Epoch: 10 Train Perplexity: 10.558
valid data size: 121300
2017-05-31 13:46:57.591855: Epoch: 10 Valid Perplexity: 9.212
Seed: free
Sample:  tez, nimanys ganlyda<eos>
2017-05-31 13:46:57.793782: Epoch: 11 Learning rate: 0.050
valid data size: 363899
2017-05-31 13:47:00.913373: 0.028 perplexity: 11.535 speed: 3638 wps
2017-05-31 13:47:10.750684: 0.127 perplexity: 10.865 speed: 3642 wps
2017-05-31 13:47:20.387024: 0.225 perplexity: 10.800 speed: 3675 wps
2017-05-31 13:47:30.152376: 0.324 perplexity: 10.839 speed: 3673 wps
2017-05-31 13:47:39.938012: 0.423 perplexity: 10.747 speed: 3671 wps
2017-05-31 13:47:49.855525: 0.521 perplexity: 10.687 speed: 3660 wps
2017-05-31 13:47:59.687334: 0.620 perplexity: 10.664 speed: 3658 wps
2017-05-31 13:48:09.607706: 0.718 perplexity: 10.641 speed: 3651 wps
2017-05-31 13:48:19.322370: 0.817 perplexity: 10.616 speed: 3656 wps
2017-05-31 13:48:29.073407: 0.915 perplexity: 10.611 speed: 3658 wps
2017-05-31 13:48:37.176634: Epoch: 11 Train Perplexity: 10.616
valid data size: 121300
2017-05-31 13:48:48.622477: Epoch: 11 Valid Perplexity: 9.154
Seed: free
Sample: lfe<eos>kekiline asmanl. O
2017-05-31 13:49:36.990432: Epoch: 12 Learning rate: 0.050
valid data size: 363899
2017-05-31 13:49:40.181648: 0.028 perplexity: 11.928 speed: 3567 wps
2017-05-31 13:49:49.932302: 0.127 perplexity: 11.655 speed: 3649 wps
2017-05-31 13:49:59.691292: 0.225 perplexity: 11.399 speed: 3659 wps
2017-05-31 13:50:09.388393: 0.324 perplexity: 11.206 speed: 3670 wps
2017-05-31 13:50:19.141987: 0.423 perplexity: 11.036 speed: 3671 wps
2017-05-31 13:50:28.856499: 0.521 perplexity: 11.002 speed: 3675 wps
2017-05-31 13:50:38.681075: 0.620 perplexity: 10.895 speed: 3670 wps
2017-05-31 13:50:48.452784: 0.718 perplexity: 10.846 speed: 3670 wps
2017-05-31 13:50:58.037586: 0.817 perplexity: 10.802 speed: 3678 wps
2017-05-31 13:51:07.919598: 0.915 perplexity: 10.793 speed: 3673 wps
2017-05-31 13:51:15.791634: Epoch: 12 Train Perplexity: 10.821
valid data size: 121300
2017-05-31 13:51:27.192780: Epoch: 12 Valid Perplexity: 9.354
Seed: free
Sample: nse<eos>keldenler daktarlard
2017-05-31 13:51:27.746843: Epoch: 13 Learning rate: 0.050
valid data size: 363899
2017-05-31 13:51:30.975272: 0.028 perplexity: 12.321 speed: 3508 wps
2017-05-31 13:51:40.800264: 0.127 perplexity: 11.167 speed: 3613 wps
2017-05-31 13:51:50.521133: 0.225 perplexity: 11.058 speed: 3645 wps
2017-05-31 13:52:00.139595: 0.324 perplexity: 11.045 speed: 3669 wps
2017-05-31 13:52:10.017119: 0.423 perplexity: 10.922 speed: 3660 wps
2017-05-31 13:52:19.639988: 0.521 perplexity: 10.873 speed: 3672 wps
2017-05-31 13:52:29.696247: 0.620 perplexity: 10.829 speed: 3654 wps
2017-05-31 13:52:39.459813: 0.718 perplexity: 10.824 speed: 3656 wps
2017-05-31 13:52:48.966575: 0.817 perplexity: 10.789 speed: 3670 wps
2017-05-31 13:52:58.751074: 0.915 perplexity: 10.790 speed: 3669 wps
2017-05-31 13:53:06.708824: Epoch: 13 Train Perplexity: 10.801
valid data size: 121300
2017-05-31 13:53:18.073539: Epoch: 13 Valid Perplexity: 9.178
Seed: free
Sample: y olsuunu basta aman s
2017-05-31 13:53:18.264506: Epoch: 14 Learning rate: 0.050
valid data size: 363899
2017-05-31 13:53:21.261020: 0.028 perplexity: 11.983 speed: 3778 wps
2017-05-31 13:53:31.234525: 0.127 perplexity: 11.202 speed: 3636 wps
2017-05-31 13:53:40.932385: 0.225 perplexity: 11.097 speed: 3662 wps
2017-05-31 13:53:50.730297: 0.324 perplexity: 11.072 speed: 3660 wps
2017-05-31 13:54:00.403030: 0.423 perplexity: 10.896 speed: 3671 wps
2017-05-31 13:54:10.188315: 0.521 perplexity: 10.867 speed: 3669 wps
2017-05-31 13:54:19.909450: 0.620 perplexity: 10.838 speed: 3672 wps
2017-05-31 13:54:29.767711: 0.718 perplexity: 10.791 speed: 3667 wps
2017-05-31 13:54:39.619507: 0.817 perplexity: 10.733 speed: 3663 wps
2017-05-31 13:54:49.455565: 0.915 perplexity: 10.735 speed: 3661 wps
2017-05-31 13:54:57.494749: Epoch: 14 Train Perplexity: 10.768
valid data size: 121300
2017-05-31 13:55:08.726672: Epoch: 14 Valid Perplexity: 9.303
Seed: free
Sample:  vu iki putda vakte bar y
2017-05-31 13:55:08.884962: Epoch: 15 Learning rate: 0.050
valid data size: 363899
2017-05-31 13:55:11.955243: 0.028 perplexity: 11.659 speed: 3684 wps
2017-05-31 13:55:21.765735: 0.127 perplexity: 11.160 speed: 3661 wps
2017-05-31 13:55:31.565044: 0.225 perplexity: 10.960 speed: 3659 wps
2017-05-31 13:55:41.152328: 0.324 perplexity: 10.903 speed: 3683 wps
2017-05-31 13:55:50.772925: 0.423 perplexity: 10.752 speed: 3693 wps
2017-05-31 13:56:00.726005: 0.521 perplexity: 10.732 speed: 3675 wps
2017-05-31 13:56:10.573229: 0.620 perplexity: 10.867 speed: 3669 wps
2017-05-31 13:56:20.280637: 0.718 perplexity: 10.854 speed: 3672 wps
2017-05-31 13:56:30.048681: 0.817 perplexity: 10.806 speed: 3672 wps
2017-05-31 13:56:39.878029: 0.915 perplexity: 10.788 speed: 3669 wps
2017-05-31 13:56:48.067100: Epoch: 15 Train Perplexity: 10.797
valid data size: 121300
2017-05-31 13:56:59.414323: Epoch: 15 Valid Perplexity: 9.199
Seed: free
Sample: reyin Be durur volra<eos>olak
2017-05-31 13:56:59.599790: Epoch: 16 Learning rate: 0.043
valid data size: 363899
2017-05-31 13:57:02.739792: 0.028 perplexity: 11.549 speed: 3615 wps
2017-05-31 13:57:12.473750: 0.127 perplexity: 10.965 speed: 3666 wps
2017-05-31 13:57:22.308950: 0.225 perplexity: 10.712 speed: 3656 wps
2017-05-31 13:57:32.229638: 0.324 perplexity: 10.725 speed: 3643 wps
2017-05-31 13:57:41.897411: 0.423 perplexity: 10.579 speed: 3658 wps
2017-05-31 13:57:51.453991: 0.521 perplexity: 10.536 speed: 3675 wps
2017-05-31 13:58:01.363990: 0.620 perplexity: 10.517 speed: 3665 wps
2017-05-31 13:58:11.223733: 0.718 perplexity: 10.484 speed: 3661 wps
2017-05-31 13:58:21.232368: 0.817 perplexity: 10.472 speed: 3651 wps
2017-05-31 13:58:30.867244: 0.915 perplexity: 10.473 speed: 3659 wps
2017-05-31 13:58:39.105874: Epoch: 16 Train Perplexity: 10.535
valid data size: 121300
2017-05-31 13:58:50.774248: Epoch: 16 Valid Perplexity: 9.213
Seed: free
Sample: rced "Sapek zlmordu.
2017-05-31 13:58:50.953228: Epoch: 17 Learning rate: 0.038
valid data size: 363899
2017-05-31 13:58:54.204615: 0.028 perplexity: 11.602 speed: 3497 wps
2017-05-31 13:59:04.037458: 0.127 perplexity: 10.942 speed: 3608 wps
2017-05-31 13:59:13.662943: 0.225 perplexity: 10.664 speed: 3657 wps
2017-05-31 13:59:23.526854: 0.324 perplexity: 10.690 speed: 3650 wps
2017-05-31 13:59:33.332807: 0.423 perplexity: 10.570 speed: 3651 wps
2017-05-31 13:59:43.076806: 0.521 perplexity: 10.542 speed: 3656 wps
2017-05-31 13:59:52.821951: 0.620 perplexity: 10.475 speed: 3660 wps
2017-05-31 14:00:02.669340: 0.718 perplexity: 10.426 speed: 3657 wps
2017-05-31 14:00:12.591596: 0.817 perplexity: 10.368 speed: 3651 wps
2017-05-31 14:00:22.412857: 0.915 perplexity: 10.348 speed: 3651 wps
2017-05-31 14:00:30.423449: Epoch: 17 Train Perplexity: 10.357
valid data size: 121300
2017-05-31 14:00:41.653368: Epoch: 17 Valid Perplexity: 9.165
Seed: free
Sample: lin viylirorlu iyincerim
2017-05-31 14:00:41.832706: Epoch: 18 Learning rate: 0.033
valid data size: 363899
2017-05-31 14:00:44.885287: 0.028 perplexity: 11.987 speed: 3710 wps
2017-05-31 14:00:54.673957: 0.127 perplexity: 11.001 speed: 3673 wps
2017-05-31 14:01:04.401685: 0.225 perplexity: 10.664 speed: 3678 wps
2017-05-31 14:01:14.199524: 0.324 perplexity: 10.654 speed: 3672 wps
2017-05-31 14:01:23.907649: 0.423 perplexity: 10.495 speed: 3676 wps
2017-05-31 14:01:33.828017: 0.521 perplexity: 10.456 speed: 3664 wps
2017-05-31 14:01:43.551913: 0.620 perplexity: 10.385 speed: 3668 wps
2017-05-31 14:01:53.496080: 0.718 perplexity: 10.346 speed: 3659 wps
2017-05-31 14:02:03.205311: 0.817 perplexity: 10.290 speed: 3663 wps
2017-05-31 14:02:12.766864: 0.915 perplexity: 10.270 speed: 3672 wps
2017-05-31 14:02:20.963061: Epoch: 18 Train Perplexity: 10.283
valid data size: 121300
2017-05-31 14:02:32.428077: Epoch: 18 Valid Perplexity: 8.968
Seed: free
Sample: ksill, onluylabyn<eos>daca
2017-05-31 14:03:19.763478: Epoch: 19 Learning rate: 0.029
valid data size: 363899
2017-05-31 14:03:22.843687: 0.028 perplexity: 11.245 speed: 3679 wps
2017-05-31 14:03:32.651736: 0.127 perplexity: 10.607 speed: 3660 wps
2017-05-31 14:03:42.193815: 0.225 perplexity: 10.456 speed: 3701 wps
2017-05-31 14:03:51.910036: 0.324 perplexity: 10.428 speed: 3697 wps
2017-05-31 14:04:01.636240: 0.423 perplexity: 10.309 speed: 3694 wps
2017-05-31 14:04:11.350586: 0.521 perplexity: 10.243 speed: 3693 wps
2017-05-31 14:04:21.058898: 0.620 perplexity: 10.177 speed: 3693 wps
2017-05-31 14:04:30.744510: 0.718 perplexity: 10.128 speed: 3694 wps
2017-05-31 14:04:40.594288: 0.817 perplexity: 10.084 speed: 3687 wps
2017-05-31 14:04:50.392489: 0.915 perplexity: 10.086 speed: 3684 wps
2017-05-31 14:04:58.548340: Epoch: 19 Train Perplexity: 10.115
valid data size: 121300
2017-05-31 14:05:09.608604: Epoch: 19 Valid Perplexity: 8.982
Seed: free
Sample:  kirimi onarakts vaan a
2017-05-31 14:05:10.000563: Epoch: 20 Learning rate: 0.025
valid data size: 363899
2017-05-31 14:05:12.976973: 0.028 perplexity: 11.256 speed: 3806 wps
2017-05-31 14:05:22.691340: 0.127 perplexity: 10.610 speed: 3717 wps
2017-05-31 14:05:32.448357: 0.225 perplexity: 10.479 speed: 3698 wps
2017-05-31 14:05:42.259198: 0.324 perplexity: 10.419 speed: 3684 wps
2017-05-31 14:05:51.922182: 0.423 perplexity: 10.295 speed: 3690 wps
2017-05-31 14:06:01.707192: 0.521 perplexity: 10.251 speed: 3685 wps
2017-05-31 14:06:11.590266: 0.620 perplexity: 10.191 speed: 3675 wps
2017-05-31 14:06:21.391506: 0.718 perplexity: 10.153 speed: 3673 wps
2017-05-31 14:06:31.300360: 0.817 perplexity: 10.096 speed: 3666 wps
2017-05-31 14:06:40.949849: 0.915 perplexity: 10.072 speed: 3671 wps
2017-05-31 14:06:48.981214: Epoch: 20 Train Perplexity: 10.092
valid data size: 121300
2017-05-31 14:07:00.129142: Epoch: 20 Valid Perplexity: 8.921
Seed: free
Sample: <eos>benenekle an girlullun
2017-05-31 14:07:47.469697: Epoch: 21 Learning rate: 0.022
valid data size: 363899
2017-05-31 14:07:50.574988: 0.028 perplexity: 11.169 speed: 3648 wps
2017-05-31 14:08:00.570340: 0.127 perplexity: 10.288 speed: 3600 wps
2017-05-31 14:08:10.218728: 0.225 perplexity: 10.042 speed: 3649 wps
2017-05-31 14:08:19.703241: 0.324 perplexity: 10.078 speed: 3687 wps
2017-05-31 14:08:29.432911: 0.423 perplexity: 9.954 speed: 3686 wps
2017-05-31 14:08:39.120065: 0.521 perplexity: 9.924 speed: 3689 wps
2017-05-31 14:08:48.950324: 0.620 perplexity: 9.895 speed: 3682 wps
2017-05-31 14:08:58.664471: 0.718 perplexity: 9.899 speed: 3683 wps
2017-05-31 14:09:08.471039: 0.817 perplexity: 9.864 speed: 3680 wps
2017-05-31 14:09:18.356263: 0.915 perplexity: 9.846 speed: 3674 wps
2017-05-31 14:09:26.701588: Epoch: 21 Train Perplexity: 9.864
valid data size: 121300
2017-05-31 14:09:38.042289: Epoch: 21 Valid Perplexity: 8.829
Seed: free
Sample: r,"<eos>dimeriyes girlerir. 
2017-05-31 14:10:30.760137: Epoch: 22 Learning rate: 0.019
valid data size: 363899
2017-05-31 14:10:33.859901: 0.028 perplexity: 10.591 speed: 3657 wps
2017-05-31 14:10:43.665515: 0.127 perplexity: 10.018 speed: 3656 wps
2017-05-31 14:10:53.519008: 0.225 perplexity: 9.991 speed: 3648 wps
2017-05-31 14:11:03.373755: 0.324 perplexity: 10.055 speed: 3644 wps
2017-05-31 14:11:13.049848: 0.423 perplexity: 9.933 speed: 3658 wps
2017-05-31 14:11:22.849335: 0.521 perplexity: 9.894 speed: 3658 wps
2017-05-31 14:11:32.736831: 0.620 perplexity: 9.806 speed: 3653 wps
2017-05-31 14:11:42.433812: 0.718 perplexity: 9.799 speed: 3658 wps
2017-05-31 14:11:52.138169: 0.817 perplexity: 9.765 speed: 3663 wps
2017-05-31 14:12:02.096809: 0.915 perplexity: 9.761 speed: 3656 wps
2017-05-31 14:12:10.185368: Epoch: 22 Train Perplexity: 9.783
valid data size: 121300
2017-05-31 14:12:21.284361: Epoch: 22 Valid Perplexity: 8.823
Seed: free
Sample:  ikinci onarak olmu bese
2017-05-31 14:13:08.433591: Epoch: 23 Learning rate: 0.016
valid data size: 363899
2017-05-31 14:13:11.408855: 0.028 perplexity: 10.750 speed: 3805 wps
2017-05-31 14:13:21.254206: 0.127 perplexity: 10.326 speed: 3678 wps
2017-05-31 14:13:31.071085: 0.225 perplexity: 10.110 speed: 3666 wps
2017-05-31 14:13:40.633401: 0.324 perplexity: 10.064 speed: 3691 wps
2017-05-31 14:13:50.423989: 0.423 perplexity: 9.884 speed: 3684 wps
2017-05-31 14:14:00.256336: 0.521 perplexity: 9.849 speed: 3676 wps
2017-05-31 14:14:10.094232: 0.620 perplexity: 9.836 speed: 3671 wps
2017-05-31 14:14:19.775799: 0.718 perplexity: 9.787 speed: 3675 wps
2017-05-31 14:14:29.663111: 0.817 perplexity: 9.740 speed: 3669 wps
2017-05-31 14:14:39.491932: 0.915 perplexity: 9.757 speed: 3667 wps
2017-05-31 14:14:47.472602: Epoch: 23 Train Perplexity: 9.765
valid data size: 121300
2017-05-31 14:14:58.903066: Epoch: 23 Valid Perplexity: 8.776
Seed: free
Sample:  geb, korusu bot alanna 
2017-05-31 14:15:46.349511: Epoch: 24 Learning rate: 0.014
valid data size: 363899
2017-05-31 14:15:49.448541: 0.028 perplexity: 10.477 speed: 3651 wps
2017-05-31 14:15:59.329474: 0.127 perplexity: 10.162 speed: 3633 wps
2017-05-31 14:16:09.249144: 0.225 perplexity: 10.149 speed: 3624 wps
2017-05-31 14:16:19.005278: 0.324 perplexity: 10.103 speed: 3639 wps
2017-05-31 14:16:28.703555: 0.423 perplexity: 9.894 speed: 3652 wps
2017-05-31 14:16:38.252561: 0.521 perplexity: 9.839 speed: 3671 wps
2017-05-31 14:16:48.098201: 0.620 perplexity: 9.784 speed: 3666 wps
2017-05-31 14:16:57.873545: 0.718 perplexity: 9.756 speed: 3666 wps
2017-05-31 14:17:07.507543: 0.817 perplexity: 9.745 speed: 3672 wps
2017-05-31 14:17:17.414918: 0.915 perplexity: 9.723 speed: 3666 wps
2017-05-31 14:17:25.490419: Epoch: 24 Train Perplexity: 9.736
valid data size: 121300
2017-05-31 14:17:36.878325: Epoch: 24 Valid Perplexity: 8.780
Seed: free
Sample: ndiri<eos>ABram. Alid<eos>de kesr
2017-05-31 14:17:37.257689: Epoch: 25 Learning rate: 0.012
valid data size: 363899
2017-05-31 14:17:40.238542: 0.028 perplexity: 11.042 speed: 3818 wps
2017-05-31 14:17:50.034667: 0.127 perplexity: 10.313 speed: 3695 wps
2017-05-31 14:17:59.790934: 0.225 perplexity: 10.028 speed: 3686 wps
2017-05-31 14:18:09.510128: 0.324 perplexity: 10.164 speed: 3686 wps
2017-05-31 14:18:19.212599: 0.423 perplexity: 9.954 speed: 3688 wps
2017-05-31 14:18:29.029369: 0.521 perplexity: 9.908 speed: 3681 wps
2017-05-31 14:18:38.861030: 0.620 perplexity: 9.823 speed: 3675 wps
2017-05-31 14:18:48.662268: 0.718 perplexity: 9.874 speed: 3673 wps
2017-05-31 14:18:58.588232: 0.817 perplexity: 9.866 speed: 3665 wps
2017-05-31 14:19:08.346517: 0.915 perplexity: 9.860 speed: 3666 wps
2017-05-31 14:19:16.378328: Epoch: 25 Train Perplexity: 9.866
valid data size: 121300
2017-05-31 14:19:27.268819: Epoch: 25 Valid Perplexity: 8.737
Seed: free
Sample: tcemdi olsan. GEREK Z
2017-05-31 14:20:16.552530: Epoch: 26 Learning rate: 0.011
valid data size: 363899
2017-05-31 14:20:19.718704: 0.028 perplexity: 10.731 speed: 3579 wps
2017-05-31 14:20:29.608337: 0.127 perplexity: 10.073 speed: 3613 wps
2017-05-31 14:20:39.430878: 0.225 perplexity: 9.846 speed: 3628 wps
2017-05-31 14:20:49.268162: 0.324 perplexity: 9.809 speed: 3633 wps
2017-05-31 14:20:59.002162: 0.423 perplexity: 9.670 speed: 3644 wps
2017-05-31 14:21:08.736305: 0.521 perplexity: 9.694 speed: 3651 wps
2017-05-31 14:21:18.484479: 0.620 perplexity: 9.675 speed: 3655 wps
2017-05-31 14:21:28.252014: 0.718 perplexity: 9.633 speed: 3657 wps
2017-05-31 14:21:38.046419: 0.817 perplexity: 9.587 speed: 3657 wps
2017-05-31 14:21:47.745487: 0.915 perplexity: 9.557 speed: 3661 wps
2017-05-31 14:21:55.798488: Epoch: 26 Train Perplexity: 9.576
valid data size: 121300
2017-05-31 14:22:06.681965: Epoch: 26 Valid Perplexity: 8.745
Seed: free
Sample:  lulgupan<eos>uygurla sir sar
2017-05-31 14:22:07.035846: Epoch: 27 Learning rate: 0.009
valid data size: 363899
2017-05-31 14:22:10.129909: 0.028 perplexity: 10.390 speed: 3658 wps
2017-05-31 14:22:19.875231: 0.127 perplexity: 10.016 speed: 3673 wps
2017-05-31 14:22:29.724368: 0.225 perplexity: 9.877 speed: 3658 wps
2017-05-31 14:22:39.502358: 0.324 perplexity: 9.991 speed: 3660 wps
2017-05-31 14:22:49.094237: 0.423 perplexity: 9.900 speed: 3678 wps
2017-05-31 14:22:58.560827: 0.521 perplexity: 9.862 speed: 3698 wps
2017-05-31 14:23:07.899036: 0.620 perplexity: 9.790 speed: 3719 wps
2017-05-31 14:23:17.334384: 0.718 perplexity: 9.755 speed: 3730 wps
2017-05-31 14:23:26.872256: 0.817 perplexity: 9.695 speed: 3733 wps
2017-05-31 14:23:36.644876: 0.915 perplexity: 9.683 speed: 3726 wps
2017-05-31 14:23:44.742742: Epoch: 27 Train Perplexity: 9.682
valid data size: 121300
2017-05-31 14:23:56.137551: Epoch: 27 Valid Perplexity: 8.735
Seed: free
Sample: tmi vok ltelce Yeneneye
2017-05-31 14:24:43.039224: Epoch: 28 Learning rate: 0.008
valid data size: 363899
2017-05-31 14:24:46.212853: 0.028 perplexity: 10.411 speed: 3587 wps
2017-05-31 14:24:55.896781: 0.127 perplexity: 9.949 speed: 3673 wps
2017-05-31 14:25:05.649025: 0.225 perplexity: 9.744 speed: 3674 wps
2017-05-31 14:25:15.424726: 0.324 perplexity: 9.698 speed: 3672 wps
2017-05-31 14:25:25.158397: 0.423 perplexity: 9.614 speed: 3674 wps
2017-05-31 14:25:34.925374: 0.521 perplexity: 9.563 speed: 3673 wps
2017-05-31 14:25:44.613558: 0.620 perplexity: 9.516 speed: 3677 wps
2017-05-31 14:25:54.208894: 0.718 perplexity: 9.509 speed: 3685 wps
2017-05-31 14:26:03.934038: 0.817 perplexity: 9.465 speed: 3685 wps
2017-05-31 14:26:13.825442: 0.915 perplexity: 9.464 speed: 3678 wps
2017-05-31 14:26:21.798117: Epoch: 28 Train Perplexity: 9.499
valid data size: 121300
2017-05-31 14:26:33.103075: Epoch: 28 Valid Perplexity: 8.724
Seed: free
Sample: liden hu kililinin rim!
2017-05-31 14:27:25.291690: Epoch: 29 Learning rate: 0.007
valid data size: 363899
2017-05-31 14:27:28.378370: 0.028 perplexity: 10.487 speed: 3670 wps
2017-05-31 14:27:38.247549: 0.127 perplexity: 10.272 speed: 3641 wps
2017-05-31 14:27:48.071528: 0.225 perplexity: 10.041 speed: 3644 wps
2017-05-31 14:27:57.709746: 0.324 perplexity: 10.195 speed: 3666 wps
2017-05-31 14:28:07.323207: 0.423 perplexity: 9.927 speed: 3680 wps
2017-05-31 14:28:17.203796: 0.521 perplexity: 9.854 speed: 3670 wps
2017-05-31 14:28:26.720852: 0.620 perplexity: 9.831 speed: 3685 wps
2017-05-31 14:28:36.474332: 0.718 perplexity: 9.807 speed: 3684 wps
2017-05-31 14:28:46.224740: 0.817 perplexity: 9.777 speed: 3683 wps
2017-05-31 14:28:55.994311: 0.915 perplexity: 9.744 speed: 3681 wps
2017-05-31 14:29:04.105471: Epoch: 29 Train Perplexity: 9.722
valid data size: 121300
2017-05-31 14:29:15.437864: Epoch: 29 Valid Perplexity: 8.707
Seed: free
Sample: smi evetise bam? Ames gak
2017-05-31 14:30:03.178416: Epoch: 30 Learning rate: 0.006
valid data size: 363899
2017-05-31 14:30:06.322116: 0.028 perplexity: 10.484 speed: 3618 wps
2017-05-31 14:30:15.867034: 0.127 perplexity: 9.751 speed: 3721 wps
2017-05-31 14:30:25.459288: 0.225 perplexity: 9.552 speed: 3728 wps
2017-05-31 14:30:35.134108: 0.324 perplexity: 9.569 speed: 3721 wps
2017-05-31 14:30:44.905739: 0.423 perplexity: 9.498 speed: 3708 wps
2017-05-31 14:30:54.511843: 0.521 perplexity: 9.562 speed: 3713 wps
2017-05-31 14:31:04.003706: 0.620 perplexity: 9.531 speed: 3722 wps
2017-05-31 14:31:14.022986: 0.718 perplexity: 9.476 speed: 3702 wps
2017-05-31 14:31:23.721843: 0.817 perplexity: 9.482 speed: 3701 wps
2017-05-31 14:31:33.496199: 0.915 perplexity: 9.562 speed: 3697 wps
2017-05-31 14:31:41.489119: Epoch: 30 Train Perplexity: 9.570
valid data size: 121300
2017-05-31 14:31:52.705836: Epoch: 30 Valid Perplexity: 8.700
########################################################################################
########################################################################################
########################################################################################
2017-05-31 14:36:48.723980: Running on UCSC:citrisdense...
Distinct terms: 83
########################################################################################
########################################################################################
########################################################################################
2017-05-31 14:39:28.323241: Running on UCSC:citrisdense...
Distinct terms: 83
epoch 56
Seed: reality
Sample: <eos>e58a':hj@D,XOf*%zvf!j -[,3S42oOZ?lq6:P'
2017-05-31 14:39:56.252250: Epoch: 1 Learning rate: 0.100
valid data size: 761929
2017-05-31 14:39:57.205985: 0.026 perplexity: 64.986 speed: 23855 wps
2017-05-31 14:39:59.255075: 0.126 perplexity: 36.015 speed: 32982 wps
2017-05-31 14:40:01.327884: 0.226 perplexity: 31.095 speed: 34496 wps
2017-05-31 14:40:03.411199: 0.326 perplexity: 30.371 speed: 35076 wps
2017-05-31 14:40:05.504768: 0.426 perplexity: 28.442 speed: 35354 wps
2017-05-31 14:40:07.614876: 0.526 perplexity: 27.283 speed: 35477 wps
2017-05-31 14:40:09.717682: 0.626 perplexity: 26.670 speed: 35581 wps
2017-05-31 14:40:11.792531: 0.726 perplexity: 26.168 speed: 35722 wps
2017-05-31 14:40:13.898263: 0.826 perplexity: 25.786 speed: 35766 wps
2017-05-31 14:40:15.991655: 0.926 perplexity: 25.451 speed: 35823 wps
2017-05-31 14:40:17.490391: Epoch: 1 Train Perplexity: 25.263
valid data size: 253977
2017-05-31 14:40:19.887315: Epoch: 1 Valid Perplexity: 22.961
Seed: reality
Sample: Rrmgne sm.e n tse ntaeohs"nr haaosemt in
2017-05-31 14:40:26.140764: Epoch: 2 Learning rate: 0.100
valid data size: 761929
2017-05-31 14:40:26.815608: 0.026 perplexity: 28.340 speed: 33304 wps
2017-05-31 14:40:28.925332: 0.126 perplexity: 24.148 speed: 35375 wps
2017-05-31 14:40:31.002424: 0.226 perplexity: 23.635 speed: 35895 wps
2017-05-31 14:40:33.046165: 0.326 perplexity: 23.383 speed: 36278 wps
2017-05-31 14:40:35.095451: 0.426 perplexity: 23.282 speed: 36464 wps
2017-05-31 14:40:37.122775: 0.526 perplexity: 23.192 speed: 36653 wps
2017-05-31 14:40:39.152968: 0.626 perplexity: 23.161 speed: 36775 wps
2017-05-31 14:40:41.146417: 0.726 perplexity: 23.114 speed: 36955 wps
2017-05-31 14:40:43.132993: 0.826 perplexity: 23.078 speed: 37107 wps
2017-05-31 14:40:45.119998: 0.926 perplexity: 23.039 speed: 37227 wps
2017-05-31 14:40:46.565973: Epoch: 2 Train Perplexity: 23.032
valid data size: 253977
2017-05-31 14:40:48.747253: Epoch: 2 Valid Perplexity: 2731.724
2017-05-31 14:40:49.000580: Seed: reality
2017-05-31 14:40:49.016244: Sample: mmki, apItea  unnae cwHjth  eat,yirotthy
valid data size: 138533
2017-05-31 14:46:54.888958: Test Perplexity: 23.491
2017-05-31 14:46:55.013734: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-31 14:51:34.671510: Running on UCSC:citrisdense...
Distinct terms: 83
epoch 56
Seed: reality
Sample: rwyet mnmrodhWlislcyaetwhhrTy ik   slstm
2017-05-31 14:52:02.930727: Epoch: 1 Learning rate: 0.100
valid data size: 761929
2017-05-31 14:52:03.727523: 0.003 perplexity: 27.784 speed: 2876 wps
2017-05-31 14:52:18.945916: 0.102 perplexity: 23.757 speed: 4893 wps
2017-05-31 14:52:34.056902: 0.202 perplexity: 23.276 speed: 4959 wps
2017-05-31 14:52:49.215786: 0.302 perplexity: 23.092 speed: 4977 wps
2017-05-31 14:53:04.330191: 0.402 perplexity: 23.033 speed: 4990 wps
2017-05-31 14:53:19.179013: 0.501 perplexity: 23.053 speed: 5015 wps
2017-05-31 14:53:33.644375: 0.601 perplexity: 23.046 speed: 5053 wps
2017-05-31 14:53:48.256769: 0.701 perplexity: 23.025 speed: 5073 wps
2017-05-31 14:54:02.984512: 0.801 perplexity: 23.057 speed: 5084 wps
2017-05-31 14:54:17.997928: 0.900 perplexity: 23.039 speed: 5082 wps
2017-05-31 14:54:32.619877: Epoch: 1 Train Perplexity: 23.045
valid data size: 253977
2017-05-31 14:54:48.343027: Epoch: 1 Valid Perplexity: 23.155
Seed: reality
Sample: olcdloh  vo e, n eit'veen nfhooypad enol
2017-05-31 14:54:55.588978: Epoch: 2 Learning rate: 0.100
valid data size: 761929
2017-05-31 14:54:56.145074: 0.003 perplexity: 22.956 speed: 4084 wps
2017-05-31 14:55:10.958838: 0.102 perplexity: 23.516 speed: 5094 wps
2017-05-31 14:55:25.811394: 0.202 perplexity: 23.165 speed: 5105 wps
2017-05-31 14:55:40.433494: 0.302 perplexity: 23.013 speed: 5135 wps
2017-05-31 14:55:55.063759: 0.402 perplexity: 22.972 speed: 5150 wps
2017-05-31 14:56:09.517976: 0.501 perplexity: 23.003 speed: 5171 wps
2017-05-31 14:56:24.008992: 0.601 perplexity: 23.004 speed: 5183 wps
2017-05-31 14:56:38.475155: 0.701 perplexity: 22.989 speed: 5193 wps
2017-05-31 14:56:52.778266: 0.801 perplexity: 23.025 speed: 5208 wps
2017-05-31 14:57:07.489143: 0.900 perplexity: 23.010 speed: 5203 wps
2017-05-31 14:57:22.279977: Epoch: 2 Train Perplexity: 23.020
valid data size: 253977
2017-05-31 14:57:37.808423: Epoch: 2 Valid Perplexity: 23.155
Seed: reality
Sample: e!e hno  nao hme.ehonia i"nomnmhsw "diey
2017-05-31 14:57:44.276281: Epoch: 3 Learning rate: 0.100
valid data size: 761929
2017-05-31 14:57:44.784877: 0.003 perplexity: 22.956 speed: 4578 wps
2017-05-31 14:57:59.605289: 0.102 perplexity: 23.516 speed: 5111 wps
2017-05-31 14:58:15.441382: 0.202 perplexity: 23.162 speed: 4952 wps
2017-05-31 14:58:30.304524: 0.302 perplexity: 23.011 speed: 5004 wps
2017-05-31 14:58:44.380610: 0.402 perplexity: 22.970 speed: 5097 wps
2017-05-31 14:58:58.837919: 0.501 perplexity: 23.002 speed: 5128 wps
2017-05-31 14:59:13.466668: 0.601 perplexity: 23.004 speed: 5139 wps
2017-05-31 14:59:28.000443: 0.701 perplexity: 22.989 speed: 5152 wps
2017-05-31 14:59:42.796703: 0.801 perplexity: 23.025 speed: 5150 wps
2017-05-31 14:59:57.730863: 0.900 perplexity: 23.010 speed: 5143 wps
2017-05-31 15:00:12.250990: Epoch: 3 Train Perplexity: 23.020
valid data size: 253977
2017-05-31 15:00:27.392727: Epoch: 3 Valid Perplexity: 23.155
Seed: reality
Sample: dga aury  inewneyteetid  .mtanr t'  mai'
2017-05-31 15:00:34.031433: Epoch: 4 Learning rate: 0.100
valid data size: 761929
2017-05-31 15:00:34.500907: 0.003 perplexity: 22.956 speed: 4862 wps
2017-05-31 15:00:49.481847: 0.102 perplexity: 23.516 speed: 5067 wps
2017-05-31 15:01:03.971513: 0.202 perplexity: 23.157 speed: 5153 wps
2017-05-31 15:01:18.856289: 0.302 perplexity: 23.007 speed: 5137 wps
2017-05-31 15:01:33.595448: 0.402 perplexity: 22.967 speed: 5142 wps
2017-05-31 15:01:48.341998: 0.501 perplexity: 22.999 speed: 5144 wps
2017-05-31 15:02:03.186235: 0.601 perplexity: 23.001 speed: 5140 wps
2017-05-31 15:02:17.966794: 0.701 perplexity: 22.986 speed: 5141 wps
2017-05-31 15:02:32.512881: 0.801 perplexity: 23.023 speed: 5151 wps
2017-05-31 15:02:47.082605: 0.900 perplexity: 23.008 speed: 5158 wps
2017-05-31 15:03:01.664916: Epoch: 4 Train Perplexity: 23.018
valid data size: 253977
2017-05-31 15:03:16.576392: Epoch: 4 Valid Perplexity: 23.155
Seed: reality
Sample:  mwht dkle   nn ee wagtrc     oi uo  tms
2017-05-31 15:03:16.972707: Epoch: 5 Learning rate: 0.100
valid data size: 761929
2017-05-31 15:03:17.498147: 0.003 perplexity: 22.956 speed: 4325 wps
2017-05-31 15:03:32.147330: 0.102 perplexity: 23.516 speed: 5159 wps
2017-05-31 15:03:47.053004: 0.202 perplexity: 23.163 speed: 5129 wps
2017-05-31 15:04:01.646583: 0.302 perplexity: 23.010 speed: 5155 wps
2017-05-31 15:04:16.372409: 0.402 perplexity: 22.970 speed: 5156 wps
2017-05-31 15:04:31.032094: 0.501 perplexity: 23.001 speed: 5162 wps
2017-05-31 15:04:46.016437: 0.601 perplexity: 23.003 speed: 5147 wps
2017-05-31 15:05:00.586408: 0.701 perplexity: 22.988 speed: 5157 wps
2017-05-31 15:05:15.554828: 0.801 perplexity: 23.024 speed: 5147 wps
2017-05-31 15:05:30.285415: 0.900 perplexity: 23.010 speed: 5148 wps
2017-05-31 15:05:44.604510: Epoch: 5 Train Perplexity: 23.019
valid data size: 253977
2017-05-31 15:05:58.622888: Epoch: 5 Valid Perplexity: 23.155
Seed: reality
Sample: giach?ip rcs<eos>stlgouP otsRrtn ootuleoe  a
2017-05-31 15:05:58.844922: Epoch: 6 Learning rate: 0.100
valid data size: 761929
2017-05-31 15:05:59.310911: 0.003 perplexity: 22.956 speed: 4873 wps
2017-05-31 15:06:13.774538: 0.102 perplexity: 23.516 speed: 5243 wps
2017-05-31 15:06:28.359544: 0.202 perplexity: 23.157 speed: 5227 wps
2017-05-31 15:06:42.790015: 0.302 perplexity: 23.007 speed: 5240 wps
2017-05-31 15:06:57.739249: 0.402 perplexity: 22.967 speed: 5200 wps
2017-05-31 15:07:12.933321: 0.501 perplexity: 22.999 speed: 5160 wps
2017-05-31 15:07:27.819775: 0.601 perplexity: 23.001 speed: 5151 wps
2017-05-31 15:07:42.421981: 0.701 perplexity: 22.986 speed: 5158 wps
2017-05-31 15:07:57.462314: 0.801 perplexity: 23.023 speed: 5145 wps
2017-05-31 15:08:12.332689: 0.900 perplexity: 23.008 speed: 5141 wps
2017-05-31 15:08:26.880274: Epoch: 6 Train Perplexity: 23.018
valid data size: 253977
2017-05-31 15:08:42.511027: Epoch: 6 Valid Perplexity: 23.155
Seed: reality
Sample: bkt iuei l,aiakre.leosw    b<eos>Pconyn nlio
2017-05-31 15:08:49.008003: Epoch: 7 Learning rate: 0.100
valid data size: 761929
2017-05-31 15:08:49.474998: 0.003 perplexity: 22.956 speed: 4875 wps
2017-05-31 15:09:04.382627: 0.102 perplexity: 23.516 speed: 5091 wps
2017-05-31 15:09:19.474438: 0.202 perplexity: 23.158 speed: 5064 wps
2017-05-31 15:09:34.104127: 0.302 perplexity: 23.008 speed: 5106 wps
2017-05-31 15:09:49.122163: 0.402 perplexity: 22.968 speed: 5095 wps
2017-05-31 15:10:04.233875: 0.501 perplexity: 23.000 speed: 5082 wps
2017-05-31 15:10:19.368258: 0.601 perplexity: 23.003 speed: 5072 wps
2017-05-31 15:10:34.390133: 0.701 perplexity: 22.988 speed: 5070 wps
2017-05-31 15:10:49.395659: 0.801 perplexity: 23.024 speed: 5069 wps
2017-05-31 15:11:03.954209: 0.900 perplexity: 23.009 speed: 5086 wps
2017-05-31 15:11:18.795329: Epoch: 7 Train Perplexity: 23.019
valid data size: 253977
2017-05-31 15:11:34.033313: Epoch: 7 Valid Perplexity: 23.155
Seed: reality
Sample:  ieY uaube  chceYuryvydvur ak thaonoee<eos>h
2017-05-31 15:11:34.449521: Epoch: 8 Learning rate: 0.100
valid data size: 761929
2017-05-31 15:11:34.946964: 0.003 perplexity: 22.956 speed: 4615 wps
2017-05-31 15:11:49.030429: 0.102 perplexity: 23.516 speed: 5371 wps
2017-05-31 15:12:03.016553: 0.202 perplexity: 23.157 speed: 5402 wps
2017-05-31 15:12:16.881417: 0.302 perplexity: 23.007 speed: 5428 wps
2017-05-31 15:12:30.633002: 0.402 perplexity: 22.967 speed: 5452 wps
2017-05-31 15:12:44.566232: 0.501 perplexity: 22.999 speed: 5453 wps
2017-05-31 15:13:00.060958: 0.601 perplexity: 23.001 speed: 5353 wps
2017-05-31 15:13:16.627471: 0.701 perplexity: 22.986 speed: 5229 wps
2017-05-31 15:13:33.116522: 0.801 perplexity: 23.023 speed: 5143 wps
2017-05-31 15:13:49.097683: 0.900 perplexity: 23.008 speed: 5097 wps
2017-05-31 15:14:05.276606: Epoch: 8 Train Perplexity: 23.018
valid data size: 253977
2017-05-31 15:14:19.669146: Epoch: 8 Valid Perplexity: 23.155
Seed: reality
Sample: e nt ihsimaenh umcoircnl,aantia udr    p
2017-05-31 15:14:19.952427: Epoch: 9 Learning rate: 0.100
valid data size: 761929
2017-05-31 15:14:20.507168: 0.003 perplexity: 22.956 speed: 4211 wps
2017-05-31 15:14:35.988408: 0.102 perplexity: 23.516 speed: 4886 wps
2017-05-31 15:14:51.047458: 0.202 perplexity: 23.160 speed: 4964 wps
2017-05-31 15:15:06.290570: 0.302 perplexity: 23.009 speed: 4971 wps
2017-05-31 15:15:22.226334: 0.402 perplexity: 22.968 speed: 4920 wps
2017-05-31 15:15:37.401131: 0.501 perplexity: 23.000 speed: 4937 wps
2017-05-31 15:15:52.624859: 0.601 perplexity: 23.002 speed: 4946 wps
2017-05-31 15:16:07.514733: 0.701 perplexity: 22.987 speed: 4968 wps
2017-05-31 15:16:22.818478: 0.801 perplexity: 23.024 speed: 4968 wps
2017-05-31 15:16:38.442270: 0.900 perplexity: 23.009 speed: 4956 wps
2017-05-31 15:16:53.442955: Epoch: 9 Train Perplexity: 23.018
valid data size: 253977
2017-05-31 15:17:07.446920: Epoch: 9 Valid Perplexity: 23.155
Seed: reality
Sample: annnldrolpn lfne nli<eos>mhhsnahihnlkn,rldtr
2017-05-31 15:17:07.731585: Epoch: 10 Learning rate: 0.100
valid data size: 761929
2017-05-31 15:17:08.290139: 0.003 perplexity: 22.956 speed: 4140 wps
2017-05-31 15:17:23.467145: 0.102 perplexity: 23.516 speed: 4978 wps
2017-05-31 15:17:39.046137: 0.202 perplexity: 23.157 speed: 4928 wps
2017-05-31 15:17:54.322076: 0.302 perplexity: 23.007 speed: 4944 wps
2017-05-31 15:18:09.818493: 0.402 perplexity: 22.967 speed: 4934 wps
2017-05-31 15:18:24.792642: 0.501 perplexity: 22.999 speed: 4961 wps
2017-05-31 15:18:39.916646: 0.601 perplexity: 23.001 speed: 4972 wps
2017-05-31 15:18:55.194793: 0.701 perplexity: 22.986 speed: 4972 wps
2017-05-31 15:19:10.394851: 0.801 perplexity: 23.023 speed: 4976 wps
2017-05-31 15:19:25.569115: 0.900 perplexity: 23.008 speed: 4979 wps
2017-05-31 15:19:41.252456: Epoch: 10 Train Perplexity: 23.018
valid data size: 253977
2017-05-31 15:19:55.970096: Epoch: 10 Valid Perplexity: 23.155
Seed: reality
Sample: Ir ,at aheyPegnie" aeih eapc"lecec eueoi
2017-05-31 15:20:02.683928: Epoch: 11 Learning rate: 0.100
valid data size: 761929
2017-05-31 15:20:03.194763: 0.003 perplexity: 22.956 speed: 4667 wps
2017-05-31 15:20:18.535295: 0.102 perplexity: 23.516 speed: 4946 wps
2017-05-31 15:20:35.110232: 0.202 perplexity: 23.157 speed: 4761 wps
2017-05-31 15:20:50.726954: 0.302 perplexity: 23.007 speed: 4795 wps
2017-05-31 15:21:05.673653: 0.402 perplexity: 22.967 speed: 4864 wps
2017-05-31 15:21:21.294670: 0.501 perplexity: 22.999 speed: 4864 wps
2017-05-31 15:21:36.547468: 0.601 perplexity: 23.001 speed: 4884 wps
2017-05-31 15:21:51.452436: 0.701 perplexity: 22.986 speed: 4913 wps
2017-05-31 15:22:06.685548: 0.801 perplexity: 23.023 speed: 4922 wps
2017-05-31 15:22:22.306293: 0.900 perplexity: 23.008 speed: 4916 wps
2017-05-31 15:22:37.552471: Epoch: 11 Train Perplexity: 23.018
valid data size: 253977
2017-05-31 15:22:51.139499: Epoch: 11 Valid Perplexity: 23.155
Seed: reality
Sample:   as"e  ha tskrs<eos>I c<eos>r,ltfow   rWaeieae 
2017-05-31 15:22:51.589924: Epoch: 12 Learning rate: 0.100
valid data size: 761929
2017-05-31 15:22:52.091487: 0.003 perplexity: 22.956 speed: 4699 wps
2017-05-31 15:23:07.515280: 0.102 perplexity: 23.516 speed: 4921 wps
2017-05-31 15:23:22.949873: 0.202 perplexity: 23.157 speed: 4922 wps
2017-05-31 15:23:38.075758: 0.302 perplexity: 23.007 speed: 4956 wps
2017-05-31 15:23:52.952012: 0.402 perplexity: 22.967 speed: 4993 wps
2017-05-31 15:24:07.936115: 0.501 perplexity: 22.999 speed: 5008 wps
2017-05-31 15:24:23.112931: 0.601 perplexity: 23.001 speed: 5008 wps
2017-05-31 15:24:38.100692: 0.701 perplexity: 22.986 speed: 5017 wps
2017-05-31 15:24:53.230382: 0.801 perplexity: 23.023 speed: 5018 wps
2017-05-31 15:25:08.503930: 0.900 perplexity: 23.008 speed: 5013 wps
2017-05-31 15:25:23.500116: Epoch: 12 Train Perplexity: 23.018
valid data size: 253977
2017-05-31 15:25:36.846689: Epoch: 12 Valid Perplexity: 23.155
Seed: reality
Sample: sc<eos> iae trrc<eos>ingsfyohuotIiIh eh hani u. 
2017-05-31 15:25:37.064542: Epoch: 13 Learning rate: 0.100
valid data size: 761929
2017-05-31 15:25:37.533296: 0.003 perplexity: 22.956 speed: 4858 wps
2017-05-31 15:25:51.436065: 0.102 perplexity: 23.516 speed: 5447 wps
2017-05-31 15:26:05.805934: 0.202 perplexity: 23.158 speed: 5368 wps
2017-05-31 15:26:20.717620: 0.302 perplexity: 23.007 speed: 5275 wps
########################################################################################
########################################################################################
########################################################################################
2017-05-31 16:19:00.357282: Running on UCSC:citrisdense...
Distinct terms: 83
epoch 56
Seed: reality
Sample: cc"etfarntenlla,hhmmne  hyais hytty'aua 
2017-05-31 16:19:30.075090: Epoch: 1 Learning rate: 0.100
valid data size: 761929
2017-05-31 16:19:30.853624: 0.003 perplexity: 22.956 speed: 2937 wps
2017-05-31 16:19:47.136009: 0.102 perplexity: 23.516 speed: 4592 wps
2017-05-31 16:20:02.978018: 0.202 perplexity: 23.157 speed: 4691 wps
2017-05-31 16:20:19.588911: 0.302 perplexity: 23.007 speed: 4652 wps
2017-05-31 16:20:35.650853: 0.402 perplexity: 22.967 speed: 4672 wps
2017-05-31 16:20:50.760776: 0.501 perplexity: 22.999 speed: 4739 wps
2017-05-31 16:21:05.715263: 0.601 perplexity: 23.001 speed: 4792 wps
2017-05-31 16:21:20.772073: 0.701 perplexity: 22.986 speed: 4827 wps
2017-05-31 16:21:35.912515: 0.801 perplexity: 23.023 speed: 4850 wps
2017-05-31 16:21:51.374813: 0.900 perplexity: 23.008 speed: 4857 wps
2017-05-31 16:22:06.288732: Epoch: 1 Train Perplexity: 23.018
valid data size: 253977
2017-05-31 16:22:21.263215: Epoch: 1 Valid Perplexity: 23.155
Seed: reality
Sample: se  is e   ihnoerr. eswrhltcoaehWhoeoet 
2017-05-31 16:22:27.717812: Epoch: 2 Learning rate: 0.100
valid data size: 761929
2017-05-31 16:22:28.281212: 0.003 perplexity: 22.956 speed: 4008 wps
2017-05-31 16:22:44.656946: 0.102 perplexity: 23.516 speed: 4620 wps
2017-05-31 16:23:00.367552: 0.202 perplexity: 23.157 speed: 4725 wps
2017-05-31 16:23:15.692284: 0.302 perplexity: 23.007 speed: 4800 wps
2017-05-31 16:23:31.087543: 0.402 perplexity: 22.967 speed: 4833 wps
2017-05-31 16:23:46.143697: 0.501 perplexity: 22.999 speed: 4874 wps
2017-05-31 16:24:01.262604: 0.601 perplexity: 23.001 speed: 4899 wps
2017-05-31 16:24:16.489784: 0.701 perplexity: 22.986 speed: 4912 wps
2017-05-31 16:24:31.731019: 0.801 perplexity: 23.023 speed: 4921 wps
2017-05-31 16:24:47.046315: 0.900 perplexity: 23.008 speed: 4926 wps
2017-05-31 16:25:01.879897: Epoch: 2 Train Perplexity: 23.018
valid data size: 253977
2017-05-31 16:25:16.622908: Epoch: 2 Valid Perplexity: 23.155
Seed: reality
Sample: hrnoens.i holdhc.i  noategoulmeuroe h . 
2017-05-31 16:25:17.012008: Epoch: 3 Learning rate: 0.100
valid data size: 761929
2017-05-31 16:25:17.501417: 0.003 perplexity: 22.956 speed: 4651 wps
2017-05-31 16:25:32.826235: 0.102 perplexity: 23.516 speed: 4950 wps
2017-05-31 16:25:48.274815: 0.202 perplexity: 23.157 speed: 4935 wps
2017-05-31 16:26:03.371115: 0.302 perplexity: 23.007 speed: 4967 wps
2017-05-31 16:26:18.284327: 0.402 perplexity: 22.967 speed: 4999 wps
2017-05-31 16:26:33.443540: 0.501 perplexity: 22.999 speed: 5002 wps
2017-05-31 16:26:48.563101: 0.601 perplexity: 23.001 speed: 5006 wps
2017-05-31 16:27:03.839258: 0.701 perplexity: 22.986 speed: 5001 wps
2017-05-31 16:27:18.998648: 0.801 perplexity: 23.023 speed: 5003 wps
2017-05-31 16:27:34.538204: 0.900 perplexity: 23.008 speed: 4990 wps
2017-05-31 16:27:49.429408: Epoch: 3 Train Perplexity: 23.018
valid data size: 253977
2017-05-31 16:28:04.734115: Epoch: 3 Valid Perplexity: 23.155
Seed: reality
Sample: k<eos>eaetba Ip hc m<eos> <eos>ironcn  r-iutmrhhTR  
2017-05-31 16:28:04.978507: Epoch: 4 Learning rate: 0.100
valid data size: 761929
2017-05-31 16:28:05.476499: 0.003 perplexity: 22.956 speed: 4548 wps
2017-05-31 16:28:20.674767: 0.102 perplexity: 23.516 speed: 4987 wps
2017-05-31 16:28:35.512285: 0.202 perplexity: 23.157 speed: 5052 wps
2017-05-31 16:28:50.950092: 0.302 perplexity: 23.007 speed: 5009 wps
2017-05-31 16:29:05.704838: 0.402 perplexity: 22.967 speed: 5043 wps
2017-05-31 16:29:20.766395: 0.501 perplexity: 22.999 speed: 5044 wps
2017-05-31 16:29:35.764887: 0.601 perplexity: 23.001 speed: 5048 wps
2017-05-31 16:29:50.884094: 0.701 perplexity: 22.986 speed: 5045 wps
2017-05-31 16:30:05.694458: 0.801 perplexity: 23.023 speed: 5055 wps
2017-05-31 16:30:20.807581: 0.900 perplexity: 23.008 speed: 5052 wps
2017-05-31 16:30:35.866923: Epoch: 4 Train Perplexity: 23.018
valid data size: 253977
2017-05-31 16:30:51.493951: Epoch: 4 Valid Perplexity: 23.155
Seed: reality
Sample: sfeho-"v? faw <eos>u uee tnr yru s 'snmi l  
2017-05-31 16:30:51.760229: Epoch: 5 Learning rate: 0.100
valid data size: 761929
2017-05-31 16:30:52.275697: 0.003 perplexity: 22.956 speed: 4390 wps
2017-05-31 16:31:07.329221: 0.102 perplexity: 23.516 speed: 5027 wps
2017-05-31 16:31:22.095651: 0.202 perplexity: 23.157 speed: 5086 wps
2017-05-31 16:31:36.403977: 0.302 perplexity: 23.007 speed: 5158 wps
2017-05-31 16:31:50.681369: 0.402 perplexity: 22.967 speed: 5198 wps
2017-05-31 16:32:04.929862: 0.501 perplexity: 22.999 speed: 5224 wps
2017-05-31 16:32:19.419022: 0.601 perplexity: 23.001 speed: 5228 wps
2017-05-31 16:32:34.259803: 0.701 perplexity: 22.986 speed: 5212 wps
2017-05-31 16:32:48.814812: 0.801 perplexity: 23.023 speed: 5214 wps
2017-05-31 16:33:03.426685: 0.900 perplexity: 23.008 speed: 5212 wps
2017-05-31 16:33:18.194340: Epoch: 5 Train Perplexity: 23.018
valid data size: 253977
2017-05-31 16:33:33.186070: Epoch: 5 Valid Perplexity: 23.155
Seed: reality
Sample: uigfotutalh<eos> l twtetw lhr bnh  agianbd<eos>s
2017-05-31 16:33:33.439491: Epoch: 6 Learning rate: 0.100
valid data size: 761929
2017-05-31 16:33:33.999246: 0.003 perplexity: 22.956 speed: 4128 wps
2017-05-31 16:33:48.440237: 0.102 perplexity: 23.516 speed: 5222 wps
2017-05-31 16:34:02.875805: 0.202 perplexity: 23.157 speed: 5243 wps
2017-05-31 16:34:16.996388: 0.302 perplexity: 23.007 speed: 5288 wps
2017-05-31 16:34:31.462879: 0.402 perplexity: 22.967 speed: 5280 wps
2017-05-31 16:34:46.042868: 0.501 perplexity: 22.999 speed: 5266 wps
